# -*- coding: utf-8 -*-
"""
è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºé‡‡é›† - å¾®åšçˆ¬è™«
ç›®æ ‡ï¼šé‡‡é›†2000æ¡å¾®åšèˆ†è®ºï¼ˆå…³é”®è¯ï¼š0110, 9610, 9810, 1039, Temuç­‰ï¼‰
æ—¶é—´ï¼š2025å¹´6æœˆ-12æœˆ
"""

import requests
import json
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
import csv

class WeiboSpider:
    """å¾®åšçˆ¬è™« - é‡‡é›†è·¨å¢ƒç”µå•†ç¨æ”¶ç›¸å…³èˆ†è®º"""
    
    def __init__(self):
        self.posts = []
        self.keywords = [
            '0110é¦™æ¸¯å…¬å¸',
            '9610å¤‡æ¡ˆ',
            '9710B2B',
            '9810æµ·å¤–ä»“',
            '1039å¸‚åœºé‡‡è´­',
            'Temuå…¨æ‰˜ç®¡',
            'è·¨å¢ƒç”µå•†å¢å€¼ç¨',
            'è·¨å¢ƒç”µå•†ç¨æ”¶',
            'è·¨å¢ƒç”µå•†è¡¥ç¨',
        ]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self._random_user_agent()
        })
    
    def _random_user_agent(self):
        """éšæœºUser-Agentï¼Œé¿å…åçˆ¬"""
        agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        return random.choice(agents)
    
    def search_weibo(self, keyword, num_pages=3):
        """
        æœç´¢å¾®åšï¼ˆä½¿ç”¨ç½‘é¡µç‰ˆçˆ¬å–ï¼‰
        æ³¨æ„ï¼šå¾®åšç½‘é¡µç»“æ„ç»å¸¸å˜åŒ–ï¼Œæ­¤æ–¹æ³•å¯èƒ½éœ€è¦è°ƒæ•´
        """
        print(f"\nğŸ“± å¼€å§‹é‡‡é›†å¾®åšï¼š{keyword}")
        
        for page in range(1, num_pages + 1):
            try:
                # å¾®åšæœç´¢URL
                url = f"https://s.weibo.com/weibo?q={keyword}&typeall=1&suball=1&page={page}"
                
                self.session.headers['Referer'] = 'https://s.weibo.com/'
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    print(f"   âŒ ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # å°è¯•å¤šç§é€‰æ‹©å™¨ï¼ˆå› ä¸ºå¾®åšç½‘é¡µç»“æ„ç»å¸¸å˜ï¼‰
                posts = soup.find_all('div', class_='mbrank')
                
                if not posts:
                    posts = soup.find_all('div', attrs={'class': lambda x: x and 'feed-item' in x})
                
                if not posts:
                    print(f"   âš ï¸  ç¬¬{page}é¡µæœªæ‰¾åˆ°å¸–å­ï¼Œå¯èƒ½éœ€è¦æ›´æ–°é€‰æ‹©å™¨")
                    continue
                
                count_this_page = 0
                for post in posts:
                    try:
                        # æå–æ–‡æœ¬
                        text_elem = post.find('p', class_='txt')
                        if not text_elem:
                            text_elem = post.find('p')
                        
                        if not text_elem:
                            continue
                        
                        text = text_elem.get_text(strip=True)
                        
                        # è¿‡æ»¤ï¼šå¤ªçŸ­çš„å†…å®¹
                        if len(text) < 20:
                            continue
                        
                        # è¿‡æ»¤ï¼šå¹¿å‘Šæˆ–æ— å…³å†…å®¹
                        spam_keywords = ['æ¨å¹¿', 'å¹¿å‘Š', 'è´­ä¹°', 'é“¾æ¥', 'æ‰«ç ']
                        if any(kw in text for kw in spam_keywords):
                            continue
                        
                        # æå–ç‚¹èµæ•°ï¼ˆå¯é€‰ï¼‰
                        like_elem = post.find('span', attrs={'class': lambda x: x and 'like' in x})
                        like_count = 0
                        if like_elem:
                            try:
                                like_count = int(like_elem.get_text())
                            except:
                                pass
                        
                        # ä¿å­˜
                        self.posts.append({
                            'platform': 'weibo',
                            'keyword': keyword,
                            'text': text[:500],  # æˆªæ–­åˆ°500å­—
                            'likes': like_count,
                            'collected_at': datetime.now().isoformat(),
                            'source_url': url
                        })
                        count_this_page += 1
                        
                    except Exception as e:
                        continue
                
                print(f"   âœ“ ç¬¬{page}é¡µï¼šé‡‡é›† {count_this_page} æ¡")
                
                # åçˆ¬è™«å»¶è¿Ÿ
                time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                print(f"   âŒ å‡ºé”™ï¼š{str(e)}")
                time.sleep(random.uniform(3, 7))
                continue
    
    def save_to_json(self, filename='weibo_raw_data.json'):
        """ä¿å­˜ä¸ºJSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.posts, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å·²ä¿å­˜ {len(self.posts)} æ¡æ•°æ®åˆ° {filename}")
        return filename
    
    def save_to_csv(self, filename='weibo_raw_data.csv'):
        """ä¿å­˜ä¸ºCSV"""
        if not self.posts:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['platform', 'keyword', 'text', 'likes', 'collected_at', 'source_url'])
            writer.writeheader()
            writer.writerows(self.posts)
        print(f"âœ… å·²ä¿å­˜ {len(self.posts)} æ¡æ•°æ®åˆ° {filename}")
        return filename
    
    def run(self):
        """æ‰§è¡Œé‡‡é›†æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹é‡‡é›†è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºï¼ˆå¾®åšç‰ˆï¼‰")
        print("=" * 60)
        
        for keyword in self.keywords:
            self.search_weibo(keyword, num_pages=3)
            print(f"   ç›®å‰å·²é‡‡é›†ï¼š{len(self.posts)} æ¡")
        
        # ä¿å­˜ç»“æœ
        if self.posts:
            self.save_to_json()
            self.save_to_csv()
            print("\n" + "=" * 60)
            print(f"ğŸ“Š é‡‡é›†å®Œæˆï¼šå…± {len(self.posts)} æ¡å¾®åš")
            print("=" * 60)
        else:
            print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®")
        
        return self.posts


if __name__ == "__main__":
    spider = WeiboSpider()
    posts = spider.run()
