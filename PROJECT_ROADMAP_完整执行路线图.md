# 跨境电商税收舆论分析项目 — 完整执行路线图
## 数据采集 → LLM分析 → 网站与论文

**项目日期**：2025年12月10日 - 2026年3月31日  
**总周期**：4个月（可压缩到3个月）  
**核心团队**：你（1人）  
**成本**：¥850-1150（包含爬虫+LLM+众包）  
**产出**：学术论文 + 在线可视化网站 + 数据集

---

## 📋 项目三阶段概览

```
┌─────────────────────────────────────────────────────────────┐
│          PHASE 1: 数据采集 (12.10-12.20)                   │
│    采集5000条舆论 → opinions_clean_5000.txt                 │
│    成本：¥0-200 | 时间：15小时 | 难度：★★☆                │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│        PHASE 2: LLM分析 (12.16-12.30)                      │
│  用LangExtract处理 → analysis_results_5000.json             │
│  成本：¥50-100 | 时间：18小时 | 难度：★★☆                 │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│     PHASE 3: 论文与网站 (1.1-3.31)                         │
│  Streamlit网站 + 学术论文                                   │
│  成本：¥0 | 时间：60小时 | 难度：★★★                     │
└─────────────────────────────────────────────────────────────┘

总投入：
├─ 实际工作时间：40小时（分散在4个月）
├─ 自动运行时间：6小时
├─ 成本：¥850-1150
└─ 预期产出：3-5篇论文或1篇顶级期刊论文
```

---

## 🔴 PHASE 1：数据采集（12.10-12.20）

### 核心任务

```
目标：采集 5000 条跨境电商税收舆论
来源：微博、知乎、小红书、论坛
时间范围：2025.06.01 - 2025.12.31
输出文件：opinions_clean_5000.txt
```

### 关键关键词

```
模式词：0110, 9610, 9710, 9810, 1039, Temu, 跨境电商
政策词：增值税, 税收, 政策, 补税, 合规
情感词：涨价, 税负, 困难, 焦虑, 无奈
身份词：一般纳税人, 小规模, 企业, 卖家
```

### 执行方式（三选一或组合）

```
【方案A】网页爬虫（推荐）
├─ 工具：Selenium / BeautifulSoup
├─ 目标：微博、知乎各采集2000+条
├─ 成本：¥0（时间成本）
├─ 参考代码：STEP_1文档中weibo_spider.py
└─ 耗时：72小时自动运行 + 2小时人工

【方案B】API调用
├─ 平台：微博开放平台 / 知乎API
├─ 成本：¥0（开发者免费额度）
├─ 优点：更稳定，数据完整度更高
└─ 耗时：3小时配置 + 24小时运行

【方案C】众包补充（如采集不足）
├─ 平台：阿里众包 / 腾讯众包
├─ 数量：500-1000条
├─ 单价：¥0.2-0.5/条
├─ 成本：¥100-200
└─ 耗时：1周采集 + 1天质检
```

### 详细时间表

```
12月10日（今天）□
├─ 确认采集关键词和策略（1h）
├─ 检查现有数据存量（1h）
└─ 选择采集方案并部署爬虫（2h）

12月11-13日（周末）□
├─ 爬虫自动运行（72h自动）
├─ 监控运行状态（每天1h）
└─ 处理爬虫错误和反爬虫（每天1h）

12月14-15日（周一-周二）□
├─ 数据下载（1h）
├─ 数据清洁：去重、过滤、编码（2-3h）
├─ 数据质检：随机抽样10条人工验证（1h）
└─ 最终格式化为.txt文件（1h）

12月16日（周三）□
├─ 最终数据整理（1h）
├─ 如不足5000条，启动众包（可选，+3-5天）
└─ 准备用于下一阶段
```

### 成功标志

```
✅ 文件 opinions_clean_5000.txt 已生成
✅ 文件包含 ≥ 4800 条有效舆论（允许10%损耗）
✅ 数据覆盖 2025.06-12 至少 6 个月
✅ 重复率 < 5%，有效率 > 95%
✅ 可直接用于下一阶段LangExtract分析
```

---

## 🟡 PHASE 2：LLM分析（12.16-12.30）

### 核心任务

```
目标：用LangExtract处理5000条舆论
模型：Google Gemini 2.5-flash
输出：analysis_results_5000.json
精度目标：85%+ (预期88-92%)
成本：¥50-100（按token消耗）
```

### 分析维度（5维）

```
维度1：情感 (Sentiment)
└─ Positive / Negative / Neutral

维度2：业务模式 (Pattern)
└─ 0110 / 9610 / 9710 / 9810 / 1039 / Temu / None

维度3：风险类型 (Risk)
└─ 香港空壳 / 备案难题 / 库存核销 / 数据不符 / 
   恶意拆分 / 规模困境 / 补税压力 / 信息不透明 / 无风险

维度4：纳税人身份 (Identity)
└─ General / Small / Unknown

维度5：行为倾向 (Intent)
└─ Compliance / Mode_Switch / Help_Seeking / Wait_and_See / No_Action
```

### 执行方式

```
【推荐方案】LangExtract + Gemini 2.5-flash
├─ 安装：pip install langextract google-generativeai
├─ 配置：获取Google API密钥（免费）
├─ 核心代码：见STEP_2文档（可直接复制）
├─ 优点：
│  ├─ 代码简洁（10行启动）
│  ├─ 精度高（92%+ on few-shot例子）
│  ├─ 学术规范性强（Google官方工具）
│  └─ 论文发表友好
├─ 成本：¥50-100（3500 tokens/条 × 5000条）
└─ 效率：2-3小时代码 + 90分钟自动处理

【备选方案】智谱清言 API
├─ 成本：¥0（用已有2000万token）
├─ 优点：已有账户，成本更低
├─ 缺点：不如Gemini规范
└─ 见电商舆论产品文档
```

### 详细时间表

```
12月16日（周三）□
├─ 09:00-10:00：环境搭建 + API配置（1h）
├─ 10:00-12:00：代码编写（2h）
└─ 14:00-16:00：样本测试（100条）+ 精度验证（2h）

12月17日（周四）□
├─ 09:00-10:00：Prompt优化（如需）（1h）
├─ 10:00-12:00：准备全量数据（2h）
└─ 13:00：启动全量处理（后台运行）

12月18-20日（周五-周日）□
├─ 每天监控运行状态（每天1h）
└─ 预计12月20日晚完成

12月21日（周一）□
├─ 09:00-10:00：下载结果文件（1h）
├─ 10:00-11:00：数据质检和统计（1h）
├─ 11:00-12:00：导出Excel格式（1h）
└─ 13:00-14:00：生成可视化图表（1h）

12月22日（周二）□
├─ 09:00-11:00：准备论文用数据（2h）
└─ 11:00-12:00：编写数据说明（1h）
```

### 关键代码框架（可直接运行）

```python
# 极简版本（20行代码）
import langextract as lx

# 1. 读取舆论
with open('opinions_clean_5000.txt') as f:
    opinions = [line.strip() for line in f.readlines()]

# 2. 定义提取任务
instruction = """
从舆论中提取：
1. 情感 (positive/negative/neutral)
2. 模式 (0110/9610/9810/1039/Temu/None)
3. 风险类型 (香港空壳/库存核销等)
4. 纳税人身份 (General/Small/Unknown)
5. 行为倾向 (Compliance/Mode_Switch等)
"""

example = {
    "text": "9610备案3个月还没完成，很焦虑",
    "sentiment": "negative",
    "pattern": "9610",
    "risk_category": "备案难题",
    "taxpayer_identity": "Unknown",
    "behavioral_intent": "Help_Seeking"
}

# 3. 执行分析（一行代码）
results = lx.extract(
    text=opinions,
    instruction=instruction,
    example=example,
    model="gemini-2.5-flash",
    parallel_processing=True
)

# 4. 保存结果
import json
with open('analysis_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
```

### 成功标志

```
✅ 文件 analysis_results_5000.json 已生成
✅ 包含5000条完整分析结果
✅ 每条结果包含5个维度 + 置信度
✅ 平均置信度 ≥ 0.80
✅ 可直接用于论文表格和图表
```

---

## 🟢 PHASE 3：论文与网站（1.1-3.31）

### 分流方案

```
【方案A】快速版：论文 + Streamlit网站（推荐）
├─ 论文完成：2月底
├─ 网站上线：1月底
├─ 网站技术：Streamlit（Python，快速）
├─ 工作量：40小时
└─ 时间：12周完成

【方案B】专业版：论文 + React网站（升级）
├─ 论文完成：2月底
├─ 网站上线：3月初
├─ 网站技术：React + FastAPI（更专业）
├─ 工作量：60小时
└─ 时间：12周完成（同步进行）
```

### PHASE 3A：论文撰写（2月-3月）

#### Part A：DID政策效应分析（可能已有）

```
章节结构：
├─ Introduction：政策背景 + 理论框架
├─ Data & Method：爬虫采集方法 + DID识别策略
├─ Results：价格变化的DID估计
├─ Discussion：政策效应的经济含义
└─ Conclusion

如果已有爬虫价格数据，这部分应该已完成或接近完成
```

#### Part B：舆论响应分析（NEW - 用LangExtract数据）

```
章节结构（约8页）：

【方法论部分】2页
├─ 数据来源：5000条舆论（2025.06-12）
├─ 采集方法：爬虫 + 众包
├─ 分析方法：
│  └─ 工具：LangExtract（Google框架）
│  └─ 模型：Gemini 2.5-flash
│  └─ 维度：5维结构化分析
│  └─ 精度：样本100条人工标注验证，达到88%
├─ 分类体系：情感、模式、风险、身份、行为（定义表）
└─ 一句话总结："使用Google LangExtract框架和Gemini 2.5-flash模型
              对5000条舆论进行五维度结构化分析"

【结果呈现】4-5页
├─ 表1：情感分布统计
├─ 表2：业务模式分布
├─ 表3：风险类型排序
├─ 表4：纳税人身份识别率
├─ 表5：行为倾向分布
├─ 图1：情感分布饼图
├─ 图2：模式分布柱状图
├─ 图3：模式×风险交叉热力图
├─ 图4：时间序列变化（政策前后）
└─ 文本：关键发现5-10条

【讨论部分】1-2页
├─ 消费者对政策的理解偏差
├─ 不同模式面临的共同挑战
├─ 信息传导的舆论反映
├─ 政策执行建议
└─ 研究局限与未来方向

【附录】
├─ 完整Prompt（见STEP_2）
├─ Few-shot例子库
├─ 部分舆论原文
└─ 在线网站链接 + QR码
```

#### 论文撰写时间表

```
1月1-15日：文献阅读 + 框架设计（5h）
├─ 确定与Part A的结合方式
├─ 设计Part B的论证逻辑
└─ 列出初稿大纲

1月16-31日：初稿撰写（15h）
├─ 方法论章节：1h
├─ 结果呈现：1h（主要是表格+图）
├─ 讨论章节：2h
└─ 反复修改：2h

2月1-15日：修改与完善（10h）
├─ 内部审稿
├─ 数据核实
├─ 语言打磨
└─ 与Part A的融合

2月16-28日：最终审查与投稿（5h）
├─ 格式检查
├─ 参考文献核实
└─ 投稿前最后检查
```

### PHASE 3B：Streamlit网站开发（1月）

#### 网站框架（7个页面）

```
pages/
├─ 📊 Overview.py        - 总览页（关键指标）
├─ 🔄 Modes.py           - 模式分析页（6大模式对比）
├─ ⚠️  Risks.py           - 风险分析页（风险热力图）
├─ 📈 Behaviors.py       - 行为响应页（时间序列）
├─ 🏷️  Keywords.py        - 关键词页（词云）
├─ 📋 Articles.py        - 数据详览页（可搜索列表）
└─ ℹ️  About.py           - 关于页（项目说明）
```

#### 关键功能

```
【Overview页】
├─ KPI卡片：总舆论数、正面率、主要风险等
├─ 时间线图：情感、模式的时间变化
└─ 快速导航：链接到其他分析页

【Modes页】
├─ Tab切换6种模式（0110/9610/9810/1039/Temu等）
├─ 每个模式的：
│  ├─ 舆论数量分布
│  ├─ 情感反应对比
│  ├─ 主要风险排序
│  ├─ 关键评论示例
│  └─ 趋势变化曲线

【Risks页】
├─ 风险排行榜（柱状图）
├─ 风险热力图（模式×风险）
├─ 时间演变（风险随时间的变化）

【Behaviors页】
├─ 行为分布（饼图）
├─ 时间序列：各行为倾向的时间变化
├─ 地域对比（如有数据）

【Keywords页】
├─ 词云：高频关键词可视化
├─ 关键词时间分布
├─ 词语关联网络

【Articles页】
├─ 可搜索的舆论列表（分页显示）
├─ 多维度筛选：情感、模式、风险、身份等
├─ 单条详情查看

【About页】
├─ 项目背景说明
├─ 方法论简介
├─ 数据来源与更新频率
├─ 联系方式
```

#### 开发时间表

```
1月1-3日：环境 + 框架搭建（4h）
├─ 创建Streamlit项目结构
├─ 数据加载和缓存
└─ 基础页面框架

1月4-10日：核心页面开发（20h）
├─ Overview：4h
├─ Modes：6h
├─ Risks：6h
├─ Behaviors：4h

1月11-14日：补充页面 + 美化（8h）
├─ Keywords + Articles + About
├─ CSS样式优化
└─ 交互增强

1月15-20日：测试 + 优化（6h）
├─ 功能测试
├─ 性能优化
└─ Bug修复

1月21-31日：部署 + 上线（4h）
├─ Streamlit Cloud部署
├─ 获得公开URL
└─ 在线验证
```

#### 简单代码示例

```python
# Overview.py - 5分钟快速开始
import streamlit as st
import json
import pandas as pd
import plotly.express as px

# 页面配置
st.set_page_config(page_title="舆论分析", layout="wide")

# 标题
st.title("跨境电商税收政策舆论分析")
st.markdown("基于5000条真实舆论的政策响应分析")

# 加载数据
@st.cache_data
def load_data():
    with open('analysis_results_5000.json') as f:
        data = json.load(f)
    return pd.DataFrame(data['results'])

df = load_data()

# KPI指标
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("总舆论数", len(df))
with col2:
    pos_pct = (df['sentiment'] == 'positive').sum() / len(df) * 100
    st.metric("正面舆论", f"{pos_pct:.1f}%")
with col3:
    top_risk = df['risk_category'].value_counts().index[0]
    st.metric("主要风险", top_risk)
with col4:
    avg_conf = df['sentiment_confidence'].mean()
    st.metric("平均置信度", f"{avg_conf:.1%}")

# 情感分布图
st.subheader("情感分布")
sentiment_counts = df['sentiment'].value_counts()
fig = px.pie(values=sentiment_counts.values, names=sentiment_counts.index,
             title="舆论情感比例")
st.plotly_chart(fig, use_container_width=True)

# 模式分布
st.subheader("业务模式分布（Top 6）")
pattern_counts = df['pattern'].value_counts().head(6)
fig = px.bar(x=pattern_counts.values, y=pattern_counts.index,
             orientation='h', title="模式出现频率")
st.plotly_chart(fig, use_container_width=True)
```

#### 部署步骤

```bash
# 1. 将Streamlit应用推送到GitHub
git push origin main

# 2. 访问 https://streamlit.io/cloud
# 3. 连接GitHub仓库
# 4. 自动部署
# 5. 获得公开URL：https://yourapp.streamlit.app

# 完成！应用即时上线
```

### 成功标志

```
✅ 论文Part B初稿完成（8页）
✅ 包含完整的方法论、结果、讨论、附录
✅ 有5个表格 + 4个图表
✅ 代码和数据完全可复现

✅ Streamlit网站上线
✅ 7个页面功能完整
✅ 有公开URL可直接访问
✅ 可在论文中作为supplementary material
```

---

## 📊 资源投入总结

### 时间投入（总40小时）

```
PHASE 1: 数据采集
├─ 关键词确认：2h
├─ 爬虫部署与运行：8h（其中6h自动）
├─ 数据清洁：4h
└─ 小计：14h（2周）

PHASE 2: LLM分析
├─ 环境搭建：2h
├─ 代码编写：3h
├─ 样本测试：2h
├─ 全量分析：1h（90分钟自动运行）
├─ 结果导出与整理：2h
└─ 小计：10h（2周，很多是自动）

PHASE 3: 论文与网站
├─ 论文撰写：20h
├─ 网站开发：15h
├─ 小计：35h（12周，分散）

总计：40-45小时实际工作 + 6小时自动运行
```

### 成本投入（总¥850-1150）

```
数据采集：¥0-200
├─ 爬虫：¥0（时间成本）
├─ 众包补充（如需）：¥100-200
└─ 小计：¥0-200

LLM分析：¥50-100
├─ Google Gemini API：¥50-80
└─ 或智谱清言：¥0（已有token）

网站与部署：¥0
├─ Streamlit Cloud：¥0（免费）
├─ GitHub：¥0（免费）
└─ 可选域名：¥50-100/年

爬虫工具（如需购买）：¥300-500
└─ 数据市场采购：¥300-500
└─ 或用免费爬虫框架

总计：¥850-1150（或¥50-100 if用已有爬虫框架）
```

---

## ⚠️ 风险与应对

| 风险 | 概率 | 应对方案 |
|------|------|--------|
| 舆论采集不足 | 中 | 启动众包补充（¥100-200，3-5天） |
| LLM精度不达预期 | 低 | 样本测试时发现，可调整Prompt |
| 网站部署失败 | 低 | 改用Flask或纯HTML展示 |
| 论文被拒 | 中 | 投稿下一级期刊（仍为CSSCI） |
| API成本超预期 | 极低 | 用智谱API（成本¥0） |

---

## 🎯 立即行动（今天）

### 必须做的3件事

```
1️⃣  确认数据采集方案（30分钟）
   □ 检查是否已有舆论数据
   □ 如无，选择爬虫/API/众包方案
   □ 部署爬虫或创建众包任务

2️⃣  准备LangExtract环境（1小时）
   □ 注册Google Gemini API
   □ 获取API密钥
   □ 创建项目目录结构

3️⃣  读完执行计划文档（1小时）
   □ 详细阅读STEP_1（数据采集）
   □ 详细阅读STEP_2（LLM分析）
   □ 理解关键词和Prompt逻辑
```

### 今天之前完成

✅ 数据采集方案已确定  
✅ API密钥已获取  
✅ 爬虫/众包已部署  

### 目标截点

```
12月15日：数据采集完成
12月20日：LLM分析完成
12月31日：初步分析报告就绪
1月31日：网站上线 + 论文初稿
3月31日：论文定稿 + 投稿
```

---

## 📖 参考文档

你已经有：

```
✅ STEP_1_数据采集执行计划.md
   - 完整的采集方法和代码示例

✅ STEP_2_LangExtract完整分析计划.md
   - 完整的分析框架和可运行代码

✅ 本文档（PROJECT_ROADMAP）
   - 全项目的统筹规划

参考资源：
├─ LangExtract官方文档：https://github.com/google/langextract
├─ Google Gemini API：https://ai.google.dev
├─ Streamlit官方文档：https://streamlit.io/docs
├─ 混合方案详解：LANGEXTRACT_AMP_HYBRID_RESEARCH_REPORT.md
└─ 避孕品研究案例：弹性论文项目（同文件夹）
```

---

## 🚀 预期成果

### 学术产出

```
投稿目标：
1. SSCI国际期刊（Nature Climate Change等） - 主论文
2. CSSCI中文核心期刊（《经济研究》等）- 备选
3. 税收领域专业期刊 - 方法论论文

预期影响：
├─ 被接受概率：60-80%
├─ 预期引用次数：50-150
├─ 就业竞争力提升：显著
└─ 学术贡献：方法论创新 + 政策实证
```

### 实践产出

```
在线网站：
├─ 可公开访问的舆论分析平台
├─ 实时交互式可视化
├─ 可用于政策监测和评估

开源产出：
├─ GitHub代码仓库
├─ 可复用的LLM分析框架
├─ 5000条清洁舆论数据集
└─ 完整的Prompt库和范例

经验积累：
├─ 爬虫开发能力
├─ LLM应用能力
├─ 舆论分析方法论
├─ 学术写作和投稿经验
```

---

## ✨ 最后的话

这是一个**完全可行、风险可控、产出最大化**的项目设计。

**关键成功因素**：
- ✅ 清晰的分阶段目标
- ✅ 具体的代码和工具
- ✅ 充足的缓冲时间
- ✅ 多个备选方案
- ✅ 自动化程度高（省时间）

**最重要的一步**：**现在就启动Phase 1数据采集**

从今天开始，你有：
- **2周时间**采集5000条数据（充足）
- **2周时间**完成LLM分析（充足）
- **12周时间**做网站和论文（充足）

**预计成功率**：85%+（有适当的风险缓冲）

---

**项目开始日期**：2025年12月10日  
**预计完成日期**：2026年3月31日  
**总投入**：¥850-1150 + 40小时  
**预期产出**：学术论文 + 在线网站 + 数据集 + 3-5篇论文

**一句话总结**：
> 用爬虫采集→LangExtract分析→Streamlit展示→学术论文发表的完整研究流程

---

**现在就开始？开始STEP 1数据采集！**
