"""
BERTopicä¸»é¢˜åˆ†æå·¥å…· - æ·±åº¦è¯é¢˜å»ºæ¨¡
"""

from __future__ import annotations
import streamlit as st
import pandas as pd
import numpy as np
from typing import Optional, List, Any
import warnings
warnings.filterwarnings('ignore')

try:
    from bertopic import BERTopic
    from sentence_transformers import SentenceTransformer
    BERTOPIC_AVAILABLE = True
except ImportError:
    BERTOPIC_AVAILABLE = False


@st.cache_resource
def get_bertopic_model() -> Optional[Any]:
    """è·å–ç¼“å­˜çš„BERTopicæ¨¡å‹ï¼ˆä»…åˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
    if not BERTOPIC_AVAILABLE:
        return None
    
    try:
        # ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„embeddingæ¨¡å‹
        embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
        model = BERTopic(
            embedding_model=embedding_model,
            language="chinese",
            calculate_probabilities=True,
            verbose=False
        )
        return model
    except Exception as e:
        st.warning(f"BERTopicåˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def train_bertopic(texts: List[str], model: Optional[Any] = None) -> tuple:
    """
    è®­ç»ƒBERTopicæ¨¡å‹æå–éšè—ä¸»é¢˜
    
    è¿”å›: (topics, probabilities, model)
    """
    if not BERTOPIC_AVAILABLE:
        st.warning("BERTopicæœªå®‰è£…ï¼Œè·³è¿‡é«˜çº§ä¸»é¢˜åˆ†æ")
        return None, None, None
    
    if model is None:
        model = get_bertopic_model()
    
    if model is None:
        return None, None, None
    
    try:
        with st.spinner("ğŸ¤– æ­£åœ¨è®­ç»ƒBERTopicæ¨¡å‹ï¼Œæå–éšè—ä¸»é¢˜..."):
            topics, probs = model.fit_transform(texts)
        return topics, probs, model
    except Exception as e:
        st.warning(f"ä¸»é¢˜æå–å¤±è´¥: {e}")
        return None, None, None


def visualize_topics_2d(model: Optional[Any], topics: Optional[np.ndarray]) -> Optional[object]:
    """ç”Ÿæˆ2Dä¸»é¢˜å¯è§†åŒ–ï¼ˆäº¤äº’å¼å›¾è¡¨ï¼‰"""
    if model is None or topics is None:
        return None
    
    try:
        return model.visualize_topics()
    except Exception as e:
        st.warning(f"ä¸»é¢˜å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_hierarchy(model: Optional[Any]) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜å±‚çº§å…³ç³»å›¾"""
    if model is None:
        return None
    
    try:
        # å°è¯•ç”Ÿæˆå±‚çº§å…³ç³»
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            return model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
        else:
            st.info("ğŸ’¡ ä¸»é¢˜æ•°é‡å¤ªå°‘ï¼Œæ— æ³•ç”Ÿæˆå±‚çº§å…³ç³»å›¾")
            return None
    except Exception as e:
        st.warning(f"å±‚çº§å…³ç³»å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_similarity(model: Optional[Any]) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜ç›¸ä¼¼åº¦çƒ­åŠ›å›¾"""
    if model is None:
        return None
    
    try:
        return model.visualize_heatmap(n_clusters=5)
    except Exception as e:
        st.warning(f"ç›¸ä¼¼åº¦çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_terms(model: Optional[Any], top_n: int = 5) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜è¯è¯­çš„é‡è¦æ€§å›¾è¡¨"""
    if model is None:
        return None
    
    try:
        return model.visualize_terms(top_n_terms=top_n)
    except Exception as e:
        st.warning(f"è¯è¯­é‡è¦æ€§å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        return None


def get_topic_keywords(model: Optional[Any], topic: int, top_n: int = 5) -> List[tuple]:
    """è·å–æŒ‡å®šä¸»é¢˜çš„å…³é”®è¯"""
    if model is None:
        return []
    
    try:
        topic_info = model.get_topic(topic)
        return topic_info[:top_n] if topic_info else []
    except Exception as e:
        return []


def get_topics_summary(model: Optional[Any]) -> pd.DataFrame:
    """è·å–æ‰€æœ‰ä¸»é¢˜çš„æ‘˜è¦ä¿¡æ¯"""
    if model is None:
        return pd.DataFrame()
    
    try:
        topic_info = model.get_topic_info()
        return topic_info[['Topic', 'Count', 'Name']].copy()
    except Exception as e:
        return pd.DataFrame()


def get_documents_by_topic(df: pd.DataFrame, topics: np.ndarray, topic_id: int, top_n: int = 5) -> pd.DataFrame:
    """è·å–æŒ‡å®šä¸»é¢˜ä¸‹çš„æ–‡æ¡£åˆ—è¡¨"""
    if topics is None:
        return pd.DataFrame()
    
    try:
        mask = topics == topic_id
        topic_docs = df[mask].head(top_n)[['source_text', 'sentiment', 'risk_level']].copy()
        return topic_docs
    except Exception as e:
        return pd.DataFrame()


def visualize_documents_2d(model: Optional[Any], docs: List[str], topics: np.ndarray) -> Optional[object]:
    """ç”Ÿæˆæ–‡æ¡£åœ¨2Dç©ºé—´çš„åˆ†å¸ƒï¼ˆUmapé™ç»´ï¼‰"""
    if model is None or topics is None:
        return None
    
    try:
        return model.visualize_documents(docs, topics=topics, hide_document_hover=False)
    except Exception as e:
        try:
            # å¦‚æœæœ‰embeddingå°±ç”¨ï¼Œæ²¡æœ‰å°±ç®€åŒ–ç‰ˆæœ¬
            return model.visualize_documents(docs, hide_document_hover=True)
        except:
            return None


def visualize_term_distribution(model: Optional[Any], top_n_topics: int = 5) -> Optional[object]:
    """ç”Ÿæˆå„ä¸»é¢˜çš„è¯é¢‘åˆ†å¸ƒ"""
    if model is None:
        return None
    
    try:
        return model.visualize_barchart(top_n_topics=top_n_topics)
    except Exception as e:
        return None


def generate_topic_tree(model: Optional[Any], df: pd.DataFrame, topics: np.ndarray) -> str:
    """ç”Ÿæˆä¸»é¢˜çš„æ ‘å½¢ç»“æ„æ–‡æœ¬"""
    if model is None or topics is None:
        return ""
    
    try:
        topic_info = model.get_topic_info()
        tree_text = ""
        
        for idx, row in topic_info[topic_info['Topic'] != -1].iterrows():
            topic_id = row['Topic']
            topic_name = row['Name']
            count = row['Count']
            
            # è·å–è¯¥ä¸»é¢˜çš„å‰3ä¸ªæ–‡æ¡£
            mask = topics == topic_id
            docs = df[mask].head(3)
            
            tree_text += f"**è¯é¢˜{int(topic_id)}: {topic_name}** ({count} æ¡æ–‡æ¡£)\n"
            
            for i, (_, doc) in enumerate(docs.iterrows(), 1):
                text_preview = doc['source_text'][:60] + "..." if len(doc['source_text']) > 60 else doc['source_text']
                tree_text += f"  â”œâ”€ {i}. \"{text_preview}\"\n"
                tree_text += f"     æƒ…æ„Ÿ: {doc['sentiment']} | é£é™©: {doc['risk_level']}\n"
            
            tree_text += "\n"
        
        return tree_text
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {e}"


def visualize_term_score_decline(model: Optional[Any], top_n_topics: int = 5) -> Optional[object]:
    """ç”Ÿæˆc-TF-IDFåˆ†æ•°è¡°å‡å›¾ï¼ˆæ˜¾ç¤ºè¯æ±‡æƒé‡çš„é€’å‡ï¼‰"""
    if model is None:
        return None
    
    try:
        return model.visualize_term_rank(top_n_topics=top_n_topics, log_scale=False)
    except Exception as e:
        return None


def get_hierarchical_topics(model: Optional[Any]) -> Optional[pd.DataFrame]:
    """è®¡ç®—å¹¶è¿”å›å±‚çº§ä¸»é¢˜ç»“æ„"""
    if model is None:
        return None
    
    try:
        # éœ€è¦æœ‰è¶³å¤Ÿçš„ä¸»é¢˜æ‰èƒ½ç”Ÿæˆå±‚çº§
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            return hierarchical_topics
        else:
            return None
    except Exception as e:
        return None


def visualize_hierarchical_documents(model: Optional[Any], texts: List[str], topics: np.ndarray) -> Optional[object]:
    """ç”Ÿæˆåˆ†å±‚æ–‡æ¡£å¯è§†åŒ–ï¼ˆåœ¨å±‚çº§æ ‘çš„2Dç©ºé—´ä¸­ï¼‰"""
    if model is None or topics is None or len(texts) == 0:
        return None
    
    try:
        # è·å–åˆ†å±‚ä¸»é¢˜
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            # å°è¯•å¯è§†åŒ–åˆ†å±‚æ–‡æ¡£
            return model.visualize_hierarchical_documents(texts, hierarchical_topics=hierarchical_topics, hide_document_hover=True)
        else:
            return None
    except Exception as e:
        return None


def get_topic_keywords_detailed(model: Optional[Any], topic_id: int, top_n: int = 10) -> pd.DataFrame:
    """è·å–æŒ‡å®šä¸»é¢˜çš„å…³é”®è¯åŠå…¶c-TF-IDFåˆ†æ•°"""
    if model is None:
        return pd.DataFrame()
    
    try:
        topic_info = model.get_topic(topic_id)
        if topic_info:
            # topic_info æ˜¯ [(word, score), ...] çš„åˆ—è¡¨
            keywords_df = pd.DataFrame(topic_info[:top_n], columns=['å…³é”®è¯', 'c-TF-IDFåˆ†æ•°'])
            keywords_df['æ’å'] = range(1, len(keywords_df) + 1)
            return keywords_df[['æ’å', 'å…³é”®è¯', 'c-TF-IDFåˆ†æ•°']]
        return pd.DataFrame()
    except Exception as e:
        return pd.DataFrame()


def get_topic_text_representation(model: Optional[Any], topic_id: int) -> str:
    """è·å–ä¸»é¢˜çš„æ–‡æœ¬è¡¨ç¤ºï¼ˆç”±ç”Ÿæˆæ¨¡å‹ç”Ÿæˆï¼‰"""
    if model is None:
        return ""
    
    try:
        # è·å–ä¸»é¢˜çš„æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        topic_info = model.get_topic_info()
        if topic_info is not None and len(topic_info) > 0:
            topic_row = topic_info[topic_info['Topic'] == topic_id]
            if not topic_row.empty:
                return topic_row.iloc[0]['Name']
        return f"è¯é¢˜{topic_id}"
    except Exception as e:
        return f"è¯é¢˜{topic_id}"


def calculate_topic_distribution(model: Optional[Any], texts: List[str]) -> Optional[np.ndarray]:
    """è®¡ç®—æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒæ¦‚ç‡çŸ©é˜µ"""
    if model is None or len(texts) == 0:
        return None
    
    try:
        # å¦‚æœä½¿ç”¨äº†calculate_probabilities=Trueï¼Œå¯ä»¥ç›´æ¥è·å–
        if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
            return model.probabilities_
        else:
            # å¦åˆ™å°è¯•ä¼°è®¡ä¸»é¢˜åˆ†å¸ƒ
            return model.approximate_distribution(texts)
    except Exception as e:
        return None


def visualize_topic_per_class(model: Optional[Any], df: pd.DataFrame, class_column: str = 'sentiment') -> Optional[object]:
    """æŒ‰åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿç±»åˆ«ï¼‰ç”Ÿæˆä¸»é¢˜åˆ†å¸ƒå¯è§†åŒ–"""
    if model is None or df is None or class_column not in df.columns:
        return None
    
    try:
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ç±»åˆ«
        classes = df[class_column].unique()
        
        # åˆ›å»ºç®€å•çš„æŸ±çŠ¶å›¾å¯¹æ¯”
        import plotly.graph_objects as go
        
        fig = go.Figure()
        topic_info = model.get_topic_info()
        
        for class_val in classes:
            class_mask = df[class_column] == class_val
            topic_counts = []
            
            for topic_id in topic_info['Topic']:
                if topic_id == -1:  # è·³è¿‡å™ªå£°
                    continue
                count = len(df[class_mask & (df.index.isin([i for i, t in enumerate(model.topics_) if t == topic_id]))])
                topic_counts.append(count)
            
            fig.add_trace(go.Bar(
                name=str(class_val),
                x=topic_info[topic_info['Topic'] != -1]['Topic'].astype(str),
                y=topic_counts,
                text=topic_counts,
                textposition='auto',
            ))
        
        fig.update_layout(
            title="æŒ‰åˆ†ç±»ç»Ÿè®¡çš„ä¸»é¢˜åˆ†å¸ƒ",
            xaxis_title="ä¸»é¢˜ID",
            yaxis_title="æ–‡æ¡£æ•°é‡",
            barmode='group',
            height=400,
            hovermode='x unified'
        )
        
        return fig
    except Exception as e:
        return None


# ============================================================================
# Phase 4 æ–°å¢å‡½æ•° - F101, F102, F103
# ============================================================================

def visualize_distribution(model: Optional[Any], topic_id: int, probabilities: Optional[np.ndarray] = None, 
                          min_probability: float = 0.015) -> Optional[object]:
    """
    F101: å•æ–‡æ¡£ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–
    
    æ˜¾ç¤ºæŸæ¡æ–‡æ¡£å±äºå„ä¸»é¢˜çš„ç½®ä¿¡åº¦ç™¾åˆ†æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    
    å‚æ•°:
        model: BERTopicæ¨¡å‹
        topic_id: æ–‡æ¡£åœ¨æ¨¡å‹ä¸­çš„index
        probabilities: å•æ¡æ–‡æ¡£çš„ä¸»é¢˜æ¦‚ç‡æ•°ç»„ (shape: [n_topics])
        min_probability: æœ€å°æ˜¾ç¤ºæ¦‚ç‡é˜ˆå€¼
    
    è¿”å›:
        Plotlyäº¤äº’å¼æŸ±çŠ¶å›¾
    """
    if model is None:
        return None
    
    try:
        # å¦‚æœæ²¡æä¾›probabilitiesï¼Œå°è¯•ä»æ¨¡å‹è·å–
        if probabilities is None:
            if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
                if topic_id < len(model.probabilities_):
                    probabilities = model.probabilities_[topic_id]
                else:
                    st.warning(f"æ–‡æ¡£ç´¢å¼•{topic_id}è¶…å‡ºèŒƒå›´")
                    return None
            else:
                st.warning("æ¨¡å‹æœªè®¡ç®—æ¦‚ç‡ï¼Œè¯·åœ¨BERTopicåˆå§‹åŒ–æ—¶è®¾ç½®calculate_probabilities=True")
                return None
        
        # è·å–ä¸»é¢˜ä¿¡æ¯
        topic_info = model.get_topic_info()
        if topic_info is None or len(topic_info) == 0:
            return None
        
        # è¿‡æ»¤ä½æ¦‚ç‡ä¸»é¢˜
        valid_indices = probabilities >= min_probability
        filtered_probs = probabilities[valid_indices]
        
        if len(filtered_probs) == 0:
            st.info("ğŸ’¡ è¯¥æ–‡æ¡£çš„ä¸»é¢˜æ¦‚ç‡éƒ½å¾ˆä½ï¼Œå¯èƒ½æ˜¯å™ªå£°æ–‡æ¡£")
            return None
        
        # è·å–å¯¹åº”çš„ä¸»é¢˜æ ‡ç­¾
        valid_topics = np.where(valid_indices)[0]
        topic_labels = []
        for idx in valid_topics:
            matching = topic_info[topic_info['Topic'] == idx]
            if not matching.empty:
                topic_labels.append(f"è¯é¢˜{int(idx)}: {matching.iloc[0]['Name'][:15]}")
            else:
                topic_labels.append(f"è¯é¢˜{int(idx)}")
        
        # åˆ›å»ºå›¾è¡¨
        import plotly.graph_objects as go
        fig = go.Figure(data=[go.Bar(
            x=topic_labels,
            y=filtered_probs,
            marker=dict(color=filtered_probs, colorscale='Viridis', showscale=True),
            text=[f'{p:.2%}' for p in filtered_probs],
            textposition='outside'
        )])
        
        fig.update_layout(
            title=f"æ–‡æ¡£{topic_id}çš„ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒ",
            xaxis_title="ä¸»é¢˜",
            yaxis_title="æ¦‚ç‡",
            height=400,
            hovermode='x unified'
        )
        
        return fig
    
    except Exception as e:
        st.warning(f"æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_approximate_distribution(model: Optional[Any], texts: List[str], 
                                       doc_index: int = 0, calculate_tokens: bool = True) -> Optional[object]:
    """
    F102: Tokençº§åˆ«ä¸»é¢˜åˆ†å¸ƒåˆ†æ
    
    ç²¾ç¡®åˆ°å•è¯çº§åˆ«ï¼Œæ˜¾ç¤ºå“ªäº›è¯è§¦å‘äº†å“ªä¸ªä¸»é¢˜
    
    å‚æ•°:
        model: BERTopicæ¨¡å‹
        texts: æ‰€æœ‰æ–‡æœ¬åˆ—è¡¨
        doc_index: è¦åˆ†æçš„æ–‡æ¡£ç´¢å¼•
        calculate_tokens: æ˜¯å¦è®¡ç®—tokençº§åˆ«çš„åˆ†å¸ƒ
    
    è¿”å›:
        åŒ…å«tokençº§åˆ†å¸ƒçš„DataFrameæˆ–å¯è§†åŒ–
    """
    if model is None or doc_index >= len(texts):
        return None
    
    try:
        # è·å–è¿‘ä¼¼åˆ†å¸ƒï¼ˆapproximate_distributionï¼‰
        topic_distr, topic_token_distr = model.approximate_distribution(
            [texts[doc_index]],
            calculate_tokens=calculate_tokens
        )
        
        if topic_distr is None:
            return None
        
        # è½¬æ¢ä¸ºDataFrameæ˜¾ç¤º
        topic_info = model.get_topic_info()
        
        # ä¸»é¢˜çº§åˆ†å¸ƒ
        result_data = []
        for topic_id, prob in enumerate(topic_distr[0]):
            if prob > 0.01:  # åªæ˜¾ç¤ºæ¦‚ç‡>1%çš„ä¸»é¢˜
                matching = topic_info[topic_info['Topic'] == topic_id]
                topic_name = matching.iloc[0]['Name'] if not matching.empty else f"è¯é¢˜{topic_id}"
                result_data.append({
                    'ä¸»é¢˜': f"{topic_id}: {topic_name}",
                    'æ¦‚ç‡': f"{prob:.2%}",
                    'æ˜¯å¦ä¸ºç›®æ ‡ä¸»é¢˜': prob > 0.3
                })
        
        result_df = pd.DataFrame(result_data)
        
        # Tokençº§åˆ†å¸ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if calculate_tokens and topic_token_distr is not None and len(topic_token_distr) > 0:
            # åˆ›å»ºtokenæ ‡è®°
            text = texts[doc_index]
            words = text.split()
            
            token_info = []
            for word_idx, word in enumerate(words):
                if word_idx < len(topic_token_distr[0]):
                    top_topic = np.argmax(topic_token_distr[0][word_idx])
                    prob = topic_token_distr[0][word_idx][top_topic]
                    matching = topic_info[topic_info['Topic'] == top_topic]
                    topic_name = matching.iloc[0]['Name'] if not matching.empty else f"è¯é¢˜{top_topic}"
                    
                    token_info.append({
                        'è¯': word,
                        'ä¸»é¢˜': f"{top_topic}: {topic_name}",
                        'ç½®ä¿¡åº¦': f"{prob:.2%}"
                    })
            
            token_df = pd.DataFrame(token_info)
            return {'ä¸»é¢˜åˆ†å¸ƒ': result_df, 'è¯çº§åˆ†å¸ƒ': token_df}
        
        return {'ä¸»é¢˜åˆ†å¸ƒ': result_df}
    
    except Exception as e:
        st.warning(f"è¿‘ä¼¼åˆ†å¸ƒè®¡ç®—å¤±è´¥: {e}")
        return None


def reduce_outliers(model: Optional[Any], topics: np.ndarray, 
                   strategy: str = "probabilities", threshold: float = 0.1) -> tuple:
    """
    F103: ç¦»ç¾¤å€¼è‡ªåŠ¨é‡åˆ†ç±»
    
    å°†noiseæ–‡æ¡£(æ ‡ç­¾-1)é‡æ–°åˆ†é…åˆ°æœ‰æ•ˆä¸»é¢˜
    
    å‚æ•°:
        model: BERTopicæ¨¡å‹
        topics: åŸå§‹ä¸»é¢˜æ•°ç»„
        strategy: é‡åˆ†ç±»ç­–ç•¥
            - "probabilities": åŸºäºHDBSCANè½¯èšç±»æ¦‚ç‡
            - "distributions": åŸºäºè¿‘ä¼¼ä¸»é¢˜åˆ†å¸ƒ
            - "c-tf-idf": åŸºäºè¯é¢‘ç›¸ä¼¼åº¦ï¼ˆæœ€å¿«ï¼‰
            - "embeddings": åŸºäºè¯­ä¹‰embeddingç›¸ä¼¼åº¦ï¼ˆæœ€å‡†ï¼‰
        threshold: åˆ†é…ç½®ä¿¡åº¦é˜ˆå€¼ (0.05-0.3)
    
    è¿”å›:
        (æ–°çš„topicsæ•°ç»„, ç»Ÿè®¡æŠ¥å‘Šdict)
    """
    if model is None or topics is None:
        return topics, {}
    
    try:
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        noise_mask = topics == -1
        noise_count_before = np.sum(noise_mask)
        
        if noise_count_before == 0:
            return topics, {'message': 'æ²¡æœ‰ç¦»ç¾¤å€¼éœ€è¦å¤„ç†'}
        
        # è°ƒç”¨BERTopicçš„reduce_outliersæ–¹æ³•
        new_topics = model.reduce_outliers(
            topics,
            strategy=strategy,
            threshold=threshold
        )
        
        # è®¡ç®—æ”¹è¿›æ•ˆæœ
        noise_count_after = np.sum(new_topics == -1)
        reclassified_count = noise_count_before - noise_count_after
        reclassified_pct = reclassified_count / noise_count_before * 100 if noise_count_before > 0 else 0
        
        report = {
            'strategy': strategy,
            'threshold': threshold,
            'é‡åˆ†ç±»å‰å™ªå£°æ•°': int(noise_count_before),
            'é‡åˆ†ç±»åå™ªå£°æ•°': int(noise_count_after),
            'é‡æ–°åˆ†é…æ•°': int(reclassified_count),
            'æ”¹è¿›ç‡': f"{reclassified_pct:.1f}%",
            'çŠ¶æ€': 'æˆåŠŸ' if reclassified_count > 0 else 'æ— æ”¹è¿›'
        }
        
        return new_topics, report
    
    except Exception as e:
        st.warning(f"ç¦»ç¾¤å€¼å¤„ç†å¤±è´¥: {e}")
        return topics, {'error': str(e)}
