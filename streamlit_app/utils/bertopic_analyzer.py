"""
BERTopicä¸»é¢˜åˆ†æå·¥å…· - æ·±åº¦è¯é¢˜å»ºæ¨¡
"""

from __future__ import annotations
import streamlit as st
import pandas as pd
import numpy as np
from typing import Optional, List, Any
import warnings
warnings.filterwarnings('ignore')

try:
    from bertopic import BERTopic
    from sentence_transformers import SentenceTransformer
    from umap import UMAP
    from hdbscan import HDBSCAN
    BERTOPIC_AVAILABLE = True
except ImportError as e:
    print(f"DEBUG: Import failed: {e}")
    import traceback
    traceback.print_exc()
    BERTOPIC_AVAILABLE = False


def get_bertopic_model() -> Optional[Any]:
    """è·å–BERTopicæ¨¡å‹"""
    import os
    from pathlib import Path
    
    print("DEBUG: get_bertopic_model() called")
    if not BERTOPIC_AVAILABLE:
        print("DEBUG: BERTOPIC_AVAILABLE is False")
        return None
    
    try:
        # ä½¿ç”¨è½»é‡çº§è‹±æ–‡æ¨¡å‹ï¼ˆæ— éœ€HuggingFaceç½‘ç»œè¿æ¥ï¼Œå·²å†…ç½®ï¼‰
        # all-MiniLM-L6-v2: 22MBï¼Œè¶…è½»ï¼Œå·²åœ¨sentence-transformersä¸­é¢„è½½
        print("DEBUG: å°è¯•åŠ è½½è½»é‡çº§embeddingæ¨¡å‹...")
        try:
            embedding_model = SentenceTransformer(
                'all-MiniLM-L6-v2',
                device='cpu'  # å¼ºåˆ¶CPUæ¨¡å¼ï¼Œé¿å…GPUé—®é¢˜
            )
            print("DEBUG: all-MiniLM-L6-v2åŠ è½½æˆåŠŸ")
        except Exception as e1:
            print(f"DEBUG: all-MiniLM-L6-v2åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ¨¡å‹: {e1}")
            try:
                # å¤‡é€‰ï¼šæè½»çš„å¤šè¯­è¨€æ¨¡å‹
                embedding_model = SentenceTransformer(
                    'distiluse-base-multilingual-cased-v2',
                    device='cpu'
                )
                print("DEBUG: å¤šè¯­è¨€æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e2:
                print(f"DEBUG: æ‰€æœ‰ç½‘ç»œæ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
                print("WARNING: å°†ä½¿ç”¨TF-IDFå‘é‡ä½œä¸ºembeddingçš„å¤‡é€‰æ–¹æ¡ˆ")
                # é™çº§ï¼šä½¿ç”¨æœ¬åœ°TF-IDFå‘é‡
                embedding_model = None
        
        # ä¼˜åŒ–HDBSCANèšç±»å‚æ•°ï¼ˆé˜²æ­¢è¯é¢˜é‡å¤ï¼‰
        umap_model = UMAP(
            n_neighbors=20,           # â† å¢åŠ åˆ°20ï¼ˆä¿ç•™æ›´å¤šå…¨å±€ç»“æ„ï¼‰
            n_components=5,
            min_dist=0.1,             # â† å¢åŠ åˆ°0.1ï¼ˆé¿å…è¿‡åº¦èšé›†ï¼‰
            metric='cosine',
            random_state=42
        )
        
        hdbscan_model = HDBSCAN(
            min_cluster_size=30,      # â† å¤§å¹…æé«˜åˆ°30ï¼ˆé˜²æ­¢å°è¯é¢˜è¢«åˆ†ç¦»ï¼‰
            min_samples=10,           # â† æé«˜åˆ°10ï¼ˆå¯†åº¦è¦æ±‚æ›´ä¸¥æ ¼ï¼‰
            cluster_selection_epsilon=0.5,  # â† æ·»åŠ ï¼šè¿›ä¸€æ­¥åˆå¹¶ç›¸ä¼¼ç°‡
            prediction_data=True      # â† æ”¯æŒæ–°æ–‡æ¡£é¢„æµ‹
        )
        
        # å¦‚æœembeddingæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨TF-IDFå‘é‡
        if embedding_model is None:
            from sklearn.feature_extraction.text import TfidfVectorizer
            print("DEBUG: é™çº§ä½¿ç”¨TF-IDFå‘é‡æ–¹æ¡ˆ")
            # ä¸è®¾embedding_modelï¼ŒBERTopicä¼šè‡ªåŠ¨ä½¿ç”¨TF-IDF
        
        model = BERTopic(
            embedding_model=embedding_model,  # å¯ä»¥ä¸ºNone
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            language="chinese",
            calculate_probabilities=True,
            verbose=False,
            top_n_words=10,
            nr_topics="auto"          # â† è‡ªåŠ¨ä¼˜åŒ–ä¸»é¢˜æ•°
        )
        return model
    except Exception as e:
        print(f"DEBUG: BERTopicåˆå§‹åŒ–å¤±è´¥è¯¦ç»†é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        st.warning(f"BERTopicåˆå§‹åŒ–å¤±è´¥: {e}")
        return None


@st.cache_resource
def train_bertopic_cached(texts_tuple: tuple) -> tuple:
     """
     ç¼“å­˜ç‰ˆBERTopicè®­ç»ƒï¼ˆåªè®­ç»ƒä¸€æ¬¡ï¼Œç»“æœä¿å­˜ï¼‰
     
     å‚æ•°ï¼štexts_tuple - æ–‡æœ¬åˆ—è¡¨çš„å…ƒç»„ç‰ˆæœ¬ï¼ˆä¾¿äºç¼“å­˜ï¼‰
     è¿”å›: (topics, probabilities, model)
     """
     if not BERTOPIC_AVAILABLE:
         st.warning("BERTopicæœªå®‰è£…ï¼Œè·³è¿‡é«˜çº§ä¸»é¢˜åˆ†æ")
         return None, None, None
     
     try:
         with st.spinner("ğŸ¤– æ­£åœ¨è®­ç»ƒBERTopicæ¨¡å‹ï¼Œæå–éšè—ä¸»é¢˜..."):
             model = get_bertopic_model()
             if model is None:
                 return None, None, None
             
             texts = list(texts_tuple)
             topics, probs = model.fit_transform(texts)
         return topics, probs, model
     except Exception as e:
         st.warning(f"ä¸»é¢˜æå–å¤±è´¥: {e}")
         return None, None, None


def train_bertopic(texts: List[str], model: Optional[Any] = None) -> tuple:
     """
     è®­ç»ƒBERTopicæ¨¡å‹æå–éšè—ä¸»é¢˜ï¼ˆè‡ªåŠ¨ç¼“å­˜ç‰ˆæœ¬ï¼‰
     
     è¿”å›: (topics, probabilities, model)
     """
     if not BERTOPIC_AVAILABLE:
         st.warning("BERTopicæœªå®‰è£…ï¼Œè·³è¿‡é«˜çº§ä¸»é¢˜åˆ†æ")
         return None, None, None
     
     # è½¬æ¢ä¸ºå…ƒç»„ä¾¿äºstreamlitç¼“å­˜
     texts_tuple = tuple(texts)
     
     return train_bertopic_cached(texts_tuple)


def visualize_topics_2d(model: Optional[Any], topics: Optional[np.ndarray]) -> Optional[object]:
    """ç”Ÿæˆ2Dä¸»é¢˜å¯è§†åŒ–ï¼ˆäº¤äº’å¼å›¾è¡¨ï¼‰"""
    if model is None or topics is None:
        return None
    
    try:
        return model.visualize_topics()
    except Exception as e:
        st.warning(f"ä¸»é¢˜å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_hierarchy(model: Optional[Any]) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜å±‚çº§å…³ç³»å›¾"""
    if model is None:
        return None
    
    try:
        # å°è¯•ç”Ÿæˆå±‚çº§å…³ç³»
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            return model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)
        else:
            st.info("ğŸ’¡ ä¸»é¢˜æ•°é‡å¤ªå°‘ï¼Œæ— æ³•ç”Ÿæˆå±‚çº§å…³ç³»å›¾")
            return None
    except Exception as e:
        st.warning(f"å±‚çº§å…³ç³»å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_similarity(model: Optional[Any]) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜ç›¸ä¼¼åº¦çƒ­åŠ›å›¾"""
    if model is None:
        return None
    
    try:
        return model.visualize_heatmap(n_clusters=5)
    except Exception as e:
        st.warning(f"ç›¸ä¼¼åº¦çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_topic_terms(model: Optional[Any], top_n: int = 5) -> Optional[object]:
    """ç”Ÿæˆä¸»é¢˜è¯è¯­çš„é‡è¦æ€§å›¾è¡¨"""
    if model is None:
        return None
    
    try:
        return model.visualize_terms(top_n_terms=top_n)
    except Exception as e:
        st.warning(f"è¯è¯­é‡è¦æ€§å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        return None


def get_topic_keywords(model: Optional[Any], topic: int, top_n: int = 5) -> List[tuple]:
    """è·å–æŒ‡å®šä¸»é¢˜çš„å…³é”®è¯"""
    if model is None:
        return []
    
    try:
        topic_info = model.get_topic(topic)
        return topic_info[:top_n] if topic_info else []
    except Exception as e:
        return []


def get_topics_summary(model: Optional[Any]) -> pd.DataFrame:
    """è·å–æ‰€æœ‰ä¸»é¢˜çš„æ‘˜è¦ä¿¡æ¯"""
    if model is None:
        return pd.DataFrame()
    
    try:
        topic_info = model.get_topic_info()
        return topic_info[['Topic', 'Count', 'Name']].copy()
    except Exception as e:
        return pd.DataFrame()


def get_documents_by_topic(df: pd.DataFrame, topics: np.ndarray, topic_id: int, top_n: int = 5) -> pd.DataFrame:
    """è·å–æŒ‡å®šä¸»é¢˜ä¸‹çš„æ–‡æ¡£åˆ—è¡¨"""
    if topics is None:
        return pd.DataFrame()
    
    try:
        mask = topics == topic_id
        topic_docs = df[mask].head(top_n)[['source_text', 'sentiment', 'risk_level']].copy()
        return topic_docs
    except Exception as e:
        return pd.DataFrame()


def visualize_documents_2d(model: Optional[Any], docs: List[str], topics: np.ndarray) -> Optional[object]:
    """ç”Ÿæˆæ–‡æ¡£åœ¨2Dç©ºé—´çš„åˆ†å¸ƒï¼ˆUmapé™ç»´ï¼‰"""
    if model is None or topics is None:
        return None
    
    try:
        return model.visualize_documents(docs, topics=topics, hide_document_hover=False)
    except Exception as e:
        try:
            # å¦‚æœæœ‰embeddingå°±ç”¨ï¼Œæ²¡æœ‰å°±ç®€åŒ–ç‰ˆæœ¬
            return model.visualize_documents(docs, hide_document_hover=True)
        except:
            return None


def visualize_term_distribution(model: Optional[Any], top_n_topics: int = 5) -> Optional[object]:
    """ç”Ÿæˆå„ä¸»é¢˜çš„è¯é¢‘åˆ†å¸ƒ"""
    if model is None:
        return None
    
    try:
        return model.visualize_barchart(top_n_topics=top_n_topics)
    except Exception as e:
        return None


def generate_topic_tree(model: Optional[Any], df: pd.DataFrame, topics: np.ndarray) -> str:
    """ç”Ÿæˆä¸»é¢˜çš„æ ‘å½¢ç»“æ„æ–‡æœ¬"""
    if model is None or topics is None:
        return ""
    
    try:
        topic_info = model.get_topic_info()
        tree_text = ""
        
        for idx, row in topic_info[topic_info['Topic'] != -1].iterrows():
            topic_id = row['Topic']
            topic_name = row['Name']
            count = row['Count']
            
            # è·å–è¯¥ä¸»é¢˜çš„å‰3ä¸ªæ–‡æ¡£
            mask = topics == topic_id
            docs = df[mask].head(3)
            
            tree_text += f"**è¯é¢˜{int(topic_id)}: {topic_name}** ({count} æ¡æ–‡æ¡£)\n"
            
            for i, (_, doc) in enumerate(docs.iterrows(), 1):
                text_preview = doc['source_text'][:60] + "..." if len(doc['source_text']) > 60 else doc['source_text']
                tree_text += f"  â”œâ”€ {i}. \"{text_preview}\"\n"
                tree_text += f"     æƒ…æ„Ÿ: {doc['sentiment']} | é£é™©: {doc['risk_level']}\n"
            
            tree_text += "\n"
        
        return tree_text
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {e}"


def visualize_term_score_decline(model: Optional[Any], top_n_topics: int = 5) -> Optional[object]:
    """ç”Ÿæˆc-TF-IDFåˆ†æ•°è¡°å‡å›¾ï¼ˆæ˜¾ç¤ºè¯æ±‡æƒé‡çš„é€’å‡ï¼‰"""
    if model is None:
        return None
    
    try:
        return model.visualize_term_rank(top_n_topics=top_n_topics, log_scale=False)
    except Exception as e:
        return None


def get_hierarchical_topics(model: Optional[Any]) -> Optional[pd.DataFrame]:
    """è®¡ç®—å¹¶è¿”å›å±‚çº§ä¸»é¢˜ç»“æ„"""
    if model is None:
        return None
    
    try:
        # éœ€è¦æœ‰è¶³å¤Ÿçš„ä¸»é¢˜æ‰èƒ½ç”Ÿæˆå±‚çº§
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            return hierarchical_topics
        else:
            return None
    except Exception as e:
        return None


def visualize_hierarchical_documents(model: Optional[Any], texts: List[str], topics: np.ndarray) -> Optional[object]:
    """ç”Ÿæˆåˆ†å±‚æ–‡æ¡£å¯è§†åŒ–ï¼ˆåœ¨å±‚çº§æ ‘çš„2Dç©ºé—´ä¸­ï¼‰"""
    if model is None or topics is None or len(texts) == 0:
        return None
    
    try:
        # è·å–åˆ†å±‚ä¸»é¢˜
        if len(model.get_topic_info()) > 2:
            hierarchical_topics = model.hierarchical_topics(
                model.documents,
                linkage_function=lambda x: __import__('scipy').cluster.hierarchy.linkage(x, "ward")
            )
            # å°è¯•å¯è§†åŒ–åˆ†å±‚æ–‡æ¡£
            return model.visualize_hierarchical_documents(texts, hierarchical_topics=hierarchical_topics, hide_document_hover=True)
        else:
            return None
    except Exception as e:
        return None


def get_topic_keywords_detailed(model: Optional[Any], topic_id: int, top_n: int = 10) -> pd.DataFrame:
    """è·å–æŒ‡å®šä¸»é¢˜çš„å…³é”®è¯åŠå…¶c-TF-IDFåˆ†æ•°"""
    if model is None:
        return pd.DataFrame()
    
    try:
        topic_info = model.get_topic(topic_id)
        if topic_info:
            # topic_info æ˜¯ [(word, score), ...] çš„åˆ—è¡¨
            keywords_df = pd.DataFrame(topic_info[:top_n], columns=['å…³é”®è¯', 'c-TF-IDFåˆ†æ•°'])
            keywords_df['æ’å'] = range(1, len(keywords_df) + 1)
            return keywords_df[['æ’å', 'å…³é”®è¯', 'c-TF-IDFåˆ†æ•°']]
        return pd.DataFrame()
    except Exception as e:
        return pd.DataFrame()


def get_topic_text_representation(model: Optional[Any], topic_id: int) -> str:
    """è·å–ä¸»é¢˜çš„æ–‡æœ¬è¡¨ç¤ºï¼ˆç”±ç”Ÿæˆæ¨¡å‹ç”Ÿæˆï¼‰"""
    if model is None:
        return ""
    
    try:
        # è·å–ä¸»é¢˜çš„æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        topic_info = model.get_topic_info()
        if topic_info is not None and len(topic_info) > 0:
            topic_row = topic_info[topic_info['Topic'] == topic_id]
            if not topic_row.empty:
                return topic_row.iloc[0]['Name']
        return f"è¯é¢˜{topic_id}"
    except Exception as e:
        return f"è¯é¢˜{topic_id}"


def calculate_topic_distribution(model: Optional[Any], texts: List[str]) -> Optional[np.ndarray]:
    """è®¡ç®—æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒæ¦‚ç‡çŸ©é˜µ"""
    if model is None or len(texts) == 0:
        return None
    
    try:
        # å¦‚æœä½¿ç”¨äº†calculate_probabilities=Trueï¼Œå¯ä»¥ç›´æ¥è·å–
        if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
            return model.probabilities_
        else:
            # å¦åˆ™å°è¯•ä¼°è®¡ä¸»é¢˜åˆ†å¸ƒ
            return model.approximate_distribution(texts)
    except Exception as e:
        return None


def visualize_topic_per_class(model: Optional[Any], df: pd.DataFrame, class_column: str = 'sentiment') -> Optional[object]:
    """æŒ‰åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿç±»åˆ«ï¼‰ç”Ÿæˆä¸»é¢˜åˆ†å¸ƒå¯è§†åŒ–"""
    if model is None or df is None or class_column not in df.columns:
        return None
    
    try:
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ç±»åˆ«
        classes = df[class_column].unique()
        
        # åˆ›å»ºç®€å•çš„æŸ±çŠ¶å›¾å¯¹æ¯”
        import plotly.graph_objects as go
        
        fig = go.Figure()
        topic_info = model.get_topic_info()
        
        for class_val in classes:
            class_mask = df[class_column] == class_val
            topic_counts = []
            
            for topic_id in topic_info['Topic']:
                if topic_id == -1:  # è·³è¿‡å™ªå£°
                    continue
                count = len(df[class_mask & (df.index.isin([i for i, t in enumerate(model.topics_) if t == topic_id]))])
                topic_counts.append(count)
            
            fig.add_trace(go.Bar(
                name=str(class_val),
                x=topic_info[topic_info['Topic'] != -1]['Topic'].astype(str),
                y=topic_counts,
                text=topic_counts,
                textposition='auto',
            ))
        
        fig.update_layout(
            title="æŒ‰åˆ†ç±»ç»Ÿè®¡çš„ä¸»é¢˜åˆ†å¸ƒ",
            xaxis_title="ä¸»é¢˜ID",
            yaxis_title="æ–‡æ¡£æ•°é‡",
            barmode='group',
            height=400,
            hovermode='x unified'
        )
        
        return fig
    except Exception as e:
        return None


# ============================================================================
# Phase 4 æ–°å¢å‡½æ•° - F101, F102, F103
# ============================================================================

def visualize_distribution(model: Optional[Any], topic_id: int, probabilities: Optional[np.ndarray] = None, 
                          min_probability: float = 0.015) -> Optional[object]:
    """
    F101: å•æ–‡æ¡£ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–
    
    æ˜¾ç¤ºæŸæ¡æ–‡æ¡£å±äºå„ä¸»é¢˜çš„ç½®ä¿¡åº¦ç™¾åˆ†æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    
    å‚æ•°:
        model: BERTopicæ¨¡å‹
        topic_id: æ–‡æ¡£åœ¨æ¨¡å‹ä¸­çš„index
        probabilities: å•æ¡æ–‡æ¡£çš„ä¸»é¢˜æ¦‚ç‡æ•°ç»„ (shape: [n_topics])
        min_probability: æœ€å°æ˜¾ç¤ºæ¦‚ç‡é˜ˆå€¼
    
    è¿”å›:
        Plotlyäº¤äº’å¼æŸ±çŠ¶å›¾
    """
    if model is None:
        return None
    
    try:
        # å¦‚æœæ²¡æä¾›probabilitiesï¼Œå°è¯•ä»æ¨¡å‹è·å–
        if probabilities is None:
            if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
                if topic_id < len(model.probabilities_):
                    probabilities = model.probabilities_[topic_id]
                else:
                    st.warning(f"æ–‡æ¡£ç´¢å¼•{topic_id}è¶…å‡ºèŒƒå›´")
                    return None
            else:
                st.warning("æ¨¡å‹æœªè®¡ç®—æ¦‚ç‡ï¼Œè¯·åœ¨BERTopicåˆå§‹åŒ–æ—¶è®¾ç½®calculate_probabilities=True")
                return None
        
        # è·å–ä¸»é¢˜ä¿¡æ¯
        topic_info = model.get_topic_info()
        if topic_info is None or len(topic_info) == 0:
            return None
        
        # è¿‡æ»¤ä½æ¦‚ç‡ä¸»é¢˜
        valid_indices = probabilities >= min_probability
        filtered_probs = probabilities[valid_indices]
        
        if len(filtered_probs) == 0:
            st.info("ğŸ’¡ è¯¥æ–‡æ¡£çš„ä¸»é¢˜æ¦‚ç‡éƒ½å¾ˆä½ï¼Œå¯èƒ½æ˜¯å™ªå£°æ–‡æ¡£")
            return None
        
        # è·å–å¯¹åº”çš„ä¸»é¢˜æ ‡ç­¾
        valid_topics = np.where(valid_indices)[0]
        topic_labels = []
        for idx in valid_topics:
            matching = topic_info[topic_info['Topic'] == idx]
            if not matching.empty:
                topic_labels.append(f"è¯é¢˜{int(idx)}: {matching.iloc[0]['Name'][:15]}")
            else:
                topic_labels.append(f"è¯é¢˜{int(idx)}")
        
        # åˆ›å»ºå›¾è¡¨
        import plotly.graph_objects as go
        fig = go.Figure(data=[go.Bar(
            x=topic_labels,
            y=filtered_probs,
            marker=dict(color=filtered_probs, colorscale='Viridis', showscale=True),
            text=[f'{p:.2%}' for p in filtered_probs],
            textposition='outside'
        )])
        
        fig.update_layout(
            title=f"æ–‡æ¡£{topic_id}çš„ä¸»é¢˜æ¦‚ç‡åˆ†å¸ƒ",
            xaxis_title="ä¸»é¢˜",
            yaxis_title="æ¦‚ç‡",
            height=400,
            hovermode='x unified'
        )
        
        return fig
    
    except Exception as e:
        st.warning(f"æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        return None


def visualize_approximate_distribution(model: Optional[Any], texts: List[str], 
                                       doc_index: int = 0, calculate_tokens: bool = True) -> Optional[object]:
    """
    F102: Tokençº§åˆ«ä¸»é¢˜åˆ†å¸ƒåˆ†æ
    
    ç²¾ç¡®åˆ°å•è¯çº§åˆ«ï¼Œæ˜¾ç¤ºå“ªäº›è¯è§¦å‘äº†å“ªä¸ªä¸»é¢˜
    
    å‚æ•°:
        model: BERTopicæ¨¡å‹
        texts: æ‰€æœ‰æ–‡æœ¬åˆ—è¡¨
        doc_index: è¦åˆ†æçš„æ–‡æ¡£ç´¢å¼•
        calculate_tokens: æ˜¯å¦è®¡ç®—tokençº§åˆ«çš„åˆ†å¸ƒ
    
    è¿”å›:
        åŒ…å«tokençº§åˆ†å¸ƒçš„DataFrameæˆ–å¯è§†åŒ–
    """
    if model is None or doc_index >= len(texts):
        return None
    
    try:
        # è·å–è¿‘ä¼¼åˆ†å¸ƒï¼ˆapproximate_distributionï¼‰
        topic_distr, topic_token_distr = model.approximate_distribution(
            [texts[doc_index]],
            calculate_tokens=calculate_tokens
        )
        
        if topic_distr is None:
            return None
        
        # è½¬æ¢ä¸ºDataFrameæ˜¾ç¤º
        topic_info = model.get_topic_info()
        
        # ä¸»é¢˜çº§åˆ†å¸ƒ
        result_data = []
        for topic_id, prob in enumerate(topic_distr[0]):
            if prob > 0.01:  # åªæ˜¾ç¤ºæ¦‚ç‡>1%çš„ä¸»é¢˜
                matching = topic_info[topic_info['Topic'] == topic_id]
                topic_name = matching.iloc[0]['Name'] if not matching.empty else f"è¯é¢˜{topic_id}"
                result_data.append({
                    'ä¸»é¢˜': f"{topic_id}: {topic_name}",
                    'æ¦‚ç‡': f"{prob:.2%}",
                    'æ˜¯å¦ä¸ºç›®æ ‡ä¸»é¢˜': prob > 0.3
                })
        
        result_df = pd.DataFrame(result_data)
        
        # Tokençº§åˆ†å¸ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if calculate_tokens and topic_token_distr is not None and len(topic_token_distr) > 0:
            # åˆ›å»ºtokenæ ‡è®°
            text = texts[doc_index]
            words = text.split()
            
            token_info = []
            for word_idx, word in enumerate(words):
                if word_idx < len(topic_token_distr[0]):
                    top_topic = np.argmax(topic_token_distr[0][word_idx])
                    prob = topic_token_distr[0][word_idx][top_topic]
                    matching = topic_info[topic_info['Topic'] == top_topic]
                    topic_name = matching.iloc[0]['Name'] if not matching.empty else f"è¯é¢˜{top_topic}"
                    
                    token_info.append({
                        'è¯': word,
                        'ä¸»é¢˜': f"{top_topic}: {topic_name}",
                        'ç½®ä¿¡åº¦': f"{prob:.2%}"
                    })
            
            token_df = pd.DataFrame(token_info)
            return {'ä¸»é¢˜åˆ†å¸ƒ': result_df, 'è¯çº§åˆ†å¸ƒ': token_df}
        
        return {'ä¸»é¢˜åˆ†å¸ƒ': result_df}
    
    except Exception as e:
        st.warning(f"è¿‘ä¼¼åˆ†å¸ƒè®¡ç®—å¤±è´¥: {e}")
        return None


def reduce_outliers(model: Optional[Any], topics: np.ndarray, 
                   strategy: str = "probabilities", threshold: float = 0.1) -> tuple:
     """
     F103: ç¦»ç¾¤å€¼è‡ªåŠ¨é‡åˆ†ç±»
     
     å°†noiseæ–‡æ¡£(æ ‡ç­¾-1)é‡æ–°åˆ†é…åˆ°æœ‰æ•ˆä¸»é¢˜
     
     å‚æ•°:
         model: BERTopicæ¨¡å‹
         topics: åŸå§‹ä¸»é¢˜æ•°ç»„
         strategy: é‡åˆ†ç±»ç­–ç•¥
             - "probabilities": åŸºäºHDBSCANè½¯èšç±»æ¦‚ç‡
             - "distributions": åŸºäºè¿‘ä¼¼ä¸»é¢˜åˆ†å¸ƒ
             - "c-tf-idf": åŸºäºè¯é¢‘ç›¸ä¼¼åº¦ï¼ˆæœ€å¿«ï¼‰
             - "embeddings": åŸºäºè¯­ä¹‰embeddingç›¸ä¼¼åº¦ï¼ˆæœ€å‡†ï¼‰
         threshold: åˆ†é…ç½®ä¿¡åº¦é˜ˆå€¼ (0.05-0.3)
     
     è¿”å›:
         (æ–°çš„topicsæ•°ç»„, ç»Ÿè®¡æŠ¥å‘Šdict)
     """
     if model is None or topics is None:
         return topics, {}
     
     try:
         # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
         noise_mask = topics == -1
         noise_count_before = np.sum(noise_mask)
         
         if noise_count_before == 0:
             return topics, {'message': 'æ²¡æœ‰ç¦»ç¾¤å€¼éœ€è¦å¤„ç†'}
         
         # è°ƒç”¨BERTopicçš„reduce_outliersæ–¹æ³•
         new_topics = model.reduce_outliers(
             topics,
             strategy=strategy,
             threshold=threshold
         )
         
         # è®¡ç®—æ”¹è¿›æ•ˆæœ
         noise_count_after = np.sum(new_topics == -1)
         reclassified_count = noise_count_before - noise_count_after
         reclassified_pct = reclassified_count / noise_count_before * 100 if noise_count_before > 0 else 0
         
         report = {
             'strategy': strategy,
             'threshold': threshold,
             'é‡åˆ†ç±»å‰å™ªå£°æ•°': int(noise_count_before),
             'é‡åˆ†ç±»åå™ªå£°æ•°': int(noise_count_after),
             'é‡æ–°åˆ†é…æ•°': int(reclassified_count),
             'æ”¹è¿›ç‡': f"{reclassified_pct:.1f}%",
             'çŠ¶æ€': 'æˆåŠŸ' if reclassified_count > 0 else 'æ— æ”¹è¿›'
         }
         
         return new_topics, report
     
     except Exception as e:
         st.warning(f"ç¦»ç¾¤å€¼å¤„ç†å¤±è´¥: {e}")
         return topics, {'error': str(e)}


# ============================================================================
# Phase 5 æ–°å¢å‡½æ•° - F104, F105, F106
# ============================================================================

def set_topic_labels(model: Optional[Any], topic_labels_dict: dict) -> tuple:
    """
    F104: è‡ªå®šä¹‰ä¸»é¢˜æ ‡ç­¾è®¾ç½®
    
    å…è®¸ç”¨æˆ·ä¸ºä¸»é¢˜æŒ‡å®šè‡ªå®šä¹‰åç§°ï¼Œæ›¿æ¢è‡ªåŠ¨ç”Ÿæˆçš„æ ‡ç­¾
    
    å‚æ•°:
         model: BERTopicæ¨¡å‹
         topic_labels_dict: æ ‡ç­¾æ˜ å°„å­—å…¸ {topic_id: custom_label}
         ä¾‹å¦‚: {0: "ç”¨æˆ·ä½“éªŒ", 1: "äº§å“è´¨é‡", 2: "é…é€é€Ÿåº¦"}
    
    è¿”å›:
         (æ›´æ–°åçš„æ¨¡å‹, æ“ä½œç»“æœdict)
    """
    if model is None or not topic_labels_dict:
        return model, {'status': 'å¤±è´¥', 'message': 'æ¨¡å‹æˆ–æ ‡ç­¾ä¸ºç©º'}
    
    try:
         # BERTopicçš„set_topic_labelsæ–¹æ³•
         model.set_topic_labels(topic_labels_dict)
         
         result = {
             'status': 'æˆåŠŸ',
             'message': f'å·²è®¾ç½®{len(topic_labels_dict)}ä¸ªä¸»é¢˜çš„è‡ªå®šä¹‰æ ‡ç­¾',
             'labels_count': len(topic_labels_dict)
         }
         
         return model, result
    
    except Exception as e:
        return model, {'status': 'å¤±è´¥', 'message': str(e)}


def visualize_barchart_comparison(model: Optional[Any], top_n_topics: int = 5, top_n_words: int = 10) -> Optional[object]:
    """
    F105: å¤šä¸»é¢˜è¯æƒé‡å¯¹æ¯”æŸ±çŠ¶å›¾
    
    å¹¶è¡Œæ˜¾ç¤ºå¤šä¸ªä¸»é¢˜çš„Topè¯åŠå…¶æƒé‡ï¼Œæ–¹ä¾¿è¿›è¡Œä¸»é¢˜å¯¹æ¯”
    
    å‚æ•°:
         model: BERTopicæ¨¡å‹
         top_n_topics: æ˜¾ç¤ºå¤šå°‘ä¸ªä¸»é¢˜
         top_n_words: æ¯ä¸ªä¸»é¢˜æ˜¾ç¤ºå¤šå°‘ä¸ªTopè¯
    
    è¿”å›:
         Plotlyäº¤äº’å¼å¯è§†åŒ–å¯¹è±¡
    """
    if model is None:
        return None
    
    try:
        # è°ƒç”¨BERTopicçš„visualize_barchartæ–¹æ³•
        fig = model.visualize_barchart(top_n_topics=top_n_topics, top_n_words=top_n_words)
        return fig
    
    except Exception as e:
        st.warning(f"å¤šä¸»é¢˜è¯æƒé‡å¯¹æ¯”ç”Ÿæˆå¤±è´¥: {e}")
        return None


def search_topics(model: Optional[Any], keywords: List[str], top_n: int = 5) -> pd.DataFrame:
    """
    F106: å…³é”®è¯ä¸»é¢˜æœç´¢
    
    è¾“å…¥å…³é”®è¯ï¼Œè¿”å›åŒ…å«è¿™äº›è¯çš„ä¸»é¢˜åˆ—è¡¨åŠç›¸å…³æ€§æ’å
    
    å‚æ•°:
         model: BERTopicæ¨¡å‹
         keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
         top_n: è¿”å›æ’åå‰nçš„ç›¸å…³ä¸»é¢˜
    
    è¿”å›:
         åŒ…å«ä¸»é¢˜ã€åŒ¹é…è¯ã€ç›¸å…³æ€§åˆ†æ•°çš„DataFrame
    """
    if model is None or not keywords:
        return pd.DataFrame()
    
    try:
        topic_info = model.get_topic_info()
        results = []
        
        for topic_id in topic_info['Topic']:
            if topic_id == -1:  # è·³è¿‡å™ªå£°
                continue
            
            # è·å–è¯¥ä¸»é¢˜çš„æ‰€æœ‰è¯
            topic_words = model.get_topic(topic_id)
            if not topic_words:
                continue
            
            word_list = [word for word, score in topic_words]
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            matched_words = []
            match_scores = []
            
            for keyword in keywords:
                for idx, (word, score) in enumerate(topic_words):
                    if keyword in word or word in keyword:
                        matched_words.append(word)
                        match_scores.append(score * (1 / (idx + 1)))  # æƒé‡ï¼šæ’åè¶Šé«˜åˆ†æ•°è¶Šé«˜
                        break
            
            if matched_words:
                # è¯¥ä¸»é¢˜ä¸æœç´¢è¯ç›¸å…³
                avg_score = np.mean(match_scores) if match_scores else 0
                topic_name = topic_info[topic_info['Topic'] == topic_id].iloc[0]['Name']
                
                results.append({
                    'ä¸»é¢˜ID': int(topic_id),
                    'ä¸»é¢˜åç§°': topic_name,
                    'åŒ¹é…è¯': ', '.join(matched_words),
                    'å¹³å‡ç›¸å…³æ€§': f"{avg_score:.3f}",
                    'æ–‡æ¡£æ•°': int(topic_info[topic_info['Topic'] == topic_id].iloc[0]['Count'])
                })
        
        if results:
            results_df = pd.DataFrame(results)
            # æŒ‰ç›¸å…³æ€§æ’åº
            results_df['ç›¸å…³æ€§åˆ†æ•°'] = results_df['å¹³å‡ç›¸å…³æ€§'].astype(float)
            results_df = results_df.sort_values('ç›¸å…³æ€§åˆ†æ•°', ascending=False).head(top_n)
            results_df = results_df.drop('ç›¸å…³æ€§åˆ†æ•°', axis=1)
            return results_df
        else:
            return pd.DataFrame()
    
    except Exception as e:
        st.warning(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
        return pd.DataFrame()


# ============================================================================
# Phase 6 æ–°å¢å‡½æ•° - F109
# ============================================================================

def get_representative_documents(df: pd.DataFrame, model: Optional[Any], topics: np.ndarray, 
                                 topic_id: int, top_n: int = 3) -> pd.DataFrame:
    """
    F109: ä¸»é¢˜ä»£è¡¨æ–‡æ¡£æå–
    
    è·å–æŸä¸ªä¸»é¢˜æœ€å…·ä»£è¡¨æ€§çš„Top Næ–‡æ¡£ï¼Œç”¨äºç†è§£ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹
    
    å‚æ•°:
         df: æ•°æ®æ¡†ï¼ˆåŒ…å«source_text, sentiment, risk_levelç­‰åˆ—ï¼‰
         model: BERTopicæ¨¡å‹
         topics: ä¸»é¢˜æ•°ç»„
         topic_id: è¦æå–ä»£è¡¨æ–‡æ¡£çš„ä¸»é¢˜ID
         top_n: è¿”å›å¤šå°‘ä¸ªä»£è¡¨æ–‡æ¡£ï¼ˆé»˜è®¤3ï¼‰
    
    è¿”å›:
         åŒ…å«ä»£è¡¨æ–‡æ¡£åŠå…¶å±æ€§çš„DataFrame
    """
    if model is None or topics is None or df is None:
        return pd.DataFrame()
    
    try:
        # è·å–è¯¥ä¸»é¢˜çš„æ‰€æœ‰æ–‡æ¡£ç´¢å¼•
        mask = topics == topic_id
        
        if not mask.any():
            return pd.DataFrame()
        
        # è·å–è¯¥ä¸»é¢˜çš„æ¦‚ç‡çŸ©é˜µï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
            # æŒ‰æ¦‚ç‡æ’åºï¼ˆæ¦‚ç‡é«˜ = ä»£è¡¨æ€§å¼ºï¼‰
            topic_probs = model.probabilities_[mask, topic_id]
            indices = np.argsort(topic_probs)[::-1][:top_n]
            doc_indices = np.where(mask)[0][indices]
        else:
            # æ²¡æœ‰æ¦‚ç‡ä¿¡æ¯ï¼Œè¿”å›å‰top_nä¸ªæ–‡æ¡£
            doc_indices = np.where(mask)[0][:top_n]
        
        # æ„å»ºç»“æœDataFrame
        results = []
        
        for idx, doc_idx in enumerate(doc_indices, 1):
            doc = df.iloc[doc_idx]
            
            # è·å–è¯¥æ–‡æ¡£å¯¹è¯¥ä¸»é¢˜çš„ç½®ä¿¡åº¦
            conf = None
            if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
                if doc_idx < len(model.probabilities_):
                    conf = model.probabilities_[doc_idx, topic_id]
            
            results.append({
                'æ’å': idx,
                'æ–‡æ¡£ID': doc_idx,
                'å†…å®¹': doc['source_text'][:100] + ('...' if len(doc['source_text']) > 100 else ''),
                'å®Œæ•´å†…å®¹': doc['source_text'],
                'æƒ…æ„Ÿ': doc.get('sentiment', 'æœªçŸ¥'),
                'é£é™©': doc.get('risk_level', 'æœªçŸ¥'),
                'ç½®ä¿¡åº¦': f"{conf:.2%}" if conf else "N/A"
            })
        
        result_df = pd.DataFrame(results)
        return result_df
    
    except Exception as e:
        st.warning(f"ä»£è¡¨æ–‡æ¡£æå–å¤±è´¥: {e}")
        return pd.DataFrame()


def get_all_topics_representative_docs(df: pd.DataFrame, model: Optional[Any], 
                                       topics: np.ndarray, top_n: int = 3) -> dict:
    """
    F109 æ‰©å±•: ä¸ºæ‰€æœ‰ä¸»é¢˜æ‰¹é‡æå–ä»£è¡¨æ–‡æ¡£
    
    å‚æ•°:
         df: æ•°æ®æ¡†
         model: BERTopicæ¨¡å‹
         topics: ä¸»é¢˜æ•°ç»„
         top_n: æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ–‡æ¡£æ•°
    
    è¿”å›:
         {topic_id: representative_docs_df} çš„å­—å…¸
    """
    if model is None:
        return {}
    
    try:
        topic_info = model.get_topic_info()
        all_docs = {}
        
        for topic_id in topic_info['Topic']:
            if topic_id == -1:  # è·³è¿‡å™ªå£°
                continue
            
            docs_df = get_representative_documents(df, model, topics, topic_id, top_n)
            
            if not docs_df.empty:
                all_docs[topic_id] = docs_df
        
        return all_docs
    
    except Exception as e:
        return {}


# ============================================================================
# Phase 7 æ–°å¢å‡½æ•° - F107
# ============================================================================

def export_visualization_to_file(fig: Optional[object], filename: str, format: str = 'png', 
                                dpi: int = 300, width: int = 1200, height: int = 700) -> bytes:
    """
    F107: è®ºæ–‡çº§é™æ€å›¾å¯¼å‡º
    
    å°†Plotlyå¯è§†åŒ–å¯¼å‡ºä¸ºé«˜åˆ†è¾¨ç‡çš„é™æ€å›¾ï¼ˆPNG/PDF/SVGï¼‰ç”¨äºæŠ¥å‘Šå’Œè®ºæ–‡
    
    å‚æ•°:
         fig: Plotlyå›¾å¯¹è±¡
         filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
         format: å¯¼å‡ºæ ¼å¼ ('png', 'pdf', 'svg', 'jpg')
         dpi: åˆ†è¾¨ç‡ï¼ˆæ¯è‹±å¯¸åƒç´ æ•°ï¼Œæ¨è300ç”¨äºæ‰“å°ï¼‰
         width: å›¾ç‰‡å®½åº¦ï¼ˆåƒç´ ï¼‰
         height: å›¾ç‰‡é«˜åº¦ï¼ˆåƒç´ ï¼‰
    
    è¿”å›:
         bytes: æ–‡ä»¶å†…å®¹ï¼ˆå¯ç”¨äºä¸‹è½½ï¼‰
    """
    if fig is None:
        st.error("âŒ å›¾è¡¨å¯¹è±¡ä¸ºç©º")
        return None
    
    try:
        # è°ƒæ•´å›¾è¡¨å°ºå¯¸å’Œæ ·å¼ä»¥é€‚åº”å¯¼å‡º
        fig.update_layout(
            width=width,
            height=height,
            font=dict(size=12),
            margin=dict(l=50, r=50, t=50, b=50),
            paper_bgcolor='white',
            plot_bgcolor='white'
        )
        
        # ä½¿ç”¨kaleidoå¯¼å‡ºï¼ˆéœ€è¦å®‰è£…ï¼‰
        format_lower = format.lower()
        if format_lower == 'png':
            file_content = fig.to_image(format='png', width=width, height=height, scale=dpi/100)
        elif format_lower == 'pdf':
            file_content = fig.to_image(format='pdf', width=width, height=height)
        elif format_lower == 'svg':
            file_content = fig.to_image(format='svg', width=width, height=height)
        elif format_lower == 'jpg':
            file_content = fig.to_image(format='jpg', width=width, height=height, quality=95)
        else:
            st.error(f"âŒ ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            return None
        
        return file_content
    
    except ImportError:
        st.warning("âš ï¸ éœ€è¦å®‰è£…kaleido: pip install kaleido")
        # é™çº§æ–¹æ¡ˆï¼šå°è¯•ç”¨plotlyçš„ç¦»çº¿å¯¼å‡º
        try:
            if format.lower() in ['html']:
                return fig.to_html().encode('utf-8')
            else:
                st.error("âŒ éœ€è¦kaleidoåº“æ‰èƒ½å¯¼å‡ºé™æ€å›¾ç‰‡")
                return None
        except:
            return None
    
    except Exception as e:
        st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return None


def batch_export_visualizations(figures_dict: dict, export_format: str = 'png', 
                               output_folder: str = 'exports', dpi: int = 300) -> dict:
    """
    F107 æ‰©å±•: æ‰¹é‡å¯¼å‡ºå¤šä¸ªå›¾è¡¨
    
    å‚æ•°:
         figures_dict: {name: fig_object} çš„å­—å…¸
         export_format: å¯¼å‡ºæ ¼å¼
         output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
         dpi: å¯¼å‡ºåˆ†è¾¨ç‡
    
    è¿”å›:
         {name: file_content} çš„å­—å…¸
    """
    if not figures_dict:
        return {}
    
    try:
        import os
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
        os.makedirs(output_folder, exist_ok=True)
        
        exported = {}
        
        for name, fig in figures_dict.items():
            try:
                file_content = export_visualization_to_file(
                    fig, 
                    name, 
                    format=export_format, 
                    dpi=dpi
                )
                
                if file_content:
                    exported[name] = file_content
            
            except Exception as e:
                st.warning(f"âš ï¸ {name}å¯¼å‡ºå¤±è´¥: {e}")
        
        return exported
    
    except Exception as e:
        st.error(f"âŒ æ‰¹é‡å¯¼å‡ºå¤±è´¥: {e}")
        return {}


def create_summary_report(model: Optional[Any], df: pd.DataFrame, topics: np.ndarray,
                         title: str = "BERTopicåˆ†ææŠ¥å‘Š") -> str:
    """
    F107 æ‰©å±•: ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„åˆ†ææŠ¥å‘Šæ‘˜è¦
    
    å‚æ•°:
         model: BERTopicæ¨¡å‹
         df: æ•°æ®æ¡†
         topics: ä¸»é¢˜æ•°ç»„
         title: æŠ¥å‘Šæ ‡é¢˜
    
    è¿”å›:
         str: Markdownæ ¼å¼çš„æŠ¥å‘Šæ–‡æœ¬
    """
    if model is None or topics is None:
        return ""
    
    try:
        topic_info = model.get_topic_info()
        
        report = f"""# {title}

## æ•°æ®æ¦‚è§ˆ

- **æ€»æ–‡æ¡£æ•°**: {len(df)}
- **å”¯ä¸€ä¸»é¢˜æ•°**: {len(np.unique(topics)) - 1}ï¼ˆä¸å«å™ªå£°ï¼‰
- **å™ªå£°æ–‡æ¡£æ•°**: {np.sum(topics == -1)} ({100*np.sum(topics == -1)/len(df):.1f}%)
- **æ•°æ®è¦†ç›–ç‡**: {100*(len(df)-np.sum(topics==-1))/len(df):.1f}%

## ä¸»é¢˜ç»Ÿè®¡

| ä¸»é¢˜ID | ä¸»é¢˜åç§° | æ–‡æ¡£æ•° | å æ¯” | Top 5å…³é”®è¯ |
|--------|---------|--------|------|-----------|
"""
        
        for _, row in topic_info[topic_info['Topic'] != -1].iterrows():
            topic_id = int(row['Topic'])
            topic_name = row['Name']
            count = row['Count']
            pct = 100 * count / (len(df) - np.sum(topics == -1))
            
            # è·å–Top 5å…³é”®è¯
            topic_words = model.get_topic(topic_id)
            if topic_words:
                top_words = ', '.join([word for word, _ in topic_words[:5]])
            else:
                top_words = "N/A"
            
            report += f"| {topic_id} | {topic_name} | {count} | {pct:.1f}% | {top_words} |\n"
        
        report += f"""

## æ•°æ®è´¨é‡æŒ‡æ ‡

- **å¹³å‡ä¸»é¢˜æ¦‚ç‡**: """
        
        if hasattr(model, 'probabilities_') and model.probabilities_ is not None:
            avg_prob = np.max(model.probabilities_, axis=1).mean()
            report += f"{avg_prob:.3f}\n"
        else:
            report += "æœªè®¡ç®—\n"
        
        report += f"""
## ç”Ÿæˆä¿¡æ¯

- **ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ä½¿ç”¨å·¥å…·**: BERTopic (Phase 4-7 åˆ†ææ¡†æ¶)
- **è¦†ç›–çš„åˆ†æå‡½æ•°**: F101-F109

---

*æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨åˆ†æç³»ç»Ÿç”Ÿæˆã€‚å»ºè®®ç»“åˆäººå·¥å®¡æ ¸ç¡®ä¿å‡†ç¡®æ€§ã€‚*
"""
        
        return report
    
    except Exception as e:
        return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"
