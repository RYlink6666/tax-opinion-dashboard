# 立即开始爬取数据

## 最简单的方式（一句话启动）

### Windows 用户

```
直接双击 START.bat 文件
或在命令行运行：START.bat
```

### Mac/Linux 用户

```bash
bash start.sh
```

---

## 它会做什么？

一键启动脚本会自动：

```
1. 检查 Python 环境
2. 检查/安装 MediaCrawler
3. 验证配置
4. 启动微博爬虫（后台，24-30小时）
5. 启动知乎爬虫（后台，15-20小时）
6. 爬虫自动运行，你什么都不用做
```

---

## 执行后会看到：

```
============================================================
【跨境电商税收舆论爬虫】- 一键启动
============================================================

✅ Python 环境正常
✅ MediaCrawler 环境正常

【验证配置】
✅ 已加载关键词库：50+ 个关键词
✅ 配置验证通过

【开始采集数据】
============================================================

🟢 启动微博爬虫（后台运行，24-30小时）
🟢 启动知乎爬虫（后台运行，15-20小时）

============================================================
✅ 爬虫已启动！
============================================================

📌 爬虫将在后台运行
📊 监控进度：查看 logs/crawl_weibo.log
🔄 数据清洁：12月13日运行 python 4_merge_and_clean.py
📁 最终输出：data/clean/opinions_clean_5000.txt
```

---

## 爬虫启动后

你什么都不用做，爬虫会自动：
- 采集微博 2500 条
- 采集知乎 1500 条
- 保存到 data/raw/ 目录
- 记录日志到 logs/ 目录

---

## 12月13日（清洁数据）

```bash
python 4_merge_and_clean.py
```

这会：
- 合并所有数据
- 自动去重
- 过滤垃圾
- 输出：data/clean/opinions_clean_5000.txt

---

## 监控进度（可选）

```bash
# 查看实时日志
tail -f logs/crawl_weibo.log

# 会显示采集进度：
# [1/50] 爬取关键词：0110
#   ✓ 获得 150 条有效数据 (总计 150/2500)
# [2/50] 爬取关键词：9610
#   ✓ 获得 200 条有效数据 (总计 350/2500)
```

---

## 完整时间表

```
现在（12月10日）
└─ 运行 START.bat → 爬虫启动

12月11-12日
├─ 微博爬虫自动运行 24-30小时
├─ 知乎爬虫自动运行 15-20小时
└─ 自动采集 4000+ 条

12月13日
├─ 爬虫完成
├─ 运行清洁脚本
└─ 得到 opinions_clean_5000.txt

12月14日
└─ 开始 LLM 分析（见 STEP_2）
```

---

## 现在就做这一件事

### Windows
```
双击 START.bat
```

### Mac/Linux
```bash
bash start.sh
```

**就这一个命令！然后等 24-30 小时，你就有 5000 条数据了！**

---

就这么简单。现在开始！
