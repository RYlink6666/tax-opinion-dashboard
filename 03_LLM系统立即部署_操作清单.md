# LLM èˆ†è®ºåˆ†æç³»ç»Ÿ â€” ç«‹å³éƒ¨ç½²æ“ä½œæ¸…å•

**ç›®çš„**ï¼šä»ä»Šå¤©ï¼ˆ12æœˆ10æ—¥ï¼‰åˆ°12æœˆ20æ—¥ï¼Œç”¨LLMå®Œå…¨è‡ªåŠ¨åŒ–å¤„ç†5000æ¡èˆ†è®º  
**æˆæœ¬**ï¼šÂ¥50-80 | **æ—¶é—´**ï¼š45å°æ—¶å·¥ä½œé‡ | **ç²¾åº¦ç›®æ ‡**ï¼š85%+ 

---

## ç¬¬ä¸€å¤©ï¼ˆ12æœˆ11æ—¥ï¼‰ï¼šç¯å¢ƒæ­å»º â€” 2å°æ—¶

### æ­¥éª¤1.1ï¼šæ³¨å†Œ Gemini APIï¼ˆ15åˆ†é’Ÿï¼‰

```bash
1. æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼šhttps://aistudio.google.com
2. ç”¨Googleè´¦å·ç™»å½•ï¼ˆæˆ–åˆ›å»ºæ–°Googleè´¦å·ï¼‰
3. ç‚¹å‡»"Get API Key" â†’ "Create API key in new project"
4. ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºAPIå¯†é’¥ï¼Œå¤åˆ¶ä¿å­˜
5. å¦¥å–„ä¿ç®¡å¯†é’¥ï¼ˆä¸è¦åˆ†äº«ã€ä¸è¦ä¸Šä¼ GitHubï¼‰
```

**éªŒè¯æˆåŠŸæ ‡å¿—**ï¼šèƒ½çœ‹åˆ°ç±»ä¼¼ `AIza...` çš„å¯†é’¥å­—ç¬¦ä¸²

### æ­¥éª¤1.2ï¼šå®‰è£… Python å’Œ LangExtractï¼ˆ30åˆ†é’Ÿï¼‰

```bash
# æ‰“å¼€å‘½ä»¤è¡Œ/Terminalï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤

# 1. æ£€æŸ¥Pythonç‰ˆæœ¬
python --version
# åº”è¯¥çœ‹åˆ° Python 3.8+ çš„ç‰ˆæœ¬

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv opinion_env

# 3. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
opinion_env\Scripts\activate
# macOS/Linux:
source opinion_env/bin/activate

# 4. å®‰è£… LangExtract
pip install langextract google-generativeai

# 5. éªŒè¯å®‰è£…
python -c "import langextract; print('âœ… LangExtract installed')"
```

**éªŒè¯æˆåŠŸæ ‡å¿—**ï¼šå‘½ä»¤è¡Œæ˜¾ç¤º "âœ… LangExtract installed"

### æ­¥éª¤1.3ï¼šé…ç½® API å¯†é’¥ï¼ˆ15åˆ†é’Ÿï¼‰

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

# åˆ›å»º .env æ–‡ä»¶ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
# å†…å®¹ï¼š
GEMINI_API_KEY=ä½ çš„APIå¯†é’¥

# Pythonä»£ç ä¸­åŠ è½½ï¼š
from dotenv import load_dotenv
import os
load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')

# æ–¹æ³•2ï¼šç›´æ¥åœ¨ä»£ç ä¸­ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ï¼‰
import google.generativeai as genai
genai.configure(api_key="ä½ çš„APIå¯†é’¥")
```

**éªŒè¯æˆåŠŸæ ‡å¿—**ï¼šèƒ½è°ƒé€šAPIä¸”è¿”å›å“åº”

### æ­¥éª¤1.4ï¼šéªŒè¯ç³»ç»Ÿå°±ç»ªï¼ˆ30åˆ†é’Ÿï¼‰

```python
# test_setup.py - è¿è¡Œè¿™ä¸ªè„šæœ¬éªŒè¯æ‰€æœ‰ç»„ä»¶å°±ç»ª

import langextract as lx

# æµ‹è¯•æ–‡æœ¬
test_text = "9610å¤‡æ¡ˆ3ä¸ªæœˆè¿˜æ²¡åŠ¨é™ï¼ŒçœŸçš„å¾ˆç„¦è™‘"

# è°ƒç”¨LangExtract
instruction = """
åˆ†æè¿™æ®µèˆ†è®ºï¼š
1. æƒ…æ„Ÿæ˜¯ä»€ä¹ˆï¼Ÿ(positive/negative/neutral)
2. æ¶‰åŠå“ªä¸ªæ¨¡å¼ï¼Ÿ(0110/9610/9710/9810/1039/Temu/None)
"""

try:
    result = lx.extract(
        text=[test_text],
        instruction=instruction,
        model="gemini-2.5-flash"
    )
    print("âœ… ç³»ç»Ÿå°±ç»ªï¼")
    print(f"ç»“æœï¼š{result}")
except Exception as e:
    print(f"âŒ å‡ºé”™ï¼š{e}")
    print("æ£€æŸ¥ï¼šAPIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
```

**éªŒè¯æˆåŠŸæ ‡å¿—**ï¼šæ”¶åˆ°JSONæ ¼å¼çš„åˆ†æç»“æœ

---

## ç¬¬äºŒå¤©ï¼ˆ12æœˆ12æ—¥ï¼‰ï¼šæ ·æœ¬æµ‹è¯• â€” 3å°æ—¶

### æ­¥éª¤2.1ï¼šå‡†å¤‡ 100 æ¡æ ·æœ¬æ•°æ®ï¼ˆ30åˆ†é’Ÿï¼‰

```python
# ä»ä½ å·²æœ‰çš„èˆ†è®ºæ•°æ®ä¸­éšæœºæŠ½å–100æ¡
# ä¿å­˜ä¸º sample_100.txtï¼Œæ¯è¡Œä¸€æ¡èˆ†è®º

import random

with open('all_opinions_5000.txt', 'r', encoding='utf-8') as f:
    all_opinions = [line.strip() for line in f.readlines()]

# éšæœºæŠ½æ ·100æ¡
sample = random.sample(all_opinions, 100)

# ä¿å­˜æ ·æœ¬
with open('sample_100.txt', 'w', encoding='utf-8') as f:
    for opinion in sample:
        f.write(opinion + '\n')

print(f"âœ… æ ·æœ¬å‡†å¤‡å®Œæˆï¼š{len(sample)} æ¡")
```

### æ­¥éª¤2.2ï¼šè¿è¡Œ LLM åˆ†ææ ·æœ¬ï¼ˆ90åˆ†é’Ÿï¼‰

```python
# analyze_sample.py

import langextract as lx
import json
from datetime import datetime

# è¯»å–æ ·æœ¬
with open('sample_100.txt', 'r', encoding='utf-8') as f:
    sample_opinions = [line.strip() for line in f.readlines()]

# å®šä¹‰å®Œæ•´Promptï¼ˆè§ç¬¬ä¸€éƒ¨åˆ†çš„2.1ï¼‰
prompt = """
ã€ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºåˆ†æç³»ç»Ÿã€‘
...ï¼ˆå®Œæ•´promptï¼Œçº¦500è¡Œï¼‰
"""

# å®šä¹‰Few-shotä¾‹å­ï¼ˆè§ç¬¬ä¸€éƒ¨åˆ†çš„3.1ï¼‰
few_shot_examples = [
    {
        "text": "9610å¤‡æ¡ˆ3ä¸ªæœˆäº†è¿˜æ²¡åŠ¨é™...",
        "sentiment": "negative",
        "pattern": "9610",
        # ... å…¶ä»–å­—æ®µ
    },
    # ... æ›´å¤šä¾‹å­
]

# æ‰§è¡Œåˆ†æ
print("ğŸ”„ å¼€å§‹å¤„ç†100æ¡æ ·æœ¬...")
results = lx.extract(
    text=sample_opinions,
    instruction=prompt,
    examples=few_shot_examples,
    model="gemini-2.5-flash",
    parallel_processing=True,
    batch_size=10,  # å°æ‰¹é‡ä»¥èŠ‚çœAPIé…é¢
    multiple_passes=True  # å¤šè½®æé«˜å‡†ç¡®æ€§
)

# ä¿å­˜ç»“æœ
output = {
    "timestamp": datetime.now().isoformat(),
    "sample_size": 100,
    "results": results
}

with open('sample_100_results.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print(f"âœ… æ ·æœ¬åˆ†æå®Œæˆï¼")
print(f"ç»“æœå·²ä¿å­˜åˆ° sample_100_results.json")
```

**é¢„æœŸè€—æ—¶**ï¼š100æ¡æ•°æ® Ã— æ¯æ¡2ç§’ â‰ˆ 3-4åˆ†é’Ÿï¼ˆparallelå¤„ç†ï¼‰  
**é¢„æœŸæˆæœ¬**ï¼š100æ¡ Ã— Â¥0.0002/token â‰ˆ Â¥0.5ï¼ˆéå¸¸ä¾¿å®œï¼‰

### æ­¥éª¤2.3ï¼šäººå·¥æ ‡æ³¨ä¸å¯¹æ¯”éªŒè¯ï¼ˆ60åˆ†é’Ÿï¼‰

```
æ“ä½œï¼šæŠ½å–æ ·æœ¬ä¸­çš„20æ¡ï¼Œæ‰‹å·¥æ ‡æ³¨ï¼ˆå‚è€ƒ01æ–‡ä»¶ä¸­çš„åˆ†ç±»æ ‡å‡†ï¼‰ï¼Œ
      ä¸LLMç»“æœå¯¹æ¯”

è¿‡ç¨‹ï¼š
â”œâ”€ é€‰å–20æ¡æœ‰ä»£è¡¨æ€§çš„èˆ†è®º
â”œâ”€ æŒ‰ç…§äº”ç»´åº¦æ‰‹å·¥æ ‡æ³¨ï¼ˆæƒ…æ„Ÿã€æ¨¡å¼ã€é£é™©ã€èº«ä»½ã€è¡Œä¸ºï¼‰
â”œâ”€ å¯¹æ¯”LLMç»“æœ
â”œâ”€ ç»Ÿè®¡åŒ¹é…ç‡
â””â”€ åˆ†æé”™è¯¯æ¨¡å¼

éªŒè¯æˆåŠŸæ ‡å¿—ï¼š
â”œâ”€ æ€»ä½“åŒ¹é…ç‡ â‰¥ 85% âœ…
â”œâ”€ æƒ…æ„Ÿè¯†åˆ« â‰¥ 90%
â”œâ”€ æ¨¡å¼è¯†åˆ« â‰¥ 88%
â”œâ”€ é£é™©è¯†åˆ« â‰¥ 82%
â””â”€ å¦‚æœä½äºè¿™äº›æ ‡å‡†ï¼Œè°ƒæ•´Promptå’ŒFew-shotä¾‹å­
```

**æ ·æœ¬å¯¹æ¯”è¡¨æ ¼ç¤ºä¾‹**ï¼š

| èˆ†è®º | LLMæƒ…æ„Ÿ | äººå·¥æƒ…æ„Ÿ | åŒ¹é… | LLMæ¨¡å¼ | äººå·¥æ¨¡å¼ | åŒ¹é… | æ€»ä½“ |
|-----|--------|--------|------|--------|--------|------|------|
| 1 | Negative | Negative | âœ… | 9610 | 9610 | âœ… | âœ… |
| 2 | Positive | Positive | âœ… | None | 1039 | âŒ | âŒ |
| 3 | Negative | Negative | âœ… | 0110 | 0110 | âœ… | âœ… |
| ... | ... | ... | ... | ... | ... | ... | ... |
| **åŒ¹é…ç‡** | | | 95% | | | 90% | 92.5% |

---

## ç¬¬ä¸‰åˆ°äº”å¤©ï¼ˆ12æœˆ13-15æ—¥ï¼‰ï¼šå…¨é‡å¤„ç† â€” 8å°æ—¶

### æ­¥éª¤3.1ï¼šå‡†å¤‡ 5000 æ¡å®Œæ•´æ•°æ®ï¼ˆ30åˆ†é’Ÿï¼‰

```python
# æ£€æŸ¥æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§

with open('all_opinions_5000.txt', 'r', encoding='utf-8') as f:
    all_opinions = [line.strip() for line in f.readlines() 
                   if line.strip()]  # è¿‡æ»¤ç©ºè¡Œ

print(f"æ€»æ•°æ®æ¡æ•°ï¼š{len(all_opinions)}")

# æ•°æ®æ¸…æ´—
cleaned_opinions = []
for opinion in all_opinions:
    # ç§»é™¤è¿‡çŸ­çš„æ–‡æœ¬ï¼ˆ< 10å­—ç¬¦ï¼‰
    if len(opinion) >= 10:
        # ç§»é™¤é‡å¤
        if opinion not in cleaned_opinions:
            cleaned_opinions.append(opinion)

print(f"æ¸…æ´—åæ¡æ•°ï¼š{len(cleaned_opinions)}")

# ä¿å­˜æ¸…æ´ç‰ˆæœ¬
with open('opinions_clean_5000.txt', 'w', encoding='utf-8') as f:
    for opinion in cleaned_opinions:
        f.write(opinion + '\n')
```

### æ­¥éª¤3.2ï¼šæ‰¹é‡å¤„ç†5000æ¡ï¼ˆ6å°æ—¶ + è‡ªåŠ¨è¿è¡Œï¼‰

```python
# process_all_opinions.py - ä¸»è¦å¤„ç†è„šæœ¬

import langextract as lx
import json
import time
from datetime import datetime

# è¯»å–æ•°æ®
with open('opinions_clean_5000.txt', 'r', encoding='utf-8') as f:
    all_opinions = [line.strip() for line in f.readlines()]

# å®šä¹‰åˆ†æä»»åŠ¡ï¼ˆå®Œæ•´ç‰ˆæœ¬è§ç¬¬ä¸€éƒ¨åˆ†ï¼‰
prompt = """ã€å®Œæ•´çš„ç³»ç»Ÿpromptã€‘"""
few_shot_examples = [...]  # å®Œæ•´çš„Few-shotä¾‹å­

# é…ç½®æ‰¹å¤„ç†å‚æ•°
batch_size = 100  # æ¯æ‰¹100æ¡ï¼ˆå¹³è¡¡é€Ÿåº¦å’ŒAPIé…é¢ï¼‰
total_batches = (len(all_opinions) + batch_size - 1) // batch_size

all_results = []
start_time = time.time()

print(f"å¼€å§‹å¤„ç† {len(all_opinions)} æ¡èˆ†è®º...")
print(f"æ€»æ‰¹æ•°ï¼š{total_batches}")

# åˆ†æ‰¹å¤„ç†
for batch_num in range(total_batches):
    batch_start = batch_num * batch_size
    batch_end = min((batch_num + 1) * batch_size, len(all_opinions))
    batch_data = all_opinions[batch_start:batch_end]
    
    print(f"\n[{batch_num + 1}/{total_batches}] å¤„ç†ç¬¬ {batch_start} - {batch_end} æ¡...")
    
    try:
        # è°ƒç”¨LLMåˆ†æ
        batch_results = lx.extract(
            text=batch_data,
            instruction=prompt,
            examples=few_shot_examples,
            model="gemini-2.5-flash",
            parallel_processing=True,
            multiple_passes=True,
            batch_size=min(10, len(batch_data))
        )
        
        all_results.extend(batch_results)
        
        # è¿›åº¦æŠ¥å‘Š
        elapsed = time.time() - start_time
        rate = len(all_results) / elapsed
        remaining = (len(all_opinions) - len(all_results)) / rate if rate > 0 else 0
        
        print(f"âœ… æ‰¹æ¬¡å®Œæˆ ({len(batch_results)} æ¡)")
        print(f"   æ€»è¿›åº¦ï¼š{len(all_results)}/{len(all_opinions)} "
              f"({100*len(all_results)/len(all_opinions):.1f}%)")
        print(f"   é¢„è®¡å‰©ä½™æ—¶é—´ï¼š{remaining/3600:.1f} å°æ—¶")
        
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡å¤±è´¥ï¼š{e}")
        print(f"   é‡è¯•ä¸­...")
        # å¯é€‰ï¼šæ·»åŠ é‡è¯•é€»è¾‘
        time.sleep(5)
        continue

# ä¿å­˜å®Œæ•´ç»“æœ
output_data = {
    "metadata": {
        "total_processed": len(all_results),
        "timestamp": datetime.now().isoformat(),
        "model": "gemini-2.5-flash",
        "duration_hours": (time.time() - start_time) / 3600,
    },
    "results": all_results
}

with open('analysis_results_5000.json', 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… å…¨é‡å¤„ç†å®Œæˆï¼")
print(f"æ€»è€—æ—¶ï¼š{(time.time() - start_time)/3600:.1f} å°æ—¶")
print(f"ç»“æœå·²ä¿å­˜åˆ° analysis_results_5000.json")
```

**è¿è¡Œæ—¶é—´é¢„ä¼°**ï¼š
```
5000æ¡ Ã— å¹³å‡2ç§’/æ¡ Ã· 10ä¸ªå¹¶è¡Œ â‰ˆ 1000ç§’ â‰ˆ 16-17åˆ†é’Ÿï¼ˆç†æƒ³æƒ…å†µï¼‰
è€ƒè™‘ç½‘ç»œå»¶è¿Ÿ â†’ é¢„è®¡30-45åˆ†é’Ÿå®é™…è¿è¡Œæ—¶é—´
ä½†å› ä¸ºæ˜¯åå°è¿è¡Œï¼Œä½ å¯ä»¥å»åšå…¶ä»–äº‹
```

**æˆæœ¬é¢„ä¼°**ï¼š
```
5000æ¡ Ã— å¹³å‡2000 tokens/æ¡ Ã— Â¥0.0001/token = Â¥1ï¼ˆè¿‘ä¹å…è´¹ï¼‰
æˆ–æ›´å‡†ç¡®åœ°è¯´ï¼šÂ¥30-80ï¼ˆå–å†³äºPrompté•¿åº¦å’ŒFew-shotæ•°é‡ï¼‰
```

### æ­¥éª¤3.3ï¼šè´¨é‡æ£€æŸ¥ï¼ˆ90åˆ†é’Ÿï¼‰

```python
# quality_check.py - æ£€æŸ¥ç»“æœè´¨é‡

import json
import pandas as pd

# è¯»å–ç»“æœ
with open('analysis_results_5000.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

results = data['results']

# ç»Ÿè®¡åˆ†æ
stats = {
    'total': len(results),
    'sentiment_dist': {},
    'pattern_dist': {},
    'risk_dist': {},
    'avg_confidence': 0,
}

sentiments = []
patterns = []
risks = []
confidences = []

for result in results:
    # æƒ…æ„Ÿåˆ†å¸ƒ
    sentiment = result.get('sentiment')
    stats['sentiment_dist'][sentiment] = stats['sentiment_dist'].get(sentiment, 0) + 1
    sentiments.append(sentiment)
    
    # æ¨¡å¼åˆ†å¸ƒ
    pattern = result.get('pattern')
    stats['pattern_dist'][pattern] = stats['pattern_dist'].get(pattern, 0) + 1
    patterns.append(pattern)
    
    # é£é™©åˆ†å¸ƒ
    risk = result.get('risk_category')
    stats['risk_dist'][risk] = stats['risk_dist'].get(risk, 0) + 1
    risks.append(risk)
    
    # å¹³å‡ç½®ä¿¡åº¦
    confidence = result.get('sentiment_confidence', 0)
    confidences.append(confidence)

stats['avg_confidence'] = sum(confidences) / len(confidences) if confidences else 0

# æ‰“å°ç»Ÿè®¡
print("=== è´¨é‡æ£€æŸ¥æŠ¥å‘Š ===\n")
print(f"æ€»å¤„ç†æ¡æ•°ï¼š{stats['total']}")
print(f"å¹³å‡ç½®ä¿¡åº¦ï¼š{stats['avg_confidence']:.2%}\n")

print("æƒ…æ„Ÿåˆ†å¸ƒï¼š")
for sentiment, count in sorted(stats['sentiment_dist'].items(), key=lambda x: x[1], reverse=True):
    print(f"  {sentiment}: {count} ({100*count/stats['total']:.1f}%)")

print("\nä¸»è¦æ¨¡å¼åˆ†å¸ƒï¼ˆå‰6ä¸ªï¼‰ï¼š")
for pattern, count in sorted(stats['pattern_dist'].items(), key=lambda x: x[1], reverse=True)[:6]:
    print(f"  {pattern}: {count} ({100*count/stats['total']:.1f}%)")

print("\né£é™©ç±»å‹åˆ†å¸ƒï¼ˆå‰8ä¸ªï¼‰ï¼š")
for risk, count in sorted(stats['risk_dist'].items(), key=lambda x: x[1], reverse=True)[:8]:
    print(f"  {risk}: {count} ({100*count/stats['total']:.1f}%)")

# ç”Ÿæˆè´¨é‡æŠ¥å‘Šæ–‡ä»¶
report = {
    'timestamp': data['metadata']['timestamp'],
    'total_processed': stats['total'],
    'statistics': stats,
    'quality_check': {
        'completeness': sum(1 for r in results if all([
            r.get('sentiment'),
            r.get('pattern'),
            r.get('risk_category')
        ])) / stats['total'],
        'confidence_threshold_90': sum(1 for r in results 
            if r.get('sentiment_confidence', 0) >= 0.90) / stats['total'],
        'recommended_action': 'Ready for research use' 
            if sum(1 for r in results if r.get('sentiment_confidence', 0) >= 0.80) / stats['total'] > 0.85
            else 'Requires Prompt adjustment'
    }
}

with open('quality_report_5000.json', 'w', encoding='utf-8') as f:
    json.dump(report, f, ensure_ascii=False, indent=2)

print("\nâœ… è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆï¼šquality_report_5000.json")
```

---

## ç¬¬å…­å¤©ï¼ˆ12æœˆ16æ—¥ï¼‰ï¼šæ•°æ®äº¤ä»˜ä¸å¯è§†åŒ– â€” 2å°æ—¶

### æ­¥éª¤4.1ï¼šç”Ÿæˆè®ºæ–‡ç”¨æ•°æ®è¡¨ï¼ˆ30åˆ†é’Ÿï¼‰

```python
# export_for_paper.py - å¯¼å‡ºè®ºæ–‡å¯ç”¨çš„è¡¨æ ¼

import json
import pandas as pd

# è¯»å–LLMåˆ†æç»“æœ
with open('analysis_results_5000.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

results = data['results']

# è½¬æ¢ä¸ºDataFrameï¼ˆä¾¿äºExcelæ“ä½œï¼‰
df = pd.DataFrame([
    {
        'ID': i + 1,
        'åŸå§‹èˆ†è®º': r.get('source_text', ''),
        'æƒ…æ„Ÿ': r.get('sentiment', ''),
        'æ¨¡å¼': r.get('pattern', ''),
        'é£é™©ç±»å‹': r.get('risk_category', ''),
        'ä¸¥é‡æ€§': r.get('risk_severity', ''),
        'èº«ä»½': r.get('taxpayer_identity', ''),
        'è¡Œä¸ºå€¾å‘': r.get('behavioral_intent', ''),
        'å…³é”®æ´å¯Ÿ': r.get('key_insight', ''),
        'ç½®ä¿¡åº¦': r.get('sentiment_confidence', 0),
    }
    for i, r in enumerate(results)
])

# å¯¼å‡ºExcelï¼ˆä¾¿äºè¿›ä¸€æ­¥åˆ†æï¼‰
df.to_excel('opinion_analysis_5000_for_paper.xlsx', index=False)
print("âœ… Excelæ•°æ®å·²å¯¼å‡ºï¼šopinion_analysis_5000_for_paper.xlsx")

# å¯¼å‡ºCSVï¼ˆå¤‡ä»½ï¼‰
df.to_csv('opinion_analysis_5000_for_paper.csv', index=False, encoding='utf-8')
print("âœ… CSVæ•°æ®å·²å¯¼å‡ºï¼šopinion_analysis_5000_for_paper.csv")

# ç”Ÿæˆç»Ÿè®¡æ‘˜è¦è¡¨
summary = pd.DataFrame({
    'åˆ†ç±»ç»´åº¦': ['æƒ…æ„Ÿåˆ†å¸ƒ', 'æ¨¡å¼åˆ†å¸ƒ', 'é£é™©åˆ†å¸ƒ', 'è¡Œä¸ºå€¾å‘'],
    'æ•°æ®': [
        df['æƒ…æ„Ÿ'].value_counts().to_dict(),
        df['æ¨¡å¼'].value_counts().to_dict(),
        df['é£é™©ç±»å‹'].value_counts().to_dict(),
        df['è¡Œä¸ºå€¾å‘'].value_counts().to_dict(),
    ]
})

summary.to_excel('summary_statistics_5000.xlsx', index=False)
print("âœ… ç»Ÿè®¡æ‘˜è¦å·²å¯¼å‡ºï¼šsummary_statistics_5000.xlsx")
```

### æ­¥éª¤4.2ï¼šç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼ˆ60åˆ†é’Ÿï¼‰

```python
# visualize_results.py - ç”Ÿæˆå¯è§†åŒ–å±•ç¤º

import matplotlib.pyplot as plt
import pandas as pd
import json

# è¯»å–æ•°æ®
with open('analysis_results_5000.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

results = data['results']
df = pd.read_excel('opinion_analysis_5000_for_paper.xlsx')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# 1. æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

sentiment_counts = df['æƒ…æ„Ÿ'].value_counts()
axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
axes[0, 0].set_title('æƒ…æ„Ÿåˆ†å¸ƒ')

# 2. æ¨¡å¼åˆ†å¸ƒæŸ±çŠ¶å›¾
pattern_counts = df['æ¨¡å¼'].value_counts().head(6)
axes[0, 1].barh(pattern_counts.index, pattern_counts.values)
axes[0, 1].set_title('ä¸»è¦æ¨¡å¼åˆ†å¸ƒï¼ˆTop 6ï¼‰')

# 3. é£é™©ç±»å‹åˆ†å¸ƒ
risk_counts = df['é£é™©ç±»å‹'].value_counts().head(8)
axes[1, 0].barh(risk_counts.index, risk_counts.values)
axes[1, 0].set_title('é£é™©ç±»å‹åˆ†å¸ƒï¼ˆTop 8ï¼‰')

# 4. ç½®ä¿¡åº¦åˆ†å¸ƒ
axes[1, 1].hist(df['ç½®ä¿¡åº¦'], bins=20, edgecolor='black')
axes[1, 1].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
axes[1, 1].set_xlabel('ç½®ä¿¡åº¦')
axes[1, 1].set_ylabel('é¢‘æ•°')

plt.tight_layout()
plt.savefig('opinion_analysis_visualization.png', dpi=300, bbox_inches='tight')
print("âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆï¼šopinion_analysis_visualization.png")

# ç”ŸæˆHTMLäº¤äº’å¼æŠ¥å‘Š
html_report = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>è·¨å¢ƒç”µå•†èˆ†è®ºåˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1 {{ color: #333; }}
        .stat {{ margin: 20px 0; padding: 10px; background: #f0f0f0; border-left: 4px solid #007bff; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #007bff; color: white; }}
    </style>
</head>
<body>
    <h1>è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºåˆ†ææŠ¥å‘Š</h1>
    <p>å¤„ç†æ—¶é—´ï¼š{data['metadata']['timestamp']}</p>
    <p>æ€»æ ·æœ¬æ•°ï¼š{len(results)}</p>
    
    <div class="stat">
        <h2>æƒ…æ„Ÿåˆ†å¸ƒ</h2>
        {''.join(f"<p>{s}: {c} ({100*c/len(results):.1f}%)</p>" 
                for s, c in df['æƒ…æ„Ÿ'].value_counts().items())}
    </div>
    
    <div class="stat">
        <h2>ä¸»è¦é£é™©ç±»å‹</h2>
        {''.join(f"<p>{r}: {c} ({100*c/len(results):.1f}%)</p>" 
                for r, c in df['é£é™©ç±»å‹'].value_counts().head(8).items())}
    </div>
    
    <h2>å…³é”®æ´å¯Ÿ</h2>
    <ul>
        {''.join(f"<li>{r.get('key_insight', '')}</li>" for r in results[:10])}
    </ul>
    
</body>
</html>
"""

with open('opinion_analysis_report.html', 'w', encoding='utf-8') as f:
    f.write(html_report)

print("âœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆï¼šopinion_analysis_report.html")
```

---

## æˆæœ¬ä¸æ—¶é—´æ€»ç»“

| é¡¹ç›® | æ—¶é—´ | æˆæœ¬ | å¤‡æ³¨ |
|-----|------|------|------|
| **ç¬¬1å¤©ï¼šç¯å¢ƒæ­å»º** | 2å°æ—¶ | Â¥0 | APIæ³¨å†Œ+å®‰è£…åº“ |
| **ç¬¬2å¤©ï¼šæ ·æœ¬æµ‹è¯•** | 3å°æ—¶ | Â¥5 | 100æ¡æµ‹è¯•ï¼ŒéªŒè¯ç²¾åº¦ |
| **ç¬¬3-5å¤©ï¼šå…¨é‡å¤„ç†** | 8å°æ—¶ | Â¥40-60 | 5000æ¡è‡ªåŠ¨åˆ†ç±» |
| **ç¬¬6å¤©ï¼šäº¤ä»˜å¯è§†åŒ–** | 2å°æ—¶ | Â¥0 | å¯¼å‡ºæ•°æ®è¡¨+å›¾è¡¨ |
| **æ€»è®¡** | **15å°æ—¶** | **Â¥45-65** | **å®Œå…¨è‡ªåŠ¨åŒ–** |

---

## é£é™©ä¸åº”å¯¹

| é£é™© | æ¦‚ç‡ | åº”å¯¹ |
|-----|------|------|
| APIé…é¢ä¸è¶³ | ä½ | Geminiæ–°è´¦æˆ·æœ‰å…è´¹é¢åº¦ |
| Promptæ•ˆæœä¸ç†æƒ³ | ä¸­ | ä½¿ç”¨ç¬¬ä¸€éƒ¨åˆ†çš„ç°æˆPrompt |
| ç²¾åº¦ä½äº85% | ä½ | è°ƒæ•´Few-shotä¾‹å­æˆ–é‡æ–°è¿è¡Œ |
| æ•°æ®æ ¼å¼é”™è¯¯ | ä½ | æå‰æ¸…æ´æ•°æ® |
| ç³»ç»Ÿå´©æºƒ | æä½ | åˆ†æ‰¹å¤„ç†ï¼Œå¯æ–­ç‚¹ç»­ä¼  |

---

## ä¸‹ä¸€æ­¥ï¼šè®ºæ–‡é›†æˆ

å®ŒæˆLLMåˆ†æåï¼ˆ12æœˆ20æ—¥å‰ï¼‰ï¼Œä½ å¯ä»¥ç›´æ¥ç”¨è¿™5000æ¡çš„ç»“æ„åŒ–æ•°æ®ï¼š

1. **Part A ï¼ˆDIDåˆ†æï¼‰**ï¼š
   - çˆ¬è™«ä»·æ ¼æ•°æ® âœ“ ï¼ˆä½ åœ¨åšï¼‰
   - èˆ†è®ºæ•°æ® âœ“ ï¼ˆLLMå®Œæˆï¼‰
   - å¯¹æ¯”åˆ†æï¼šæ”¿ç­–å‰åçš„ä»·æ ¼å˜åŒ–vsèˆ†è®ºå˜åŒ–

2. **Part B ï¼ˆèˆ†è®ºåˆ†æï¼‰**ï¼š
   - ç›´æ¥ç”¨LLMç»“æœç”Ÿæˆè¡¨æ ¼å’Œå›¾è¡¨
   - æŒ‰ç…§æ¨¡å¼/é£é™©/è¡Œä¸ºç»´åº¦åˆ†æ
   - å†™å‡º"æ¶ˆè´¹è€…å¦‚ä½•å“åº”æ”¿ç­–"çš„æ•…äº‹

3. **æ–¹æ³•è®ºè¯´æ˜**ï¼š
   - ç¬¬ä¸€éƒ¨åˆ†ï¼šLLMç³»ç»Ÿè®¾è®¡
   - ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ¤æ–­é€»è¾‘æ¼”ç¤º
   - è®ºæ–‡ä¸­ï¼šç®€å•ä¸€å¥è¯"ä½¿ç”¨LangExtract + Gemini 2.5-flashåˆ†æ"
   - é™„å½•ï¼šFew-shotä¾‹å­å’Œéƒ¨åˆ†ç»“æœ

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿæ˜å¤©ï¼ˆ12æœˆ11æ—¥ï¼‰å°±å¯ä»¥å¼€å§‹ã€‚**

é¢„è®¡12æœˆ20æ—¥å‰æ‰€æœ‰5000æ¡èˆ†è®ºçš„ç»“æ„åŒ–æ•°æ®å°±èƒ½äº¤ä»˜åˆ°è®ºæ–‡ä¸­ã€‚
