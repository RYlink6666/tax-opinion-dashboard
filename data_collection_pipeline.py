# -*- coding: utf-8 -*-
"""
è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºæ•°æ®é‡‡é›†ç®¡é“ (Pipeline)
ä¸€ä¸ªè„šæœ¬å®Œæˆæ‰€æœ‰é‡‡é›†å·¥ä½œï¼šçˆ¬è™« â†’ æ¸…æ´ â†’ è¾“å‡º

ä½¿ç”¨æ–¹å¼ï¼š
    python data_collection_pipeline.py

é¢„æœŸäº§å‡ºï¼š
    âœ… opinions_clean_5000.txt (æœ€ç»ˆæ ¼å¼ï¼Œç”¨äºLLMåˆ†æ)
    âœ… opinions_clean_5000.json
    âœ… opinions_clean_5000.csv
"""

import sys
import os
import json
import time
import requests
import random
from datetime import datetime
from pathlib import Path
import csv

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

# å°è¯•å¯¼å…¥pandasï¼Œå¦‚æœæ²¡æœ‰åˆ™åç»­å®‰è£…
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

from bs4 import BeautifulSoup


class PipelineConfig:
    """é…ç½®å‚æ•°"""
    
    # å¾®åšé…ç½®
    WEIBO_KEYWORDS = [
        '0110é¦™æ¸¯å…¬å¸',
        '9610å¤‡æ¡ˆ',
        '9710B2B',
        '9810æµ·å¤–ä»“',
        '1039å¸‚åœºé‡‡è´­',
        'Temuå…¨æ‰˜ç®¡',
        'è·¨å¢ƒç”µå•†å¢å€¼ç¨',
        'è·¨å¢ƒç”µå•†ç¨æ”¶',
        'è·¨å¢ƒç”µå•†è¡¥ç¨',
    ]
    WEIBO_PAGES = 2  # æ¯ä¸ªå…³é”®è¯é‡‡é›†é¡µæ•°
    
    # çŸ¥ä¹é…ç½®
    ZHIHU_KEYWORDS = [
        'è·¨å¢ƒç”µå•†å¢å€¼ç¨',
        'è·¨å¢ƒç”µå•†ç¨æ”¶æ”¿ç­–',
        '9610æµ·å…³ç¼–ç ',
        '1039å¸‚åœºé‡‡è´­',
        'Temuç¨åŠ¡',
        'é¦™æ¸¯å…¬å¸ç¨æ”¶å±…æ°‘',
        'æµ·å¤–ä»“æŠ¥å…³',
        'ç”µå•†è¡¥ç¨',
    ]
    ZHIHU_PAGES = 2
    
    # è¾“å‡ºé…ç½®
    WEIBO_RAW_FILE = 'weibo_raw_data.json'
    ZHIHU_RAW_FILE = 'zhihu_raw_data.json'
    FINAL_TXT_FILE = 'opinions_clean_5000.txt'
    FINAL_JSON_FILE = 'opinions_clean_5000.json'
    FINAL_CSV_FILE = 'opinions_clean_5000.csv'


class Logger:
    """æ—¥å¿—è®°å½•"""
    
    @staticmethod
    def info(msg):
        """æ™®é€šä¿¡æ¯"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] â„¹ï¸  {msg}")
    
    @staticmethod
    def success(msg):
        """æˆåŠŸ"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âœ… {msg}")
    
    @staticmethod
    def warning(msg):
        """è­¦å‘Š"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âš ï¸  {msg}")
    
    @staticmethod
    def error(msg):
        """é”™è¯¯"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] âŒ {msg}")
    
    @staticmethod
    def section(title):
        """ç« èŠ‚æ ‡é¢˜"""
        print("\n" + "=" * 70)
        print(f"  {title}")
        print("=" * 70 + "\n")


class WeiboCollector:
    """å¾®åšæ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.posts = []
        self.session = self._create_session()
    
    def _create_session(self):
        """åˆ›å»ºè¯·æ±‚ä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self._random_user_agent()
        })
        return session
    
    @staticmethod
    def _random_user_agent():
        """éšæœºUser-Agent"""
        agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        return random.choice(agents)
    
    def collect(self, keyword, num_pages=2):
        """é‡‡é›†å•ä¸ªå…³é”®è¯çš„å¾®åš"""
        Logger.info(f"é‡‡é›†å¾®åšï¼š{keyword}")
        
        for page in range(1, num_pages + 1):
            try:
                url = f"https://s.weibo.com/weibo?q={keyword}&typeall=1&suball=1&page={page}"
                
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    Logger.warning(f"  é¡µé¢ {page} è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                posts = soup.find_all('div', class_='mbrank')
                
                count = 0
                for post in posts:
                    try:
                        text_elem = post.find('p', class_='txt')
                        if not text_elem:
                            continue
                        
                        text = text_elem.get_text(strip=True)
                        
                        if len(text) < 20:
                            continue
                        
                        # è¿‡æ»¤åƒåœ¾
                        if any(kw in text for kw in ['æ¨å¹¿', 'å¹¿å‘Š', 'é“¾æ¥']):
                            continue
                        
                        self.posts.append({
                            'platform': 'weibo',
                            'keyword': keyword,
                            'text': text[:500],
                            'collected_at': datetime.now().isoformat()
                        })
                        count += 1
                    except:
                        continue
                
                Logger.info(f"  é¡µé¢ {page}ï¼šé‡‡é›† {count} æ¡")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                Logger.warning(f"  é¡µé¢ {page} å‡ºé”™ï¼š{str(e)}")
                time.sleep(random.uniform(3, 5))
        
        return len(self.posts)
    
    def run(self):
        """æ‰§è¡Œå¾®åšé‡‡é›†"""
        Logger.section("ğŸ“± ç¬¬1æ­¥ï¼šé‡‡é›†å¾®åšæ•°æ®")
        
        total_before = len(self.posts)
        
        for keyword in PipelineConfig.WEIBO_KEYWORDS:
            self.collect(keyword, num_pages=PipelineConfig.WEIBO_PAGES)
        
        total_after = len(self.posts)
        new_count = total_after - total_before
        
        Logger.success(f"å¾®åšé‡‡é›†å®Œæˆï¼šå…± {new_count} æ¡")
        
        return self.posts
    
    def save(self, filename=None):
        """ä¿å­˜æ•°æ®"""
        if not filename:
            filename = PipelineConfig.WEIBO_RAW_FILE
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.posts, f, ensure_ascii=False, indent=2)
        
        Logger.success(f"å·²ä¿å­˜åˆ° {filename}")
        return filename


class ZhihuCollector:
    """çŸ¥ä¹æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.posts = []
        self.session = self._create_session()
    
    def _create_session(self):
        """åˆ›å»ºè¯·æ±‚ä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': self._random_user_agent()
        })
        return session
    
    @staticmethod
    def _random_user_agent():
        """éšæœºUser-Agent"""
        agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        ]
        return random.choice(agents)
    
    def collect(self, keyword, num_pages=2):
        """é‡‡é›†çŸ¥ä¹"""
        Logger.info(f"é‡‡é›†çŸ¥ä¹ï¼š{keyword}")
        
        for page in range(1, num_pages + 1):
            try:
                url = f"https://www.zhihu.com/search?type=content&q={keyword}&page={page}"
                
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    Logger.warning(f"  é¡µé¢ {page} è¯·æ±‚å¤±è´¥")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å†…å®¹å…ƒç´ 
                items = soup.find_all('div', attrs={'class': lambda x: x and 'SearchResult' in (x or '')})
                
                count = 0
                for item in items:
                    try:
                        title_elem = item.find('h2')
                        content_elem = item.find('p')
                        
                        title = title_elem.get_text(strip=True) if title_elem else ""
                        content = content_elem.get_text(strip=True) if content_elem else ""
                        
                        text = f"{title} {content}".strip()
                        
                        if len(text) < 20:
                            continue
                        
                        if any(kw in text for kw in ['æ¨å¹¿', 'å¹¿å‘Š']):
                            continue
                        
                        self.posts.append({
                            'platform': 'zhihu',
                            'keyword': keyword,
                            'text': text[:500],
                            'collected_at': datetime.now().isoformat()
                        })
                        count += 1
                    except:
                        continue
                
                Logger.info(f"  é¡µé¢ {page}ï¼šé‡‡é›† {count} æ¡")
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                Logger.warning(f"  é¡µé¢ {page} å‡ºé”™ï¼š{str(e)}")
                time.sleep(random.uniform(3, 5))
        
        return len(self.posts)
    
    def run(self):
        """æ‰§è¡ŒçŸ¥ä¹é‡‡é›†"""
        Logger.section("ğŸ’¡ ç¬¬2æ­¥ï¼šé‡‡é›†çŸ¥ä¹æ•°æ®")
        
        total_before = len(self.posts)
        
        for keyword in PipelineConfig.ZHIHU_KEYWORDS:
            self.collect(keyword, num_pages=PipelineConfig.ZHIHU_PAGES)
        
        total_after = len(self.posts)
        new_count = total_after - total_before
        
        Logger.success(f"çŸ¥ä¹é‡‡é›†å®Œæˆï¼šå…± {new_count} æ¡")
        
        return self.posts
    
    def save(self, filename=None):
        """ä¿å­˜æ•°æ®"""
        if not filename:
            filename = PipelineConfig.ZHIHU_RAW_FILE
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.posts, f, ensure_ascii=False, indent=2)
        
        Logger.success(f"å·²ä¿å­˜åˆ° {filename}")
        return filename


class DataCleaner:
    """æ•°æ®æ¸…æ´å™¨"""
    
    @staticmethod
    def clean_text(text):
        """æ¸…æ´æ–‡æœ¬"""
        if not isinstance(text, str):
            return ""
        
        text = ' '.join(text.split())
        text = ''.join(c for c in text if c.isprintable())
        text = text[:500].strip()
        
        return text
    
    @staticmethod
    def is_valid(text):
        """åˆ¤æ–­æ˜¯å¦æœ‰æ•ˆ"""
        if not text or len(text) < 10 or len(text) > 500:
            return False
        
        # æ£€æŸ¥ä¸­æ–‡æ¯”ä¾‹
        chinese_count = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        if chinese_count < len(text) * 0.3:
            return False
        
        return True
    
    def clean_and_deduplicate(self, all_posts):
        """æ¸…æ´å’Œå»é‡"""
        Logger.section("ğŸ§¹ ç¬¬3æ­¥ï¼šæ•°æ®æ¸…æ´å’Œå»é‡")
        
        seen = set()
        cleaned = []
        duplicates = 0
        invalid = 0
        
        Logger.info(f"å¼€å§‹å¤„ç† {len(all_posts)} æ¡åŸå§‹æ•°æ®...")
        
        for post in all_posts:
            text = post.get('text', '') if isinstance(post, dict) else str(post)
            text = self.clean_text(text)
            
            if not self.is_valid(text):
                invalid += 1
                continue
            
            if text in seen:
                duplicates += 1
                continue
            
            seen.add(text)
            cleaned.append(text)
        
        Logger.info(f"åŸå§‹æ¡æ•°ï¼š{len(all_posts)}")
        Logger.info(f"å·²æ¸…æ´ï¼š{len(cleaned)}")
        Logger.info(f"é‡å¤ç§»é™¤ï¼š{duplicates}")
        Logger.info(f"æ— æ•ˆç§»é™¤ï¼š{invalid}")
        Logger.success(f"æœ€ç»ˆæ¡æ•°ï¼š{len(cleaned)}")
        
        return cleaned
    
    @staticmethod
    def save_txt(texts, filename):
        """ä¿å­˜ä¸ºTXT"""
        with open(filename, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + '\n')
        Logger.success(f"TXT å·²ä¿å­˜åˆ° {filename}")
    
    @staticmethod
    def save_json(texts, filename):
        """ä¿å­˜ä¸ºJSON"""
        data = {
            'metadata': {
                'total': len(texts),
                'created_at': datetime.now().isoformat()
            },
            'data': texts
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        Logger.success(f"JSON å·²ä¿å­˜åˆ° {filename}")
    
    @staticmethod
    def save_csv(texts, filename):
        """ä¿å­˜ä¸ºCSV"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'text'])
            for i, text in enumerate(texts, 1):
                writer.writerow([i, text])
        Logger.success(f"CSV å·²ä¿å­˜åˆ° {filename}")
    
    def save_all_formats(self, texts):
        """ä¿å­˜ä¸ºæ‰€æœ‰æ ¼å¼"""
        self.save_txt(texts, PipelineConfig.FINAL_TXT_FILE)
        self.save_json(texts, PipelineConfig.FINAL_JSON_FILE)
        self.save_csv(texts, PipelineConfig.FINAL_CSV_FILE)
    
    @staticmethod
    def quality_report(texts):
        """è´¨é‡æŠ¥å‘Š"""
        Logger.section("ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š")
        
        if not texts:
            Logger.error("æ— æ•°æ®")
            return
        
        lengths = [len(t) for t in texts]
        
        print(f"æ€»æ¡æ•°ï¼š          {len(texts):,}")
        print(f"å¹³å‡é•¿åº¦ï¼š        {sum(lengths) / len(lengths):.0f} å­—ç¬¦")
        print(f"æœ€çŸ­ï¼š            {min(lengths)} å­—ç¬¦")
        print(f"æœ€é•¿ï¼š            {max(lengths)} å­—ç¬¦")
        print(f"ä¸­ä½æ•°ï¼š          {sorted(lengths)[len(lengths)//2]} å­—ç¬¦")
        
        print("\nğŸ“Œ éšæœºæŠ½æ ·ï¼ˆå‰5æ¡ï¼‰ï¼š")
        for i, text in enumerate(texts[:5], 1):
            preview = text[:70] + "..." if len(text) > 70 else text
            print(f"  {i}. {preview}")
        
        print("\nâœ… æ•°æ®è´¨é‡ï¼šåˆæ ¼ï¼ˆå·²å‡†å¤‡å¥½ç”¨äºLLMåˆ†æï¼‰")


class DataCollectionPipeline:
    """æ•°æ®é‡‡é›†ä¸»ç®¡é“"""
    
    def __init__(self):
        self.weibo_collector = WeiboCollector()
        self.zhihu_collector = ZhihuCollector()
        self.cleaner = DataCleaner()
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´ç®¡é“"""
        print("\n")
        print("â•”" + "=" * 68 + "â•—")
        print("â•‘" + " " * 12 + "ğŸš€ è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºæ•°æ®é‡‡é›†ç®¡é“" + " " * 22 + "â•‘")
        print("â•‘" + " " * 20 + "ä¸€é”®å®Œæˆï¼šçˆ¬è™« â†’ æ¸…æ´ â†’ è¾“å‡º" + " " * 20 + "â•‘")
        print("â•š" + "=" * 68 + "â•\n")
        
        try:
            # ç¬¬1æ­¥ï¼šé‡‡é›†å¾®åš
            weibo_posts = self.weibo_collector.run()
            self.weibo_collector.save()
            
            # ç¬¬2æ­¥ï¼šé‡‡é›†çŸ¥ä¹
            zhihu_posts = self.zhihu_collector.run()
            self.zhihu_collector.save()
            
            # åˆå¹¶
            all_posts = weibo_posts + zhihu_posts
            Logger.section("ğŸ“¦ æ•°æ®åˆå¹¶")
            Logger.success(f"åˆå¹¶å®Œæˆï¼šå¾®åš {len(weibo_posts)} + çŸ¥ä¹ {len(zhihu_posts)} = {len(all_posts)} æ¡")
            
            # ç¬¬3æ­¥ï¼šæ¸…æ´
            cleaned_texts = self.cleaner.clean_and_deduplicate(all_posts)
            
            # ç¬¬4æ­¥ï¼šä¿å­˜
            Logger.section("ğŸ’¾ ä¿å­˜æ•°æ®")
            self.cleaner.save_all_formats(cleaned_texts)
            
            # ç¬¬5æ­¥ï¼šæŠ¥å‘Š
            self.cleaner.quality_report(cleaned_texts)
            
            # æœ€ç»ˆæ€»ç»“
            self._final_summary(cleaned_texts)
            
        except KeyboardInterrupt:
            Logger.warning("ç”¨æˆ·ä¸­æ–­äº†é‡‡é›†")
        except Exception as e:
            Logger.error(f"é‡‡é›†å¤±è´¥ï¼š{str(e)}")
            raise
    
    @staticmethod
    def _final_summary(texts):
        """æœ€ç»ˆæ€»ç»“"""
        Logger.section("âœ¨ é‡‡é›†å®Œæˆæ€»ç»“")
        
        if len(texts) >= 4800:
            status = "âœ… ä¼˜ç§€"
        elif len(texts) >= 3000:
            status = "âš ï¸  è­¦å‘Š"
        else:
            status = "âŒ å¤±è´¥"
        
        print(f"çŠ¶æ€ï¼š              {status}")
        print(f"æœ€ç»ˆæ•°æ®é‡ï¼š        {len(texts):,} æ¡")
        print(f"ç›®æ ‡ï¼š              5000 æ¡")
        print(f"å®Œæˆåº¦ï¼š            {len(texts) / 5000 * 100:.1f}%")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š")
        print(f"  â€¢ {PipelineConfig.FINAL_TXT_FILE} (ç”¨äºLLMåˆ†æ)")
        print(f"  â€¢ {PipelineConfig.FINAL_JSON_FILE}")
        print(f"  â€¢ {PipelineConfig.FINAL_CSV_FILE}")
        
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥ï¼š")
        if len(texts) >= 4800:
            print(f"  âœ… æ•°æ®å……è¶³ï¼Œå¯å¼€å§‹ LLM åˆ†æ")
            print(f"     è§ï¼šSTEP_2_LangExtractå®Œæ•´åˆ†æè®¡åˆ’.md")
        else:
            print(f"  âš ï¸  æ•°æ®ä¸è¶³ï¼Œéœ€è¦è¡¥å……é‡‡é›†")
            print(f"     æ–¹æ¡ˆï¼šä¼—åŒ…æˆ–å¢åŠ çˆ¬è™«é¡µæ•°")
        
        print("\n" + "=" * 70 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        from bs4 import BeautifulSoup
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·è¿è¡Œï¼š")
        print("   pip install requests beautifulsoup4 pandas")
        sys.exit(1)
    
    # è¿è¡Œç®¡é“
    pipeline = DataCollectionPipeline()
    pipeline.run()


if __name__ == "__main__":
    main()
