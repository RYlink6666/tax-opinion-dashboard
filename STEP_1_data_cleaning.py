# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´å’Œåˆå¹¶è„šæœ¬
1. å»é‡
2. è¿‡æ»¤ä½è´¨é‡æ•°æ®
3. ç»Ÿä¸€æ ¼å¼
4. ç”Ÿæˆæœ€ç»ˆçš„ opinions_clean_5000.txt
"""

import json
import pandas as pd
import glob
import os
from pathlib import Path

class DataCleaner:
    """æ•°æ®æ¸…æ´ç±»"""
    
    def __init__(self):
        self.all_texts = set()  # ç”¨setåšå»é‡
        self.all_posts = []
    
    def load_json_files(self, pattern='*_raw_data.json'):
        """åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½JSONæ–‡ä»¶...")
        
        files = glob.glob(pattern)
        if not files:
            print(f"   âŒ æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„æ–‡ä»¶")
            return
        
        for file in files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        self.all_posts.extend(data)
                        print(f"   âœ“ {file}: {len(data)} æ¡")
                    else:
                        print(f"   âš ï¸  {file} æ ¼å¼éåˆ—è¡¨ï¼Œè·³è¿‡")
            except Exception as e:
                print(f"   âŒ {file}: {str(e)}")
    
    def load_csv_files(self, pattern='*_raw_data.csv'):
        """åŠ è½½æ‰€æœ‰CSVæ–‡ä»¶"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½CSVæ–‡ä»¶...")
        
        files = glob.glob(pattern)
        if not files:
            print(f"   â„¹ï¸  æœªæ‰¾åˆ°åŒ¹é… '{pattern}' çš„æ–‡ä»¶")
            return
        
        for file in files:
            try:
                df = pd.read_csv(file, encoding='utf-8')
                # è½¬ä¸ºå­—å…¸åˆ—è¡¨
                posts = df.to_dict('records')
                self.all_posts.extend(posts)
                print(f"   âœ“ {file}: {len(df)} æ¡")
            except Exception as e:
                print(f"   âŒ {file}: {str(e)}")
    
    def clean_text(self, text):
        """æ¸…æ´å•æ¡æ–‡æœ¬"""
        if not isinstance(text, str):
            return ""
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ§åˆ¶ç¬¦
        text = ''.join(c for c in text if c.isprintable())
        
        # æˆªæ–­åˆ°500å­—ï¼ˆèˆ†è®ºé€šå¸¸ä¸å¤ªé•¿ï¼‰
        text = text[:500].strip()
        
        return text
    
    def is_valid_post(self, text):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„èˆ†è®º"""
        if not text:
            return False
        
        # é•¿åº¦æ£€æŸ¥
        if len(text) < 10 or len(text) > 500:
            return False
        
        # ä¸­æ–‡å†…å®¹æ£€æŸ¥ï¼ˆè‡³å°‘50%ä¸­æ–‡ï¼‰
        chinese_count = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        if chinese_count < len(text) * 0.4:
            return False
        
        # åƒåœ¾å†…å®¹è¿‡æ»¤
        spam_keywords = [
            'æ¨å¹¿', 'å¹¿å‘Š', 'ç‚¹å‡»', 'å…³æ³¨', 'è½¬å‘', 'åˆ†äº«',
            'è´­ä¹°', 'é“¾æ¥', 'æ‰«ç ', 'ä¸‹è½½', 'å®‰è£…',
            'http', 'www', '.com', '.cn',  # URL
            'ğŸŒŸ', 'ğŸ’', 'ğŸ”¥', 'ğŸ’°',  # è¿‡å¤šemoji
        ]
        if any(kw in text for kw in spam_keywords):
            return False
        
        return True
    
    def clean_and_deduplicate(self):
        """æ¸…æ´å’Œå»é‡"""
        print("\nğŸ§¹ æ­£åœ¨æ¸…æ´æ•°æ®...")
        
        cleaned_texts = []
        duplicates = 0
        invalid = 0
        
        for post in self.all_posts:
            # æå–æ–‡æœ¬
            if isinstance(post, dict):
                text = post.get('text', '')
            else:
                text = str(post)
            
            # æ¸…æ´
            text = self.clean_text(text)
            
            # æœ‰æ•ˆæ€§æ£€æŸ¥
            if not self.is_valid_post(text):
                invalid += 1
                continue
            
            # å»é‡
            if text in self.all_texts:
                duplicates += 1
                continue
            
            self.all_texts.add(text)
            cleaned_texts.append(text)
        
        print(f"   âœ“ åŸå§‹æ¡æ•°ï¼š{len(self.all_posts)}")
        print(f"   âœ“ å·²æ¸…æ´ï¼š{len(cleaned_texts)}")
        print(f"   âœ“ é‡å¤ç§»é™¤ï¼š{duplicates}")
        print(f"   âœ“ æ— æ•ˆç§»é™¤ï¼š{invalid}")
        print(f"   âœ“ æœ€ç»ˆæ¡æ•°ï¼š{len(cleaned_texts)}")
        
        return cleaned_texts
    
    def save_txt(self, texts, filename='opinions_clean_5000.txt'):
        """ä¿å­˜ä¸ºTXTï¼ˆæ¯è¡Œä¸€æ¡ï¼‰"""
        with open(filename, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + '\n')
        print(f"\nâœ… å·²ä¿å­˜åˆ° {filename}")
        return filename
    
    def save_json(self, texts, filename='opinions_clean_5000.json'):
        """ä¿å­˜ä¸ºJSON"""
        data = {
            'metadata': {
                'total': len(texts),
                'format': 'list of strings'
            },
            'data': texts
        }
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜åˆ° {filename}")
        return filename
    
    def save_csv(self, texts, filename='opinions_clean_5000.csv'):
        """ä¿å­˜ä¸ºCSV"""
        df = pd.DataFrame({
            'id': range(1, len(texts) + 1),
            'text': texts
        })
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"âœ… å·²ä¿å­˜åˆ° {filename}")
        return filename
    
    def quality_report(self, texts):
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š")
        print("=" * 60)
        
        if not texts:
            print("âŒ æ— æ•°æ®")
            return
        
        lengths = [len(t) for t in texts]
        
        print(f"æ€»æ¡æ•°ï¼š{len(texts)}")
        print(f"å¹³å‡é•¿åº¦ï¼š{sum(lengths) / len(lengths):.0f} å­—ç¬¦")
        print(f"æœ€çŸ­ï¼š{min(lengths)} å­—ç¬¦")
        print(f"æœ€é•¿ï¼š{max(lengths)} å­—ç¬¦")
        print(f"ä¸­ä½æ•°ï¼š{sorted(lengths)[len(lengths)//2]} å­—ç¬¦")
        
        # éšæœºæŠ½æ ·
        print("\nğŸ“Œ éšæœºæŠ½æ ·ï¼ˆå‰10æ¡ï¼‰ï¼š")
        for i, text in enumerate(texts[:10], 1):
            preview = text[:60] + "..." if len(text) > 60 else text
            print(f"{i:2d}. {preview}")
        
        print("=" * 60)
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æ¸…æ´æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹æ•°æ®æ¸…æ´æµç¨‹")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        self.load_json_files()
        self.load_csv_files()
        
        if not self.all_posts:
            print("\nâŒ æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼")
            print("   è¯·ç¡®ä¿å·²è¿è¡Œ STEP_1_weibo_spider.py å’Œ STEP_1_zhihu_spider.py")
            return
        
        # æ¸…æ´
        cleaned_texts = self.clean_and_deduplicate()
        
        if len(cleaned_texts) < 100:
            print(f"\nâš ï¸  è­¦å‘Šï¼šæ¸…æ´åä»… {len(cleaned_texts)} æ¡æ•°æ®ï¼Œå¯èƒ½ä¸è¶³")
            print("   å»ºè®®é‡æ–°è¿è¡Œçˆ¬è™«æˆ–æ£€æŸ¥æ•°æ®è´¨é‡")
        
        # ä¿å­˜
        self.save_txt(cleaned_texts)
        self.save_json(cleaned_texts)
        self.save_csv(cleaned_texts)
        
        # æŠ¥å‘Š
        self.quality_report(cleaned_texts)
        
        return cleaned_texts


if __name__ == "__main__":
    cleaner = DataCleaner()
    cleaned = cleaner.run()
