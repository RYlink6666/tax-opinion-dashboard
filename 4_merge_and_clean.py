"""
æ•°æ®åˆå¹¶ä¸æ¸…æ´è„šæœ¬
å°†æ‰€æœ‰å¹³å°çš„åŸå§‹æ•°æ®åˆå¹¶ã€å»é‡ã€æ¸…æ´

ä½¿ç”¨æ–¹æ³•ï¼š
    python 4_merge_and_clean.py

è¾“å…¥ï¼š
    data/raw/weibo/*.json
    data/raw/zhihu/*.json
    data/raw/xiaohongshu/*.json

è¾“å‡ºï¼š
    data/clean/opinions_clean_5000.txt
    data/clean/opinions_clean_5000.json
"""

import json
import logging
import hashlib
from pathlib import Path
from typing import List, Dict, Set
from collections import defaultdict

import config

# ============================================================================
# æ—¥å¿—è®¾ç½®
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler(config.LOGS_DIR / "clean.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# æ•°æ®æ¸…æ´å™¨
# ============================================================================

class DataCleaner:
    """æ•°æ®æ¸…æ´å’Œå»é‡"""
    
    def __init__(self):
        self.all_data = []
        self.dedup_hashes = set()
        self.stats = {
            "total_raw": 0,
            "after_dedup": 0,
            "after_filter_length": 0,
            "after_filter_ads": 0,
            "final": 0
        }
    
    def load_raw_data(self) -> List[Dict]:
        """ä»æ‰€æœ‰å¹³å°åŠ è½½åŸå§‹æ•°æ®"""
        logger.info("ã€ç¬¬1æ­¥ã€‘åŠ è½½åŸå§‹æ•°æ®")
        
        all_files = []
        for platform_dir in [config.WEIBO_RAW_DIR, config.ZHIHU_RAW_DIR, config.XIAOHONGSHU_RAW_DIR]:
            json_files = list(platform_dir.glob("*.json"))
            all_files.extend(json_files)
            logger.info(f"  {platform_dir.name}: {len(json_files)} ä¸ªæ–‡ä»¶")
        
        if not all_files:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•åŸå§‹æ•°æ®æ–‡ä»¶ï¼")
            logger.error(f"   æ£€æŸ¥æ˜¯å¦è¿è¡Œäº†çˆ¬è™«è„šæœ¬ (1_crawl_weibo...)")
            return []
        
        # åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in all_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # å¤„ç†ä¸åŒçš„JSONæ ¼å¼
                    if isinstance(data, list):
                        self.all_data.extend(data)
                    else:
                        # å­—å…¸æ ¼å¼ï¼šå°è¯•æå–dataå­—æ®µæˆ–æ ¹å­—æ®µ
                        if isinstance(data, dict):
                            if "data" in data:
                                items = data["data"]
                                if isinstance(items, list):
                                    self.all_data.extend(items)
                                else:
                                    self.all_data.append(items)
                            else:
                                # ç›´æ¥ä½œä¸ºå•ä¸ªitem
                                self.all_data.append(data)
                
                logger.info(f"  âœ“ å·²åŠ è½½ {json_file.name} ({len(self.all_data)} æ¡æ•°æ®)")
            except Exception as e:
                logger.warning(f"  âœ— åŠ è½½å¤±è´¥ {json_file.name}: {e}")
        
        self.stats["total_raw"] = len(self.all_data)
        logger.info(f"âœ… æ€»å…±åŠ è½½ {len(self.all_data)} æ¡åŸå§‹æ•°æ®\n")
        
        return self.all_data
    
    def deduplicate(self) -> List[Dict]:
        """å»é‡"""
        logger.info("ã€ç¬¬2æ­¥ã€‘å»é‡å¤„ç†")
        
        unique_data = []
        dedup_hashes = set()
        
        for item in self.all_data:
            # è·å–å†…å®¹æ–‡æœ¬
            content = item.get("content", "")
            if not content:
                content = item.get("text", "")
            
            # è®¡ç®—å“ˆå¸Œ
            content_hash = self._hash_content(content)
            
            # æ£€æŸ¥é‡å¤
            if content_hash not in dedup_hashes:
                unique_data.append(item)
                dedup_hashes.add(content_hash)
        
        removed = len(self.all_data) - len(unique_data)
        self.stats["after_dedup"] = len(unique_data)
        
        logger.info(f"  å»é‡å‰ï¼š{len(self.all_data)} æ¡")
        logger.info(f"  å»é‡åï¼š{len(unique_data)} æ¡")
        logger.info(f"  åˆ é™¤ï¼š{removed} æ¡é‡å¤ ({100*removed/len(self.all_data):.1f}%)\n")
        
        return unique_data
    
    def filter_by_length(self, data: List[Dict]) -> List[Dict]:
        """æŒ‰é•¿åº¦è¿‡æ»¤"""
        logger.info("ã€ç¬¬3æ­¥ã€‘é•¿åº¦è¿‡æ»¤")
        
        min_len = config.CLEAN_CONFIG["min_length"]
        max_len = config.CLEAN_CONFIG["max_length"]
        
        filtered = []
        
        for item in data:
            content = item.get("content", "")
            if not content:
                content = item.get("text", "")
            
            # é•¿åº¦æ£€æŸ¥
            if len(content) < min_len:
                continue
            
            # æˆªæ–­
            if len(content) > max_len:
                content = content[:max_len]
                item["content"] = content
            
            filtered.append(item)
        
        removed = len(data) - len(filtered)
        self.stats["after_filter_length"] = len(filtered)
        
        logger.info(f"  é•¿åº¦èŒƒå›´ï¼š{min_len}-{max_len} å­—ç¬¦")
        logger.info(f"  è¿‡æ»¤åï¼š{len(filtered)} æ¡")
        logger.info(f"  åˆ é™¤ï¼š{removed} æ¡\n")
        
        return filtered
    
    def filter_ads_and_spam(self, data: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤å¹¿å‘Šå’Œåƒåœ¾ä¿¡æ¯"""
        logger.info("ã€ç¬¬4æ­¥ã€‘å¹¿å‘Šå’Œåƒåœ¾è¿‡æ»¤")
        
        # å¹¿å‘Šç‰¹å¾è¯
        ad_keywords = [
            "è´­ä¹°", "ç‚¹å‡»è¿™é‡Œ", "æ‰«ç ", "è”ç³»æˆ‘", "å¾®ä¿¡å·",
            "å¯ä»¥èµšé’±", "æ—¥èµš", "æœˆå…¥", "åŒ…é‚®", "é™æ—¶",
            "ç‚¹ä¸€ä¸‹", "é•¿æŒ‰è¯†åˆ«", "ç‚¹å‡»é“¾æ¥", "é¢†ä¼˜æƒ ",
            "ä»£ç†", "åŠ ç›Ÿ", "æŠ•èµ„", "è¿”åˆ©"
        ]
        
        filtered = []
        
        for item in data:
            content = item.get("content", "")
            if not content:
                content = item.get("text", "")
            
            # æ£€æŸ¥å¹¿å‘Šç‰¹å¾
            is_ad = False
            for ad_kw in ad_keywords:
                if ad_kw in content:
                    # è¿‡äºæ˜æ˜¾çš„å¹¿å‘Š
                    if content.count(ad_kw) > 1 or len(content) < 20:
                        is_ad = True
                        break
            
            if not is_ad:
                filtered.append(item)
        
        removed = len(data) - len(filtered)
        self.stats["after_filter_ads"] = len(filtered)
        
        logger.info(f"  è¿‡æ»¤å‰ï¼š{len(data)} æ¡")
        logger.info(f"  è¿‡æ»¤åï¼š{len(filtered)} æ¡")
        logger.info(f"  åˆ é™¤ï¼š{removed} æ¡å¹¿å‘Š\n")
        
        return filtered
    
    def normalize_content(self, data: List[Dict]) -> List[Dict]:
        """è§„èŒƒåŒ–å†…å®¹"""
        logger.info("ã€ç¬¬5æ­¥ã€‘å†…å®¹è§„èŒƒåŒ–")
        
        import re
        normalized = []
        
        for item in data:
            # è·å–å†…å®¹ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
            content = item.get("content", "")
            if not content:
                content = item.get("text", "")
            if not content:
                content = item.get("desc", "")  # å°çº¢ä¹¦çš„descriptionå­—æ®µ
            if not content:
                content = item.get("title", "")  # å°çº¢ä¹¦çš„titleå­—æ®µ
            
            # è·³è¿‡ç©ºå†…å®¹
            if not content:
                continue
            
            # ç§»é™¤URLï¼ˆå¦‚éœ€è¦ï¼‰
            if config.CLEAN_CONFIG.get("remove_urls", True):
                content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)
            
            # ç§»é™¤emojiå’Œç‰¹æ®Šç¬¦å·
            if config.CLEAN_CONFIG.get("remove_emojis", True):
                # ç§»é™¤emoji
                content = re.sub(r'[\U0001F300-\U0001F9FF]|[\u2600-\u27BF]', '', content)
                # ç§»é™¤[xxx]å½¢å¼çš„æ ‡ç­¾
                content = re.sub(r'\[.*?\]', '', content)
            
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            content = ' '.join(content.split())
            
            # æ ‡å‡†åŒ–å­—æ®µ
            clean_item = {
                "platform": item.get("platform", "xiaohongshu"),  # é»˜è®¤å°çº¢ä¹¦
                "content": content,
                "keywords": item.get("keyword", "") or item.get("tag_list", "") or item.get("source_keyword", ""),
                "source_url": item.get("source_url", "") or item.get("note_url", ""),
                "crawl_time": item.get("crawl_time", "") or item.get("time", "")
            }
            
            normalized.append(clean_item)
        
        logger.info(f"âœ… è§„èŒƒåŒ–å®Œæˆï¼š{len(normalized)} æ¡\n")
        
        return normalized
    
    def clean(self) -> List[Dict]:
        """æ‰§è¡Œå®Œæ•´çš„æ¸…æ´æµç¨‹"""
        logger.info("\n" + "=" * 70)
        logger.info("ã€æ•°æ®æ¸…æ´æµç¨‹ã€‘")
        logger.info("=" * 70 + "\n")
        
        # 1. åŠ è½½
        self.load_raw_data()
        
        if not self.all_data:
            logger.error("âŒ æ— åŸå§‹æ•°æ®ï¼Œæ— æ³•ç»§ç»­")
            return []
        
        # 2. å»é‡
        data = self.deduplicate()
        
        # 3. é•¿åº¦è¿‡æ»¤
        data = self.filter_by_length(data)
        
        # 4. å¹¿å‘Šè¿‡æ»¤
        data = self.filter_ads_and_spam(data)
        
        # 5. è§„èŒƒåŒ–
        data = self.normalize_content(data)
        
        self.stats["final"] = len(data)
        
        return data
    
    def _hash_content(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 70)
        logger.info("ã€æœ€ç»ˆç»Ÿè®¡ã€‘")
        logger.info("=" * 70)
        
        logger.info(f"\næ•°æ®é‡å˜åŒ–ï¼š")
        logger.info(f"  åŸå§‹æ•°æ®ï¼š      {self.stats['total_raw']:6d} æ¡")
        logger.info(f"  å»é‡åï¼š        {self.stats['after_dedup']:6d} æ¡ "
                   f"(-{self.stats['total_raw'] - self.stats['after_dedup']})")
        logger.info(f"  é•¿åº¦è¿‡æ»¤åï¼š    {self.stats['after_filter_length']:6d} æ¡ "
                   f"(-{self.stats['after_dedup'] - self.stats['after_filter_length']})")
        logger.info(f"  å¹¿å‘Šè¿‡æ»¤åï¼š    {self.stats['after_filter_ads']:6d} æ¡ "
                   f"(-{self.stats['after_filter_length'] - self.stats['after_filter_ads']})")
        logger.info(f"  æœ€ç»ˆæ¸…æ´æ•°æ®ï¼š  {self.stats['final']:6d} æ¡")
        
        if self.stats['total_raw'] > 0:
            retention_rate = 100 * self.stats['final'] / self.stats['total_raw']
            logger.info(f"\n  æ•°æ®ä¿ç•™ç‡ï¼š{retention_rate:.1f}%")
        
        logger.info("\nâœ… æ•°æ®æ¸…æ´å®Œæˆ")


# ============================================================================
# è¾“å‡ºå¤„ç†
# ============================================================================

class DataExporter:
    """æ•°æ®å¯¼å‡º"""
    
    @staticmethod
    def export_txt(data: List[Dict], output_file: Path):
        """å¯¼å‡ºä¸ºTXTæ ¼å¼ï¼ˆæ¯è¡Œä¸€æ¡ï¼‰"""
        logger.info(f"\nã€å¯¼å‡ºä¸ºTXTã€‘{output_file}")
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                for item in data:
                    content = item.get("content", "")
                    f.write(content + "\n")
            
            logger.info(f"âœ… å·²ä¿å­˜ {len(data)} æ¡åˆ° {output_file.name}")
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå¤±è´¥ï¼š{e}")
    
    @staticmethod
    def export_json(data: List[Dict], output_file: Path):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        logger.info(f"\nã€å¯¼å‡ºä¸ºJSONã€‘{output_file}")
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total": len(data),
                    "data": data
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… å·²ä¿å­˜ {len(data)} æ¡åˆ° {output_file.name}")
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå¤±è´¥ï¼š{e}")
    
    @staticmethod
    def export_excel(data: List[Dict], output_file: Path):
        """å¯¼å‡ºä¸ºExcelæ ¼å¼ï¼ˆç”¨äºåç»­åˆ†æï¼‰"""
        logger.info(f"\nã€å¯¼å‡ºä¸ºExcelã€‘{output_file}")
        
        try:
            import pandas as pd
            
            df = pd.DataFrame(data)
            df.to_excel(output_file, index=False)
            
            logger.info(f"âœ… å·²ä¿å­˜ {len(data)} æ¡åˆ° {output_file.name}")
            
        except ImportError:
            logger.warning("âš ï¸  æœªå®‰è£…pandasï¼Œè·³è¿‡Excelå¯¼å‡º")
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå¤±è´¥ï¼š{e}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    logger.info("\n" + "=" * 70)
    logger.info("ã€è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®º - æ•°æ®æ¸…æ´ã€‘")
    logger.info("=" * 70)
    
    # 1. æ¸…æ´æ•°æ®
    cleaner = DataCleaner()
    clean_data = cleaner.clean()
    
    if not clean_data:
        logger.error("âŒ æ¸…æ´å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ®")
        return False
    
    # 2. ç»Ÿè®¡
    cleaner.print_statistics()
    
    # 3. å¯¼å‡º
    exporter = DataExporter()
    
    # å¯¼å‡ºä¸ºTXTï¼ˆç”¨äºLLMåˆ†æï¼‰
    exporter.export_txt(clean_data, config.OUTPUT_CONFIG["clean_opinions_file"])
    
    # å¯¼å‡ºä¸ºJSONï¼ˆå¤‡ä»½ï¼‰
    exporter.export_json(clean_data, config.OUTPUT_CONFIG["clean_json_file"])
    
    # å°è¯•å¯¼å‡ºExcel
    try:
        exporter.export_excel(clean_data, config.OUTPUT_CONFIG["clean_excel_file"])
    except:
        pass
    
    logger.info("\n" + "=" * 70)
    logger.info("ã€æ¸…æ´å®Œæˆã€‘")
    logger.info("=" * 70)
    logger.info(f"\nâœ… è¾“å‡ºæ–‡ä»¶ï¼š")
    logger.info(f"   {config.OUTPUT_CONFIG['clean_opinions_file']}")
    logger.info(f"\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼šè¿è¡ŒLLMåˆ†æ")
    logger.info(f"   è§ï¼šSTEP_2_LangExtractå®Œæ•´åˆ†æè®¡åˆ’.md")
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
