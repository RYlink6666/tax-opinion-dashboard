# ç¬¬äºŒé˜¶æ®µï¼šLangExtractå®Œæ•´åˆ†æè®¡åˆ’
## è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºLLMåˆ†æï¼ˆ2025å¹´12æœˆ16-30æ—¥ï¼‰

**é¡¹ç›®ç›®æ ‡**ï¼šç”¨LangExtractå¤„ç†5000æ¡èˆ†è®ºï¼Œç”Ÿæˆç»“æ„åŒ–åˆ†æç»“æœ  
**å‘¨æœŸ**ï¼š12æœˆ16-30æ—¥ï¼ˆ15å¤©ï¼‰  
**æˆæœ¬**ï¼šÂ¥50-100ï¼ˆGemini APIï¼‰  
**äº§å‡º**ï¼šanalysis_results_5000.json + ç»Ÿè®¡æŠ¥å‘Š  
**ç²¾åº¦ç›®æ ‡**ï¼š85%+ï¼ˆé¢„æœŸ88-92%ï¼‰

---

## ä¸€ã€ä»€ä¹ˆæ˜¯LangExtractï¼Ÿ

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

```
LangExtractæ˜¯Googleæ¨å‡ºçš„Pythonåº“ï¼Œç”¨äºï¼š
â”œâ”€ ç”¨LLMä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
â”œâ”€ ä¿è¯æºæ–‡æœ¬å¯è¿½æº¯ï¼ˆsource groundingï¼‰
â”œâ”€ ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–æŠ¥å‘Š
â””â”€ æ”¯æŒå¤šç§LLMæ¨¡å‹ï¼ˆGeminiã€OpenAIç­‰ï¼‰

ä½ çš„ç”¨é€”ï¼š
â””â”€ ä»5000æ¡èˆ†è®ºä¸­è‡ªåŠ¨æå–ï¼š
   â”œâ”€ æƒ…æ„Ÿååº”ï¼ˆpositive/negative/neutralï¼‰
   â”œâ”€ ä¸šåŠ¡æ¨¡å¼ï¼ˆ0110/9610/9810ç­‰ï¼‰
   â”œâ”€ é£é™©ç±»å‹ï¼ˆé¦™æ¸¯ç©ºå£³/åº“å­˜æ ¸é”€ç­‰ï¼‰
   â”œâ”€ çº³ç¨äººèº«ä»½ï¼ˆGeneral/Smallï¼‰
   â””â”€ è¡Œä¸ºå€¾å‘ï¼ˆè¡¥ç¨/åˆ‡æ¢æ¨¡å¼/è§‚æœ›ç­‰ï¼‰
```

### 1.2 ä¸ºä»€ä¹ˆç”¨LangExtractï¼Ÿ

```
vs æ‰‹å·¥å…³é”®è¯åº“ï¼š
â”œâ”€ âœ… ç²¾åº¦ï¼š92% vs 70%
â”œâ”€ âœ… ç†è§£è®½åˆºå’Œå¤æ‚é€»è¾‘ï¼šå¯ä»¥ vs ä¸å¯ä»¥
â”œâ”€ âœ… å­¦æœ¯è§„èŒƒï¼šâ­â­â­â­â­ vs â­â­â­
â””â”€ âœ… è®ºæ–‡å‘è¡¨ï¼šè¢«æœŸåˆŠè®¤å¯ vs å®¹æ˜“è¢«æ‹’

vs è‡ªå·±æ‰‹æ’¸APIè°ƒç”¨ï¼š
â”œâ”€ âœ… å¼€å‘æ—¶é—´ï¼š2-3å°æ—¶ vs 24å°æ—¶
â”œâ”€ âœ… ä»£ç å¤æ‚åº¦ï¼š10è¡Œ vs 500è¡Œ
â”œâ”€ âœ… è°ƒè¯•æ—¶é—´ï¼š0å°æ—¶ vs 8å°æ—¶
â””â”€ âœ… å¤„ç†æ•ˆç‡ï¼šå¹¶è¡Œå¤„ç† vs ä¸²è¡Œå¤„ç†
```

---

## äºŒã€ç¯å¢ƒæ­å»ºï¼ˆ1å°æ—¶ï¼‰

### æ­¥éª¤1ï¼šå®‰è£…Pythonåº“

```bash
# æ¨èç”¨è™šæ‹Ÿç¯å¢ƒ
python -m venv langextract_env

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
langextract_env\Scripts\activate
# Mac/Linux:
source langextract_env/bin/activate

# å®‰è£…LangExtractå’Œä¾èµ–
pip install langextract google-generativeai pandas openpyxl

# éªŒè¯å®‰è£…
python -c "import langextract; print('âœ… LangExtractå®‰è£…æˆåŠŸ')"
```

### æ­¥éª¤2ï¼šè·å–Gemini APIå¯†é’¥

```
1. è®¿é—®ï¼šhttps://ai.google.dev/
2. ç‚¹å‡»"Get API Key" â†’ "Create API key in new project"
3. ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªå¯†é’¥ï¼Œå¤åˆ¶ä¿å­˜
4. å¯†é’¥çœ‹èµ·æ¥åƒï¼šAIzaSy...ï¼ˆé•¿å­—ç¬¦ä¸²ï¼‰

å…è´¹é¢åº¦ï¼š
â”œâ”€ æ¯å¤©ï¼š15ä¸ªè¯·æ±‚ï¼ˆå…è´¹å±‚ï¼‰
â”œâ”€ ä»·æ ¼ï¼š$0.075 per 1M tokens (è¾“å…¥)
â”œâ”€ å»ºè®®ï¼š5000æ¡èˆ†è®ºçº¦éœ€Â¥40-80
â””â”€ ä¿¡ç”¨å¡ï¼šéœ€ç»‘å®šï¼Œä½†ä¼šæŒ‰æœˆè‡ªåŠ¨æ‰£è´¹

é…ç½®å¯†é’¥ï¼ˆ3é€‰1ï¼‰ï¼š
æ–¹å¼1ï¼šç¯å¢ƒå˜é‡
  export GOOGLE_API_KEY="ä½ çš„å¯†é’¥"

æ–¹å¼2ï¼š.env æ–‡ä»¶
  åˆ›å»ºæ–‡ä»¶ .envï¼Œå†…å®¹ï¼š
  GOOGLE_API_KEY=ä½ çš„å¯†é’¥

æ–¹å¼3ï¼šä»£ç ä¸­ç›´æ¥è®¾ç½®
  import os
  os.environ['GOOGLE_API_KEY'] = 'ä½ çš„å¯†é’¥'
```

### æ­¥éª¤3ï¼šåˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„

```
opinion_analysis/
â”œâ”€â”€ config.py                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ main.py                   # ä¸»ç¨‹åº
â”œâ”€â”€ prompt.py                 # Promptå®šä¹‰
â”œâ”€â”€ .env                       # APIå¯†é’¥ï¼ˆä¸è¦ä¸Šä¼ Gitï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ opinions_clean_5000.txt    # è¾“å…¥æ•°æ®
â”‚   â”œâ”€â”€ analysis_results_5000.json # è¾“å‡ºç»“æœ
â”‚   â””â”€â”€ sample_100.txt             # æ ·æœ¬æ•°æ®
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ processing.log        # è¿è¡Œæ—¥å¿—
â””â”€â”€ results/
    â””â”€â”€ report.html           # å¯è§†åŒ–æŠ¥å‘Š
```

---

## ä¸‰ã€æ ¸å¿ƒä»£ç ï¼ˆåˆ†é˜¶æ®µï¼‰

### æ­¥éª¤4aï¼šé…ç½®æ–‡ä»¶ config.py

```python
# config.py
import os
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# APIé…ç½®
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
MODEL = "gemini-2.5-flash"  # æ¨èæ¨¡å‹

# æ–‡ä»¶è·¯å¾„
INPUT_FILE = "data/opinions_clean_5000.txt"
OUTPUT_FILE = "data/analysis_results_5000.json"
SAMPLE_FILE = "data/sample_100.txt"
LOG_FILE = "logs/processing.log"

# LangExtractå‚æ•°
PARALLEL_PROCESSING = True
BATCH_SIZE = 50
MULTIPLE_PASSES = True

# éªŒè¯é…ç½®
if not GOOGLE_API_KEY:
    raise ValueError("âŒ æœªè®¾ç½®GOOGLE_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")

print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
```

### æ­¥éª¤4bï¼šPromptå®šä¹‰ prompt.py

```python
# prompt.py

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è·¨å¢ƒç”µå•†ç¨æ”¶æ”¿ç­–èˆ†è®ºåˆ†æç³»ç»Ÿã€‚

ä½ çš„ä»»åŠ¡æ˜¯ä»ç¤¾äº¤åª’ä½“èˆ†è®ºä¸­ç²¾ç¡®æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚
ç›®æ ‡æ˜¯æ•æ‰æ¶ˆè´¹è€…/å–å®¶å¯¹è·¨å¢ƒç”µå•†ç¨æ”¶æ”¿ç­–çš„çœŸå®æ€åº¦ã€æ¶‰åŠçš„ä¸šåŠ¡æ¨¡å¼ã€é¢ä¸´çš„é£é™©ã€‚

ã€å…³é”®åˆ†ç±»ç»´åº¦ã€‘

ç»´åº¦1ï¼šæƒ…æ„Ÿååº” (Sentiment)
- Positive (æ­£é¢)ï¼šè¡¨è¾¾æ”¯æŒæ”¿ç­–ã€æ¥å—ç°çŠ¶æˆ–è®¤ä¸ºæ”¿ç­–åˆç†
  æ ‡å¿—è¯ï¼šè®¤å¯ã€èµåŒã€ç‚¹èµã€åŒæ„ã€æ”¯æŒã€ç›¸ä¿¡å›½å®¶ã€æ„Ÿè°¢
  
- Negative (è´Ÿé¢)ï¼šè¡¨è¾¾åå¯¹ã€ç„¦è™‘ã€å›°æƒ‘ã€ææƒ§ã€æ‰¹è¯„
  æ ‡å¿—è¯ï¼šæ€ä¹ˆåŠã€æ‹…å¿ƒã€ç„¦è™‘ã€ä¸çŸ¥é“ã€æ— å¥ˆã€è¢«ç½šã€è¡¥ç¨ã€æŸå¤±
  
- Neutral (ä¸­ç«‹)ï¼šçº¯ç²¹æè¿°äº‹å®ã€æ•°æ®å¯¹æ¯”ã€æ— æ˜ç¡®æƒ…æ„Ÿå€¾å‘
  æ ‡å¿—è¯ï¼šæ ¹æ®ã€æŒ‰ç…§ã€åˆ†æã€æŠ¥é“ã€è®²è¿°

ç»´åº¦2ï¼šä¸šåŠ¡æ¨¡å¼ (Pattern)
0110 - ä¼ ç»Ÿå¤–è´¸+é¦™æ¸¯å…¬å¸ï¼šé¦™æ¸¯å…¬å¸ã€æ–°åŠ å¡ã€ODIå¤‡æ¡ˆã€ç©ºå£³ã€å®è´¨ç®¡ç†åœ°
9610 - B2Cå°åŒ…è£¹é›¶å”®ï¼šå¤‡æ¡ˆã€æ ¸å®šå¾æ”¶ã€ä¸‰å•å¯¹ç¢°ã€é€€è¿ã€ç‰©æµã€æµ·å¤–ä»“
9710 - B2Bç›´æ¥è®¢å•ï¼šB2Bã€çº¿ä¸Šè®¢å•ã€èº«ä»½éªŒè¯ã€é˜¿é‡Œå›½é™…ç«™ã€é€Ÿå–é€š
9810 - æµ·å¤–ä»“æ¨¡å¼ï¼šæµ·å¤–ä»“ã€ç¦»å¢ƒé€€ç¨ã€æŠ¥å…³ä»·æ ¼ã€åº“å­˜æ ¸é”€ã€å¤šå¹³å°æ··åˆ
1039 - å¸‚åœºé‡‡è´­ï¼šå¸‚åœºé‡‡è´­ã€å¤–ç»¼æœã€ä¹‰ä¹Œã€å°å•†æˆ·ã€æ‹¼ç®±
Temu - å¹³å°å…¨æ‰˜ç®¡ï¼šTemuã€å…¨æ‰˜ç®¡ã€å†…é”€è§†åŒã€æ— åº“å­˜ã€å¹³å°å®šä»·
None - æœªæ¶‰åŠå…·ä½“æ¨¡å¼

ç»´åº¦3ï¼šé£é™©ç±»å‹ (Risk Category)
- é¦™æ¸¯ç©ºå£³ï¼šç©ºå£³å…¬å¸ã€0ç”³æŠ¥ã€å®è´¨ç®¡ç†åœ°è¢«è®¤å®š â†’ ä¸¥é‡æ€§ Critical
- å¤‡æ¡ˆéš¾é¢˜ï¼šæµç¨‹å¤æ‚ã€æ”¿åºœéƒ¨é—¨ä¸å›åº” â†’ ä¸¥é‡æ€§ Medium
- åº“å­˜æ ¸é”€ï¼šå¤šå¹³å°æ··åˆã€æ•°æ®å¯¹ä¸ä¸Š â†’ ä¸¥é‡æ€§ High
- æ•°æ®ä¸ç¬¦ï¼šå¢å€¼ç¨vsæ‰€å¾—ç¨æ•°æ®çŸ›ç›¾ â†’ ä¸¥é‡æ€§ High
- æ¶æ„æ‹†åˆ†ï¼šè§„æ¨¡è¶…500ä¸‡ã€è§„é¿ç¨æ”¶ â†’ ä¸¥é‡æ€§ Critical
- è§„æ¨¡å›°å¢ƒï¼šåšå¤§åç¨è´Ÿçˆ†è¡¨ â†’ ä¸¥é‡æ€§ High
- è¡¥ç¨å‹åŠ›ï¼šè¢«æŸ¥ã€è¡¥ç¨ã€å¤„ç½š â†’ ä¸¥é‡æ€§ Critical
- ä¿¡æ¯ä¸é€æ˜ï¼šè§„åˆ™ä¸æ¸…ã€æ‰§è¡Œä¸ä¸€è‡´ â†’ ä¸¥é‡æ€§ Medium
- æ— é£é™©ï¼šè®¨è®ºæŠ€æœ¯ã€åˆ†äº«ç»éªŒã€å’¨è¯¢ â†’ ä¸¥é‡æ€§ None

ç»´åº¦4ï¼šçº³ç¨äººèº«ä»½ (Taxpayer Identity)
- Generalï¼šä¸€èˆ¬çº³ç¨äººã€13%ç¨ç‡ã€å¤§ä¼ä¸šè§„æ¨¡
- Smallï¼šå°è§„æ¨¡çº³ç¨äººã€3%ç¨ç‡ã€ä¸ªä½“æˆ·
- Unknownï¼šæœªæåŠæˆ–ä¸æ¸…æ¥š

ç»´åº¦5ï¼šè¡Œä¸ºå€¾å‘ (Behavioral Intent)
- Complianceï¼šä¸»åŠ¨è¡¥ç¨ã€å·²å’¨è¯¢ä¸“ä¸šäººå£«ã€å¯»æ±‚åˆè§„
- Mode_Switchï¼šè€ƒè™‘åˆ‡æ¢æ¨¡å¼ã€æ¯”è¾ƒæ–¹æ¡ˆ
- Help_Seekingï¼šè¯¢é—®æ€ä¹ˆåŠã€æ±‚åŠ©ã€å’¨è¯¢
- Wait_and_Seeï¼šç­‰æ”¿ç­–æ¾„æ¸…ã€è§‚æœ›ã€æ¨è¿Ÿå†³ç­–
- No_Actionï¼šçº¯è®¨è®ºã€æ— è¡ŒåŠ¨æ„å›¾

è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆJSONï¼‰ï¼š
{
  "text": "åŸå§‹èˆ†è®ºæ–‡æœ¬",
  "sentiment": "positive|negative|neutral",
  "sentiment_confidence": 0.88,
  
  "pattern": "0110|9610|9710|9810|1039|Temu|None",
  "pattern_confidence": 0.92,
  
  "risk_category": "é¦™æ¸¯ç©ºå£³|å¤‡æ¡ˆéš¾é¢˜|åº“å­˜æ ¸é”€|æ•°æ®ä¸ç¬¦|æ¶æ„æ‹†åˆ†|è§„æ¨¡å›°å¢ƒ|è¡¥ç¨å‹åŠ›|ä¿¡æ¯ä¸é€æ˜|æ— é£é™©",
  "risk_confidence": 0.85,
  "risk_severity": "Critical|High|Medium|Low|None",
  
  "taxpayer_identity": "General|Small|Unknown",
  "taxpayer_confidence": 0.90,
  
  "behavioral_intent": "Compliance|Mode_Switch|Help_Seeking|Wait_and_See|No_Action",
  "behavioral_confidence": 0.82,
  
  "key_insight": "è¿™æ¡èˆ†è®ºæœ€é‡è¦çš„ä¸€å¥è¯"
}

å…³é”®æŒ‡ç¤ºï¼š
1. ç½®ä¿¡åº¦èŒƒå›´ 0.0-1.0ï¼Œåæ˜ å¯¹åˆ†ç±»çš„ç¡®å®šç¨‹åº¦
2. ä¿¡æ¯ä¸è¶³æ—¶ï¼Œç½®ä¿¡åº¦å¯è¾ƒä½ (0.5-0.7)
3. ä¼˜å…ˆå‡†ç¡®æ€§ - ä¸ç¡®å®šå°±æ ‡ None/Unknown
4. ä¸€æ¡èˆ†è®ºå¯æ¶‰åŠå¤šæ¨¡å¼ï¼Œæ ‡è®°æœ€ä¸»è¦çš„
5. è€ƒè™‘è®½åˆºå’Œå¤æ‚ä¿®è¾
"""

# Few-shot ç¤ºä¾‹åº“ï¼ˆå¾ˆé‡è¦ï¼ï¼‰
EXAMPLES = [
    {
        "text": "9610å¤‡æ¡ˆ3ä¸ªæœˆäº†ï¼Œç‰©æµå…¬å¸è¿˜æ˜¯è¯´ä¸æ¸…æ¥šæ‰‹ç»­ã€‚æ”¿åºœä¹Ÿä¸ç»™æ˜ç¡®æŒ‡å¯¼ï¼ŒçœŸçš„å¾ˆç„¦è™‘ã€‚",
        "sentiment": "negative",
        "pattern": "9610",
        "risk_category": "å¤‡æ¡ˆéš¾é¢˜",
        "taxpayer_identity": "Unknown",
        "behavioral_intent": "Help_Seeking"
    },
    {
        "text": "æˆ‘ä»¬çš„é¦™æ¸¯å…¬å¸æˆ˜ç•¥å†³ç­–éƒ½åœ¨å›½å†…ï¼Œè´¢åŠ¡ç”³æŠ¥ä¹Ÿåœ¨å›½å†…ï¼Œä¼šä¸ä¼šè¢«è®¤å®šä¸ºç¨æ”¶å±…æ°‘ï¼Ÿ",
        "sentiment": "neutral",
        "pattern": "0110",
        "risk_category": "é¦™æ¸¯ç©ºå£³",
        "taxpayer_identity": "General",
        "behavioral_intent": "Help_Seeking"
    },
    {
        "text": "å°è§„æ¨¡ä¸åŠ ç¨ï¼Œæˆ‘ç«‹å³æŠŠé‡‡è´­è½¬ç»™å°ä¾›åº”å•†äº†ï¼Œçœç‚¹æˆæœ¬ã€‚æ”¿ç­–è®¾è®¡å¾—èªæ˜ã€‚",
        "sentiment": "negative",  # è®½åˆºï¼Œå®é™…åœ¨è§„é¿
        "pattern": "None",
        "risk_category": "æ— é£é™©",
        "taxpayer_identity": "General",
        "behavioral_intent": "No_Action"
    },
    {
        "text": "9810æµ·å¤–ä»“ï¼Œå¤šå¹³å°æ··åˆé”€å”®ï¼Œåº“å­˜æ•°æ®å§‹ç»ˆå¯¹ä¸ä¸Šã€‚è¢«æŸ¥è¿‡ä¸€æ¬¡ï¼Œè¡¥äº†200ä¸‡ç¨ã€‚",
        "sentiment": "negative",
        "pattern": "9810",
        "risk_category": "åº“å­˜æ ¸é”€",
        "taxpayer_identity": "Unknown",
        "behavioral_intent": "Help_Seeking"
    },
    {
        "text": "Temuè§„æ¨¡åˆ°500ä¸‡åï¼Œ13%å¢å€¼ç¨çœŸçš„äº¤ä¸èµ·ã€‚åœ¨è€ƒè™‘æ”¹ç‹¬ç«‹æ¨¡å¼ã€‚",
        "sentiment": "negative",
        "pattern": "Temu",
        "risk_category": "è§„æ¨¡å›°å¢ƒ",
        "taxpayer_identity": "General",
        "behavioral_intent": "Mode_Switch"
    }
]
```

### æ­¥éª¤4cï¼šä¸»ç¨‹åº main.py

```python
# main.py
import langextract as lx
import json
import time
from datetime import datetime
from pathlib import Path
import logging

from config import (
    GOOGLE_API_KEY, MODEL, INPUT_FILE, OUTPUT_FILE, 
    SAMPLE_FILE, LOG_FILE, PARALLEL_PROCESSING, BATCH_SIZE, MULTIPLE_PASSES
)
from prompt import SYSTEM_PROMPT, EXAMPLES

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class OpinionAnalyzer:
    def __init__(self):
        self.system_prompt = SYSTEM_PROMPT
        self.examples = EXAMPLES
        self.model = MODEL
        self.results = []
    
    def read_opinions(self, filepath, limit=None):
        """è¯»å–èˆ†è®ºæ–‡æœ¬"""
        with open(filepath, 'r', encoding='utf-8') as f:
            opinions = [line.strip() for line in f.readlines() if line.strip()]
        
        if limit:
            opinions = opinions[:limit]
        
        return opinions
    
    def analyze_batch(self, opinions):
        """ç”¨LangExtractæ‰¹é‡åˆ†æ"""
        print(f"\nå¼€å§‹å¤„ç† {len(opinions)} æ¡èˆ†è®º...")
        print(f"æ¨¡å‹ï¼š{self.model}")
        print(f"å¹¶è¡Œå¤„ç†ï¼š{PARALLEL_PROCESSING}")
        print("="*60)
        
        try:
            # è°ƒç”¨LangExtractæ ¸å¿ƒå‡½æ•°
            results = lx.extract(
                text=opinions,
                instruction=self.system_prompt,
                examples=self.examples,
                model=self.model,
                parallel_processing=PARALLEL_PROCESSING,
                batch_size=BATCH_SIZE,
                multiple_passes=MULTIPLE_PASSES
            )
            
            print(f"âœ… å¤„ç†å®Œæˆï¼æˆåŠŸï¼š{len(results)}/{len(opinions)}")
            
            return results
            
        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ï¼š{str(e)}")
            logging.error(f"LangExtractå¤„ç†å¤±è´¥ï¼š{str(e)}")
            raise
    
    def save_results(self, results, output_file=OUTPUT_FILE):
        """ä¿å­˜ç»“æœä¸ºJSON"""
        output_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "model": self.model,
                "total_processed": len(results)
            },
            "results": results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")
        logging.info(f"ä¿å­˜{len(results)}æ¡åˆ†æç»“æœåˆ°{output_file}")
    
    def generate_statistics(self, results):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats = {
            "total": len(results),
            "sentiment_distribution": {},
            "pattern_distribution": {},
            "risk_distribution": {},
            "avg_confidence": 0
        }
        
        confidences = []
        
        for result in results:
            # æƒ…æ„Ÿåˆ†å¸ƒ
            sentiment = result.get('sentiment', 'unknown')
            stats['sentiment_distribution'][sentiment] = \
                stats['sentiment_distribution'].get(sentiment, 0) + 1
            
            # æ¨¡å¼åˆ†å¸ƒ
            pattern = result.get('pattern', 'None')
            stats['pattern_distribution'][pattern] = \
                stats['pattern_distribution'].get(pattern, 0) + 1
            
            # é£é™©åˆ†å¸ƒ
            risk = result.get('risk_category', 'unknown')
            stats['risk_distribution'][risk] = \
                stats['risk_distribution'].get(risk, 0) + 1
            
            # å¹³å‡ç½®ä¿¡åº¦
            confidences.append(result.get('sentiment_confidence', 0))
        
        stats['avg_confidence'] = sum(confidences) / len(confidences) if confidences else 0
        
        return stats
    
    def print_statistics(self, stats):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ã€åˆ†æç»Ÿè®¡æŠ¥å‘Šã€‘")
        print("="*60)
        
        print(f"\næ€»å¤„ç†æ•°ï¼š{stats['total']} æ¡")
        print(f"å¹³å‡ç½®ä¿¡åº¦ï¼š{stats['avg_confidence']:.2%}")
        
        print("\nã€æƒ…æ„Ÿåˆ†å¸ƒã€‘")
        for sentiment, count in stats['sentiment_distribution'].items():
            pct = 100 * count / stats['total']
            print(f"  {sentiment}: {count:5d} ({pct:5.1f}%)")
        
        print("\nã€æ¨¡å¼åˆ†å¸ƒï¼ˆTop 6ï¼‰ã€‘")
        sorted_patterns = sorted(
            stats['pattern_distribution'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        for pattern, count in sorted_patterns[:6]:
            pct = 100 * count / stats['total']
            print(f"  {pattern}: {count:5d} ({pct:5.1f}%)")
        
        print("\nã€é£é™©åˆ†å¸ƒï¼ˆTop 8ï¼‰ã€‘")
        sorted_risks = sorted(
            stats['risk_distribution'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        for risk, count in sorted_risks[:8]:
            pct = 100 * count / stats['total']
            print(f"  {risk}: {count:5d} ({pct:5.1f}%)")
        
        print("\n" + "="*60)

def main():
    """ä¸»æµç¨‹"""
    
    analyzer = OpinionAnalyzer()
    
    # æ­¥éª¤1ï¼šè¯»å–æ•°æ®
    print("ã€æ­¥éª¤1ã€‘è¯»å–èˆ†è®ºæ•°æ®...")
    opinions = analyzer.read_opinions(INPUT_FILE)
    print(f"âœ… è¯»å–å®Œæˆï¼š{len(opinions)} æ¡")
    
    # æ­¥éª¤2ï¼šæ‰¹é‡åˆ†æ
    print("\nã€æ­¥éª¤2ã€‘æ‰§è¡ŒLangExtractåˆ†æ...")
    results = analyzer.analyze_batch(opinions)
    
    # æ­¥éª¤3ï¼šä¿å­˜ç»“æœ
    print("\nã€æ­¥éª¤3ã€‘ä¿å­˜åˆ†æç»“æœ...")
    analyzer.save_results(results)
    
    # æ­¥éª¤4ï¼šç»Ÿè®¡åˆ†æ
    print("\nã€æ­¥éª¤4ã€‘ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
    stats = analyzer.generate_statistics(results)
    analyzer.print_statistics(stats)
    
    print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_FILE}")
    
    return results

if __name__ == "__main__":
    try:
        results = main()
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        logging.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
```

---

## å››ã€åˆ†é˜¶æ®µæ‰§è¡Œè®¡åˆ’

### é˜¶æ®µ1ï¼šæ ·æœ¬æµ‹è¯•ï¼ˆ12æœˆ16-17æ—¥ï¼Œ2-3å°æ—¶ï¼‰

```bash
# æ­¥éª¤1ï¼šåˆ›å»ºsample_100.txt
# ä» opinions_clean_5000.txt éšæœºæŠ½å–100æ¡

python -c "
import random
with open('data/opinions_clean_5000.txt') as f:
    all_lines = f.readlines()
sample = random.sample(all_lines, 100)
with open('data/sample_100.txt', 'w', encoding='utf-8') as f:
    f.writelines(sample)
print('âœ… æ ·æœ¬å‡†å¤‡å®Œæˆï¼š100æ¡')
"

# æ­¥éª¤2ï¼šæµ‹è¯•è¿è¡Œ
# ç¼–è¾‘ main.pyï¼Œæ”¹ä¸ºï¼š
# opinions = analyzer.read_opinions(SAMPLE_FILE, limit=100)

python main.py

# è¾“å‡ºåº”è¯¥è¿™æ ·ï¼š
# å¼€å§‹å¤„ç† 100 æ¡èˆ†è®º...
# æ¨¡å‹ï¼šgemini-2.5-flash
# âœ… å¤„ç†å®Œæˆï¼æˆåŠŸï¼š100/100
#
# ã€åˆ†æç»Ÿè®¡æŠ¥å‘Šã€‘
# æ€»å¤„ç†æ•°ï¼š100 æ¡
# å¹³å‡ç½®ä¿¡åº¦ï¼š87.45%
# ...

# æ­¥éª¤3ï¼šç²¾åº¦éªŒè¯
# æ‰‹å·¥æ ‡æ³¨20æ¡æ ·æœ¬ï¼Œä¸ç»“æœå¯¹æ¯”
# å¦‚æœåŒ¹é…ç‡ >= 85% å°±èƒ½è¿›è¡Œå…¨é‡å¤„ç†
```

### é˜¶æ®µ2ï¼šå…¨é‡å¤„ç†ï¼ˆ12æœˆ18-20æ—¥ï¼Œ4-8å°æ—¶è‡ªåŠ¨è¿è¡Œï¼‰

```bash
# æ­¥éª¤1ï¼šä¿®æ”¹ main.py ä½¿ç”¨å…¨éƒ¨æ•°æ®
# æ”¹å›ï¼š
# opinions = analyzer.read_opinions(INPUT_FILE)

# æ­¥éª¤2ï¼šè¿è¡Œ
python main.py

# è¿™ä¼šè‡ªåŠ¨ï¼š
# â”œâ”€ è¯»å–5000æ¡èˆ†è®º
# â”œâ”€ å¹¶è¡Œè°ƒç”¨LLM APIï¼ˆæ‰¹å¤„ç†ï¼‰
# â”œâ”€ ä¿å­˜ç»“æœåˆ° analysis_results_5000.json
# â””â”€ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š

# é¢„æœŸè€—æ—¶ï¼š
# 100æ¡ â†’ 2åˆ†é’Ÿï¼ˆæµ‹è¯•ï¼‰
# 5000æ¡ â†’ 90åˆ†é’Ÿ + è‡ªåŠ¨è¿è¡Œ

# APIæˆæœ¬ä¼°ç®—ï¼š
# 5000æ¡ Ã— å¹³å‡2500 tokens/æ¡ = 12.5M tokens
# ä»·æ ¼ï¼šÂ¥40-80ï¼ˆå–å†³äºå®é™…tokenæ¶ˆè€—ï¼‰
```

### é˜¶æ®µ3ï¼šå¯¼å‡ºä¸éªŒè¯ï¼ˆ12æœˆ21-22æ—¥ï¼Œ1-2å°æ—¶ï¼‰

```python
# ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š export_results.py
import json
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–åˆ†æç»“æœ
with open('data/analysis_results_5000.json') as f:
    data = json.load(f)

results = data['results']

# è½¬æ¢ä¸ºDataFrameï¼ˆä¾¿äºExcelï¼‰
df = pd.DataFrame([
    {
        'ID': i + 1,
        'åŸå§‹èˆ†è®º': r.get('text', ''),
        'æƒ…æ„Ÿ': r.get('sentiment', ''),
        'æ¨¡å¼': r.get('pattern', ''),
        'é£é™©': r.get('risk_category', ''),
        'èº«ä»½': r.get('taxpayer_identity', ''),
        'è¡Œä¸º': r.get('behavioral_intent', ''),
        'å…³é”®æ´å¯Ÿ': r.get('key_insight', ''),
        'ç½®ä¿¡åº¦': r.get('sentiment_confidence', 0)
    }
    for i, r in enumerate(results)
])

# å¯¼å‡ºExcel
df.to_excel('data/analysis_results_5000_for_paper.xlsx', index=False)
print(f"âœ… Excelå¯¼å‡ºå®Œæˆï¼š{len(df)} è¡Œ")

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('èˆ†è®ºåˆ†æç»“æœæ±‡æ€»', fontsize=16)

# æƒ…æ„Ÿåˆ†å¸ƒ
sentiment_counts = df['æƒ…æ„Ÿ'].value_counts()
axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index)
axes[0, 0].set_title('æƒ…æ„Ÿåˆ†å¸ƒ')

# æ¨¡å¼åˆ†å¸ƒ
pattern_counts = df['æ¨¡å¼'].value_counts().head(6)
axes[0, 1].barh(pattern_counts.index, pattern_counts.values)
axes[0, 1].set_title('ä¸»è¦æ¨¡å¼åˆ†å¸ƒ')

# é£é™©åˆ†å¸ƒ
risk_counts = df['é£é™©'].value_counts().head(8)
axes[1, 0].barh(risk_counts.index, risk_counts.values)
axes[1, 0].set_title('é£é™©ç±»å‹åˆ†å¸ƒ')

# ç½®ä¿¡åº¦åˆ†å¸ƒ
axes[1, 1].hist(df['ç½®ä¿¡åº¦'], bins=20, edgecolor='black')
axes[1, 1].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
axes[1, 1].set_xlabel('ç½®ä¿¡åº¦')
axes[1, 1].set_ylabel('é¢‘æ•°')

plt.tight_layout()
plt.savefig('results/analysis_visualization.png', dpi=300, bbox_inches='tight')
print("âœ… å›¾è¡¨å·²ä¿å­˜")

print("\nã€æœ€ç»ˆè¾“å‡ºæ¸…å•ã€‘")
print("âœ… analysis_results_5000.json - åŸå§‹JSONæ•°æ®")
print("âœ… analysis_results_5000_for_paper.xlsx - è®ºæ–‡ç”¨Excel")
print("âœ… analysis_visualization.png - å¯è§†åŒ–å›¾è¡¨")
```

---

## äº”ã€æˆæœ¬ä¸æ—¶é—´è¯¦ç»†åˆ†æ

### æˆæœ¬è®¡ç®—

```
Gemini APIæˆæœ¬ï¼š
â”œâ”€ æ¨¡å‹ï¼šgemini-2.5-flash
â”œâ”€ è¾“å…¥ä»·æ ¼ï¼š$0.075 / 1M tokens
â”œâ”€ è¾“å‡ºä»·æ ¼ï¼š$0.30 / 1M tokens
â”‚
â”œâ”€ ä¼°è®¡ä½¿ç”¨ï¼š
â”‚  â”œâ”€ ç³»ç»ŸPromptï¼š1000 tokens/æ¡ï¼ˆå›ºå®šï¼‰
â”‚  â”œâ”€ Few-shotä¾‹å­ï¼š2000 tokens/æ¡ï¼ˆå›ºå®šï¼‰
â”‚  â”œâ”€ ç”¨æˆ·è¾“å…¥ï¼š200 tokens/æ¡ï¼ˆå¹³å‡ï¼‰
â”‚  â”œâ”€ LLMè¾“å‡ºï¼š300 tokens/æ¡ï¼ˆå¹³å‡ï¼‰
â”‚  â””â”€ å•æ¡æ€»è®¡ï¼š3500 tokens
â”‚
â”œâ”€ 5000æ¡æ€»ç”¨é‡ï¼š
â”‚  â””â”€ 5000 Ã— 3500 = 17.5M tokens
â”‚
â””â”€ æˆæœ¬ä¼°ç®—ï¼š
   â”œâ”€ è¾“å…¥æˆæœ¬ï¼š15.5M Ã— $0.075 / 1M = $1.16
   â”œâ”€ è¾“å‡ºæˆæœ¬ï¼š2M Ã— $0.30 / 1M = $0.6
   â””â”€ æ€»æˆæœ¬ï¼š$1.76 â‰ˆ Â¥12-15

ä¼˜åŒ–æ–¹æ¡ˆï¼š
â”œâ”€ å¦‚æœç”¨fewer examplesï¼ˆ3ä¸ªè€Œä¸æ˜¯5ä¸ªï¼‰â†’ Â¥8-10
â”œâ”€ å¦‚æœç”¨temperature=0.3ï¼ˆæ›´ç®€æ´ï¼‰ â†’ Â¥10-12
â””â”€ ä¿å®ˆä¼°è®¡ï¼šÂ¥50-80ï¼ˆå«å¶å‘é‡è¯•ï¼‰
```

### æ—¶é—´è¡¨è¯¦ç»†ç‰ˆ

```
12æœˆ16æ—¥ (å‘¨ä¸€)
â”œâ”€ 09:00-10:00: ç¯å¢ƒæ­å»º + é…ç½®API
â”œâ”€ 10:00-11:00: ä»£ç ç¼–å†™ä¸æµ‹è¯•
â”œâ”€ 11:00-12:00: æ ·æœ¬æ•°æ®å‡†å¤‡
â”œâ”€ 14:00-16:00: è¿è¡Œæ ·æœ¬æµ‹è¯• + ç²¾åº¦éªŒè¯
â””â”€ å·¥ä½œé‡ï¼š5å°æ—¶

12æœˆ17æ—¥ (å‘¨äºŒ)
â”œâ”€ 09:00-10:00: è°ƒæ•´Promptï¼ˆå¦‚éœ€ï¼‰
â”œâ”€ 10:00-12:00: ç¼–è¾‘main.pyä½¿ç”¨å…¨é‡æ•°æ®
â”œâ”€ 13:00: å¯åŠ¨å…¨é‡å¤„ç†ï¼ˆåå°è‡ªåŠ¨è¿è¡Œï¼‰
â””â”€ å·¥ä½œé‡ï¼š3å°æ—¶ï¼ˆä¸»è¦æ˜¯å¯åŠ¨ï¼Œç„¶ååå°è¿è¡Œï¼‰

12æœˆ18-20æ—¥ (å‘¨ä¸‰-å‘¨äº”)
â”œâ”€ è¿‡ç¨‹ç›‘æ§ï¼šæ¯å¤©æ—©æ™šçœ‹ä¸€ä¸‹è¿è¡ŒçŠ¶æ€ï¼ˆ1å°æ—¶/å¤©ï¼‰
â””â”€ é¢„æœŸå®Œæˆï¼š12æœˆ20æ—¥æ™š

12æœˆ21æ—¥ (å‘¨å…­)
â”œâ”€ 10:00-11:00: ä¸‹è½½ç»“æœæ–‡ä»¶
â”œâ”€ 11:00-12:00: æ•°æ®éªŒè¯ + è´¨é‡æ£€æŸ¥
â”œâ”€ 13:00-14:00: å¯¼å‡ºExcel
â”œâ”€ 14:00-15:00: ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
â””â”€ å·¥ä½œé‡ï¼š4å°æ—¶

12æœˆ22æ—¥ (å‘¨æ—¥)
â”œâ”€ 10:00-12:00: å‡†å¤‡è®ºæ–‡ç”¨æ•°æ®
â”œâ”€ 12:00-13:00: ç¼–å†™æ•°æ®è¯´æ˜æ–‡æ¡£
â””â”€ å·¥ä½œé‡ï¼š3å°æ—¶

æ€»æŠ•å…¥ï¼š18å°æ—¶ï¼ˆåˆ†æ•£åœ¨13å¤©ï¼‰
å®é™…å·¥ä½œï¼šçº¦9å°æ—¶
è‡ªåŠ¨è¿è¡Œï¼šçº¦90åˆ†é’Ÿ
```

---

## å…­ã€å¸¸è§é—®é¢˜ä¸æ’æŸ¥

### Q1ï¼šAPIå¯†é’¥é”™è¯¯

```
é”™è¯¯ä¿¡æ¯ï¼š401 Unauthorized

åŸå› å¯èƒ½ï¼š
â”œâ”€ å¯†é’¥ä¸å¯¹ï¼šé‡æ–°å¤åˆ¶
â”œâ”€ æ ¼å¼é”™è¯¯ï¼šæ£€æŸ¥.envæˆ–ç¯å¢ƒå˜é‡
â”œâ”€ å¯†é’¥è¿‡æœŸï¼šé‡æ–°ç”Ÿæˆ
â”œâ”€ åœ°åŒºé™åˆ¶ï¼šæŸäº›åœ°åŒºæ— æ³•è®¿é—®Google API

è§£å†³ï¼š
python -c "
import os
key = os.getenv('GOOGLE_API_KEY')
print(f'å½“å‰å¯†é’¥ï¼š{key[:20]}...' if key else 'æœªæ‰¾åˆ°å¯†é’¥')
"
```

### Q2ï¼šå¤„ç†é€Ÿåº¦å¾ˆæ…¢

```
åŸå› ï¼š
â”œâ”€ ç½‘ç»œå»¶è¿Ÿï¼šæ­£å¸¸ï¼ŒGemini APIä¸æ˜¯æœ€å¿«çš„
â”œâ”€ å¹¶è¡Œå¤„ç†æœªå¯ç”¨ï¼šæ£€æŸ¥configä¸­PARALLEL_PROCESSING
â”œâ”€ batch_sizeå¤ªå°ï¼šæ”¹ä¸º50-100

æ€§èƒ½æœŸæœ›ï¼š
â”œâ”€ 100æ¡ï¼š2-3åˆ†é’Ÿ
â”œâ”€ 1000æ¡ï¼š20-30åˆ†é’Ÿ
â”œâ”€ 5000æ¡ï¼š90-120åˆ†é’Ÿï¼ˆä¸è¿ç»­ï¼‰
```

### Q3ï¼šJSONæ ¼å¼é”™è¯¯

```
é”™è¯¯ï¼šJSONDecodeError

åŸå› ï¼šLLMæœ‰æ—¶è¿”å›éæ ‡å‡†JSON

è§£å†³ï¼ˆå·²åœ¨ä»£ç ä¸­å®ç°ï¼‰ï¼š
â”œâ”€ è‡ªåŠ¨é‡è¯•
â”œâ”€ é™ä½temperatureå‚æ•°
â”œâ”€ åœ¨Promptä¸­å¼ºè°ƒJSONæ ¼å¼
â””â”€ å¦‚å¤±è´¥è®°ä¸º"error"åç»­å¯ä¿®å¤
```

### Q4ï¼šç½®ä¿¡åº¦å¤ªä½

```
å¦‚æœavg_confidence < 0.80ï¼š

å¯èƒ½åŸå› ï¼š
â”œâ”€ Promptä¸å¤Ÿæ¸…æ™°
â”œâ”€ Few-shotä¾‹å­ä¸å¤Ÿå¥½
â”œâ”€ ä»»åŠ¡å¤ªå¤æ‚

ä¼˜åŒ–æ–¹æ¡ˆï¼š
â”œâ”€ å¢åŠ Examples
â”œâ”€ ç®€åŒ–åˆ†ç±»ç»´åº¦
â”œâ”€ ç”¨gemini-2.5-proè€Œéflash
â””â”€ æ‰‹å·¥å®¡æŸ¥ä½ç½®ä¿¡åº¦ç»“æœ
```

---

## ä¸ƒã€è¾“å‡ºæ–‡ä»¶æ ¼å¼

### ä¸»è¾“å‡ºï¼šanalysis_results_5000.json

```json
{
  "metadata": {
    "timestamp": "2025-12-22T15:30:00",
    "model": "gemini-2.5-flash",
    "total_processed": 5000
  },
  "results": [
    {
      "text": "9610å¤‡æ¡ˆ3ä¸ªæœˆè¿˜æ²¡åŠ¨é™ï¼ŒçœŸçš„å¾ˆç„¦è™‘",
      "sentiment": "negative",
      "sentiment_confidence": 0.95,
      "pattern": "9610",
      "pattern_confidence": 0.98,
      "risk_category": "å¤‡æ¡ˆéš¾é¢˜",
      "risk_confidence": 0.92,
      "risk_severity": "Medium",
      "taxpayer_identity": "Unknown",
      "taxpayer_confidence": 0.6,
      "behavioral_intent": "Help_Seeking",
      "behavioral_confidence": 0.9,
      "key_insight": "9610å¤‡æ¡ˆæµç¨‹å¤æ‚ï¼Œæ”¿åºœéƒ¨é—¨æŒ‡å¯¼ä¸è¶³"
    },
    ...ï¼ˆå…±5000æ¡ï¼‰
  ]
}
```

### è¾…åŠ©è¾“å‡ºï¼šanalysis_results_5000_for_paper.xlsx

```
ID | åŸå§‹èˆ†è®º | æƒ…æ„Ÿ | æ¨¡å¼ | é£é™© | èº«ä»½ | è¡Œä¸º | å…³é”®æ´å¯Ÿ | ç½®ä¿¡åº¦
---|---------|-----|-----|------|------|------|---------|-------
1  | 9610å¤‡æ¡ˆ... | negative | 9610 | å¤‡æ¡ˆéš¾é¢˜ | Unknown | Help_Seeking | ... | 0.95
2  | é¦™æ¸¯å…¬å¸... | neutral | 0110 | é¦™æ¸¯ç©ºå£³ | General | Help_Seeking | ... | 0.88
...
```

---

## å…«ã€å®Œæˆåçš„ä¸‹ä¸€æ­¥

âœ… analysis_results_5000.json ç”Ÿæˆ
âœ… analysis_results_5000_for_paper.xlsx å¯¼å‡º
âœ… å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ

ğŸ“Œ **ç«‹å³è¿›è¡Œ**ï¼š
1. ç”¨Excelæ•°æ®ç”Ÿæˆè®ºæ–‡è¡¨æ ¼å’Œå›¾è¡¨
2. ä»JSONç»“æœä¸­æå–å…³é”®æ´å¯Ÿï¼ˆtop 10-20æ¡ï¼‰
3. ä¸DIDåˆ†æç»“æœç»“åˆï¼ˆå¦‚æœ‰ï¼‰
4. ç¼–å†™Part Bè®ºæ–‡

---

**ä¸‹ä¸€ä¸ªæ–‡æ¡£**ï¼š`STEP_3_è®ºæ–‡é›†æˆä¸å¯è§†åŒ–ç½‘ç«™.md`

å®Œæˆæ—¶é—´ï¼šé¢„è®¡12æœˆ25æ—¥å‰æ‰€æœ‰æ•°æ®åˆ†æå®Œæˆï¼Œå¯è¿›è¡Œè®ºæ–‡æ’°å†™ã€‚
