# -*- coding: utf-8 -*-
"""
è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºé‡‡é›† - çŸ¥ä¹çˆ¬è™«
ç›®æ ‡ï¼šé‡‡é›†1500æ¡çŸ¥ä¹ç›¸å…³é—®ç­”
æ—¶é—´ï¼š2025å¹´6æœˆ-12æœˆ
"""

import requests
import json
import time
import random
from datetime import datetime
from bs4 import BeautifulSoup
import csv

class ZhihuSpider:
    """çŸ¥ä¹çˆ¬è™« - é‡‡é›†è·¨å¢ƒç”µå•†ç¨æ”¶è®¨è®º"""
    
    def __init__(self):
        self.posts = []
        # çŸ¥ä¹å…³é”®è¯ï¼šé—®é¢˜å…³é”®è¯
        self.keywords = [
            'è·¨å¢ƒç”µå•†å¢å€¼ç¨',
            'è·¨å¢ƒç”µå•†ç¨æ”¶æ”¿ç­–',
            '9610æµ·å…³ç¼–ç ',
            '1039å¸‚åœºé‡‡è´­',
            'Temuç¨åŠ¡',
            'é¦™æ¸¯å…¬å¸ç¨æ”¶å±…æ°‘',
            'æµ·å¤–ä»“æŠ¥å…³',
            'ç”µå•†è¡¥ç¨',
        ]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self._random_user_agent()
        })
    
    def _random_user_agent(self):
        """éšæœºUser-Agent"""
        agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        return random.choice(agents)
    
    def search_zhihu(self, keyword, num_pages=2):
        """
        æœç´¢çŸ¥ä¹é—®ç­”
        """
        print(f"\nğŸ’¡ å¼€å§‹é‡‡é›†çŸ¥ä¹ï¼š{keyword}")
        
        for page in range(1, num_pages + 1):
            try:
                # çŸ¥ä¹æœç´¢URL
                url = f"https://www.zhihu.com/search?type=content&q={keyword}&page={page}"
                
                response = self.session.get(url, timeout=10)
                response.encoding = 'utf-8'
                
                if response.status_code != 200:
                    print(f"   âŒ ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # çŸ¥ä¹æœç´¢ç»“æœé€‰æ‹©å™¨ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
                items = soup.find_all('div', attrs={'class': lambda x: x and 'SearchResult' in (x or '')})
                
                if not items:
                    # å°è¯•å…¶ä»–é€‰æ‹©å™¨
                    items = soup.find_all('article', attrs={'class': lambda x: x and 'search' in (x or '').lower()})
                
                count_this_page = 0
                for item in items:
                    try:
                        # æå–æ ‡é¢˜å’Œå†…å®¹
                        title_elem = item.find('h2')
                        if not title_elem:
                            title_elem = item.find('a', attrs={'class': lambda x: x and 'title' in (x or '').lower()})
                        
                        content_elem = item.find('p', attrs={'class': lambda x: x and 'content' in (x or '').lower()})
                        if not content_elem:
                            content_elem = item.find('p')
                        
                        title = title_elem.get_text(strip=True) if title_elem else ""
                        content = content_elem.get_text(strip=True) if content_elem else ""
                        
                        # ç»„åˆæ–‡æœ¬
                        text = f"{title} {content}".strip()
                        
                        if len(text) < 20:
                            continue
                        
                        # è¿‡æ»¤å¹¿å‘Š
                        spam_keywords = ['æ¨å¹¿', 'å¹¿å‘Š', 'è´­ä¹°', 'é“¾æ¥']
                        if any(kw in text for kw in spam_keywords):
                            continue
                        
                        # æå–ç‚¹èµæ•°
                        vote_elem = item.find('button', attrs={'class': lambda x: x and 'vote' in (x or '').lower()})
                        votes = 0
                        if vote_elem:
                            try:
                                votes = int(vote_elem.get_text())
                            except:
                                pass
                        
                        self.posts.append({
                            'platform': 'zhihu',
                            'keyword': keyword,
                            'text': text[:500],
                            'votes': votes,
                            'collected_at': datetime.now().isoformat(),
                            'source_url': url
                        })
                        count_this_page += 1
                        
                    except Exception as e:
                        continue
                
                print(f"   âœ“ ç¬¬{page}é¡µï¼šé‡‡é›† {count_this_page} æ¡")
                
                # å»¶è¿Ÿ
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"   âŒ å‡ºé”™ï¼š{str(e)}")
                time.sleep(random.uniform(3, 7))
    
    def save_to_json(self, filename='zhihu_raw_data.json'):
        """ä¿å­˜ä¸ºJSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.posts, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜ {len(self.posts)} æ¡æ•°æ®åˆ° {filename}")
        return filename
    
    def save_to_csv(self, filename='zhihu_raw_data.csv'):
        """ä¿å­˜ä¸ºCSV"""
        if not self.posts:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['platform', 'keyword', 'text', 'votes', 'collected_at', 'source_url'])
            writer.writeheader()
            writer.writerows(self.posts)
        print(f"âœ… å·²ä¿å­˜ {len(self.posts)} æ¡æ•°æ®åˆ° {filename}")
        return filename
    
    def run(self):
        """æ‰§è¡Œé‡‡é›†æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹é‡‡é›†è·¨å¢ƒç”µå•†ç¨æ”¶èˆ†è®ºï¼ˆçŸ¥ä¹ç‰ˆï¼‰")
        print("=" * 60)
        
        for keyword in self.keywords:
            self.search_zhihu(keyword, num_pages=2)
            print(f"   ç›®å‰å·²é‡‡é›†ï¼š{len(self.posts)} æ¡")
        
        if self.posts:
            self.save_to_json()
            self.save_to_csv()
            print("\n" + "=" * 60)
            print(f"ğŸ“Š é‡‡é›†å®Œæˆï¼šå…± {len(self.posts)} æ¡çŸ¥ä¹å†…å®¹")
            print("=" * 60)
        else:
            print("\nâŒ æœªé‡‡é›†åˆ°ä»»ä½•æ•°æ®")
        
        return self.posts


if __name__ == "__main__":
    spider = ZhihuSpider()
    posts = spider.run()
